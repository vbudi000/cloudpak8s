{"componentChunkName":"component---src-pages-content-integration-deploy-fast-file-transfer-md","path":"/content/integration/deploy-fast-file-transfer/","result":{"pageContext":{"frontmatter":{"title":"deploy-fast-file-transfer","weight":600},"relativePagePath":"/content/integration/deploy-fast-file-transfer.md","titleType":"page","MdxNode":{"id":"eb5a3473-22f6-5e7c-a977-e11b2d9c894e","children":[],"parent":"65e37c71-4730-5052-8309-ce026b7ae7a2","internal":{"content":"---\ntitle: deploy-fast-file-transfer\nweight: 600\n---\n\n- [Introduction](#introduction)\n- [Prepare Installation](#prepare-installation)\n- [Begin Installation](#begin-installation)\n- [Validate installation](#validate-installation)\n\n### Introduction\nThis page contains guidance on how to configure the Aspera release for both on-prem and ROKS.\n\n### Prepare Installation\n\n1. **Change project to aspera**\n   ```\n   oc project aspera\n   ```\n2. **Use Node Labels:**  \n\n    In order to ensure high availability, the Aspera Swarm services will attempt to create a configurable number of pods on each node in the Kubernetes cluster. The nodes on which the receiver pods are running can be restricted via the nodeLabels values.  \n    \n    For example, the following would restrict pods to nodes with the `node-role.kubernetes.io/ascp=true` label or `node-role.kubernetes.io/noded=true` label.\n\n    ```\n    ascpSwarm:\n    config:\n        nodeLabels:\n        node-role.kubernetes.io/ascp: \"true\"\n\n    nodedSwarm:\n    config:\n        nodeLabels:\n        node-role.kubernetes.io/noded: \"true\"\n    ```      \n    \n    Label Nodes using the command  \n\n    ```\n    oc label node &lt;node-name&gt; node-role.kubernetes.io/&lt;role&gt;=true\n    ```\n\n3. **Additional RBAC Requirements:**  \n\n    The following RBAC resources are also required before you deploy the chart. Use the command `oc create -f &lt;filename.yaml&gt;`\n\n    - **Cluster Admin**\n      - [ClusterRole](/assets/img/integration/aspera/files/cluster-admin-clusterrole.yaml)\n    - **Namespace User**  \n      Substitute {{ NAMESPACE }} with the namespace the chart will be deployed in.\n      - [ClusterRoleBinding](/assets/img/integration/aspera/files/namespace-user-clusterrole.yaml)\n      - [Role](/assets/img/integration/aspera/files/namespace-user-role.yaml)\n      - [RoleBinding](/assets/img/integration/aspera/files/namespace-user-rolebinding.yaml)\n      - [RoleBinding](/assets/img/integration/aspera/files/hsts-prod-rolebinding.yaml)\n      - [ServiceAccount](/assets/img/integration/aspera/files/apsera-sa-role.yaml) - Set to `ibm-entitlement-key` if using entitled registry or if offline use the `deployer-dockercfg-XX` secret in your namespace.  Use `oc get secrets` to get the value.\n      - [Secret Generation Role](/assets/img/integration/aspera/files/secret-gen-role.yaml)\n      - [Secret Generation RoleBinding](/assets/img/integration/aspera/files/secret-gen-rolebinding.yaml)\n      - [Secret Generation ServiceAccount](/assets/img/integration/aspera/files/secret-gen-sa.yaml)  \n\n4. **Create the secrets**\n\nMake sure you have copied your aspera license key to the location where you will be creating the secrets.  The following command assumes it is named `aspera-license`.\n   \n   ```\n   oc create secret generic aspera-server --from -file=ASPERA_LICENSE=\"./aspera-license\" --from-literal=TOKEN_ENCRYPTION_KEY=\"my_encryption_key\"\n\n   kubectl create secret generic asperanode-nodeadmin --from-literal=NODE_USER=\"myuser\" --from-literal=NODE_PASS=\"mypassword\"\n   \n   kubectl create secret generic asperanode-accesskey --from-literal=ACCESS_KEY_ID=\"my_access_key\" --from-literal=ACCESS_KEY_SECRET=\"my_access_key_secret\"\n   ```\n\n### Begin Installation\n1. Go to CP4I Platform Home. Click **Create instance** inside the **Aspera** tile.    \n1. A window will pop up with a description of the requirements for installing. Click **Continue** to the helm chart deployment configuration.\n2. Click **Overview** to view the chart information and pre-reqs that were covered in [Prepare Installation](#prepare-installation).\n3. Click **Configure**\n4. Enter the Helm release name. In our example, **Aspera-1**\n5. Enter Target Namespace - **Aspera**\n6. Select a Cluster - **local-cluster**.\n7. Tick the license agreement checkbox.\n8. Under Parameters -> Quick start\n   1. Ingress - icp-proxy address defined during icp / common-services installation - icp-proxy.\\&lt;openshift-router-domain&gt;  \n   2. Aspera Node - Server Secret - the secret created using the license - `aspera-server`\n   3. Aspera Event Journal - Kafka Host - use hostname of bootstrap server of existing eventstreams installation. Get this value from the Eventstreams web ui.  \n   4. Aspera Rproxy - address of cluster proxy.  This can be configured later if need be.\n9.  Click All Parameters\n10. Uncheck production usage\n11. Image Pull Secret - the secret used to pull images for install from the docker registry. Set to `ibm-entitlement-key` if using entitled registry or if offline use the `deployer-dockercfg-XX` secret in your namespace.\n12. Scroll down to the Redis section.\n13. Check Persistence Enabled.\n14. Check Use dynamic provisioning.\n15. Storage Class Name - enter storage class for file storage\n16. Image Pull Secret - same as step 11.  \n17. Scroll down to Persistence\n18. Enter the same Storage Class Name as step 15\n19. Proceed to the section Aspera Node\n20. Node Admin Secret - enter the nodeadmin secret created in the preious section - `asperanode-nodeadmin`\n21. Access Key Secret - enter the access key secret created in the previous section - `asperanode-accesskey`\n22. Proceed to the section - Aspera Event Journal\n23. Kafka Port - change to Kafka port found in Eventstreams bootstrapi server.  \n24. Proceed to section Ascp Swarm\n25. Node Labels - enter the node labels created in the previous section for identifying ascp swarm nodes -  \n```\n{-node-role.kubernetes.io/ascp: true}\n```\n26. Proceed to section - Noded Swarm\n27. Node Labels - set to the node label created for noded from the previous section\n```\n{-node-role.kubernetes.io/noded: \"true\"}\n```\n28. Scroll to section - Sch\n29. Image Pull Secret - the secret used to pull images for install from the docker registry. Set to `ibm-entitlement-key` if using entitled registry or if offline use the `deployer-dockercfg-XX` secret in your namespace.\n\n### Validate installation    \n\n1. View all pods running\n    ```\n    NAME                                                       READY     STATUS      RESTARTS   AGE\n    aspera-1-aspera-hsts-aej-d8c5b5569-24vh8                   1/1       Running     0          3m\n    aspera-1-aspera-hsts-aej-d8c5b5569-68nvj                   1/1       Running     0          3m\n    aspera-1-aspera-hsts-aej-d8c5b5569-v5xgb                   1/1       Running     0          3m\n    aspera-1-aspera-hsts-ascp-loadbalancer-75849464b-lq8lz     1/1       Running     0          3m\n    aspera-1-aspera-hsts-ascp-swarm-54c98cb6bb-hznw5           2/2       Running     0          3m\n    aspera-1-aspera-hsts-create-access-key-v1-24hdg            0/1       Completed   0          3m\n    aspera-1-aspera-hsts-http-proxy-8b86df4f-8hd6d             1/1       Running     0          3m\n    aspera-1-aspera-hsts-node-api-796f5c8ccc-r9xs2             2/2       Running     0          3m\n    aspera-1-aspera-hsts-node-master-788774bbc7-8sl2s          2/2       Running     0          3m\n    aspera-1-aspera-hsts-noded-loadbalancer-844977799b-f4gd6   1/1       Running     0          3m\n    aspera-1-aspera-hsts-noded-swarm-6b8498fd-slj8g            2/2       Running     0          3m\n    aspera-1-aspera-hsts-prometheus-endpoint-bc5974d79-4fv4t   2/2       Running     0          3m\n    aspera-1-aspera-hsts-prometheus-endpoint-bc5974d79-d426s   2/2       Running     0          3m\n    aspera-1-aspera-hsts-prometheus-endpoint-bc5974d79-t7f8l   2/2       Running     0          3m\n    aspera-1-aspera-hsts-stats-5c5c8cc8fc-c2gbr                2/2       Running     0          3m\n    aspera-1-aspera-hsts-stats-5c5c8cc8fc-lcbxr                2/2       Running     0          3m\n    aspera-1-aspera-hsts-stats-5c5c8cc8fc-qpj5l                2/2       Running     0          3m\n    aspera-1-aspera-hsts-tcp-proxy-748b6bb64-j478m             1/1       Running     0          3m\n    aspera-1-redis-ha-sentinel-0                               1/1       Running     0          3m\n    aspera-1-redis-ha-sentinel-1                               1/1       Running     0          2m\n    aspera-1-redis-ha-sentinel-2                               1/1       Running     0          1m\n    aspera-1-redis-ha-server-0                                 1/1       Running     0          3m\n    aspera-1-redis-ha-server-1                                 1/1       Running     0          2m\n    aspera-1-redis-ha-server-2                                 1/1       Running     0          2m\n    ```\n\n\n\n","type":"Mdx","contentDigest":"48eb5ad0f4e3a68902d55d8c4a79fb36","counter":176,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"deploy-fast-file-transfer","weight":600},"exports":{},"rawBody":"---\ntitle: deploy-fast-file-transfer\nweight: 600\n---\n\n- [Introduction](#introduction)\n- [Prepare Installation](#prepare-installation)\n- [Begin Installation](#begin-installation)\n- [Validate installation](#validate-installation)\n\n### Introduction\nThis page contains guidance on how to configure the Aspera release for both on-prem and ROKS.\n\n### Prepare Installation\n\n1. **Change project to aspera**\n   ```\n   oc project aspera\n   ```\n2. **Use Node Labels:**  \n\n    In order to ensure high availability, the Aspera Swarm services will attempt to create a configurable number of pods on each node in the Kubernetes cluster. The nodes on which the receiver pods are running can be restricted via the nodeLabels values.  \n    \n    For example, the following would restrict pods to nodes with the `node-role.kubernetes.io/ascp=true` label or `node-role.kubernetes.io/noded=true` label.\n\n    ```\n    ascpSwarm:\n    config:\n        nodeLabels:\n        node-role.kubernetes.io/ascp: \"true\"\n\n    nodedSwarm:\n    config:\n        nodeLabels:\n        node-role.kubernetes.io/noded: \"true\"\n    ```      \n    \n    Label Nodes using the command  \n\n    ```\n    oc label node &lt;node-name&gt; node-role.kubernetes.io/&lt;role&gt;=true\n    ```\n\n3. **Additional RBAC Requirements:**  \n\n    The following RBAC resources are also required before you deploy the chart. Use the command `oc create -f &lt;filename.yaml&gt;`\n\n    - **Cluster Admin**\n      - [ClusterRole](/assets/img/integration/aspera/files/cluster-admin-clusterrole.yaml)\n    - **Namespace User**  \n      Substitute {{ NAMESPACE }} with the namespace the chart will be deployed in.\n      - [ClusterRoleBinding](/assets/img/integration/aspera/files/namespace-user-clusterrole.yaml)\n      - [Role](/assets/img/integration/aspera/files/namespace-user-role.yaml)\n      - [RoleBinding](/assets/img/integration/aspera/files/namespace-user-rolebinding.yaml)\n      - [RoleBinding](/assets/img/integration/aspera/files/hsts-prod-rolebinding.yaml)\n      - [ServiceAccount](/assets/img/integration/aspera/files/apsera-sa-role.yaml) - Set to `ibm-entitlement-key` if using entitled registry or if offline use the `deployer-dockercfg-XX` secret in your namespace.  Use `oc get secrets` to get the value.\n      - [Secret Generation Role](/assets/img/integration/aspera/files/secret-gen-role.yaml)\n      - [Secret Generation RoleBinding](/assets/img/integration/aspera/files/secret-gen-rolebinding.yaml)\n      - [Secret Generation ServiceAccount](/assets/img/integration/aspera/files/secret-gen-sa.yaml)  \n\n4. **Create the secrets**\n\nMake sure you have copied your aspera license key to the location where you will be creating the secrets.  The following command assumes it is named `aspera-license`.\n   \n   ```\n   oc create secret generic aspera-server --from -file=ASPERA_LICENSE=\"./aspera-license\" --from-literal=TOKEN_ENCRYPTION_KEY=\"my_encryption_key\"\n\n   kubectl create secret generic asperanode-nodeadmin --from-literal=NODE_USER=\"myuser\" --from-literal=NODE_PASS=\"mypassword\"\n   \n   kubectl create secret generic asperanode-accesskey --from-literal=ACCESS_KEY_ID=\"my_access_key\" --from-literal=ACCESS_KEY_SECRET=\"my_access_key_secret\"\n   ```\n\n### Begin Installation\n1. Go to CP4I Platform Home. Click **Create instance** inside the **Aspera** tile.    \n1. A window will pop up with a description of the requirements for installing. Click **Continue** to the helm chart deployment configuration.\n2. Click **Overview** to view the chart information and pre-reqs that were covered in [Prepare Installation](#prepare-installation).\n3. Click **Configure**\n4. Enter the Helm release name. In our example, **Aspera-1**\n5. Enter Target Namespace - **Aspera**\n6. Select a Cluster - **local-cluster**.\n7. Tick the license agreement checkbox.\n8. Under Parameters -> Quick start\n   1. Ingress - icp-proxy address defined during icp / common-services installation - icp-proxy.\\&lt;openshift-router-domain&gt;  \n   2. Aspera Node - Server Secret - the secret created using the license - `aspera-server`\n   3. Aspera Event Journal - Kafka Host - use hostname of bootstrap server of existing eventstreams installation. Get this value from the Eventstreams web ui.  \n   4. Aspera Rproxy - address of cluster proxy.  This can be configured later if need be.\n9.  Click All Parameters\n10. Uncheck production usage\n11. Image Pull Secret - the secret used to pull images for install from the docker registry. Set to `ibm-entitlement-key` if using entitled registry or if offline use the `deployer-dockercfg-XX` secret in your namespace.\n12. Scroll down to the Redis section.\n13. Check Persistence Enabled.\n14. Check Use dynamic provisioning.\n15. Storage Class Name - enter storage class for file storage\n16. Image Pull Secret - same as step 11.  \n17. Scroll down to Persistence\n18. Enter the same Storage Class Name as step 15\n19. Proceed to the section Aspera Node\n20. Node Admin Secret - enter the nodeadmin secret created in the preious section - `asperanode-nodeadmin`\n21. Access Key Secret - enter the access key secret created in the previous section - `asperanode-accesskey`\n22. Proceed to the section - Aspera Event Journal\n23. Kafka Port - change to Kafka port found in Eventstreams bootstrapi server.  \n24. Proceed to section Ascp Swarm\n25. Node Labels - enter the node labels created in the previous section for identifying ascp swarm nodes -  \n```\n{-node-role.kubernetes.io/ascp: true}\n```\n26. Proceed to section - Noded Swarm\n27. Node Labels - set to the node label created for noded from the previous section\n```\n{-node-role.kubernetes.io/noded: \"true\"}\n```\n28. Scroll to section - Sch\n29. Image Pull Secret - the secret used to pull images for install from the docker registry. Set to `ibm-entitlement-key` if using entitled registry or if offline use the `deployer-dockercfg-XX` secret in your namespace.\n\n### Validate installation    \n\n1. View all pods running\n    ```\n    NAME                                                       READY     STATUS      RESTARTS   AGE\n    aspera-1-aspera-hsts-aej-d8c5b5569-24vh8                   1/1       Running     0          3m\n    aspera-1-aspera-hsts-aej-d8c5b5569-68nvj                   1/1       Running     0          3m\n    aspera-1-aspera-hsts-aej-d8c5b5569-v5xgb                   1/1       Running     0          3m\n    aspera-1-aspera-hsts-ascp-loadbalancer-75849464b-lq8lz     1/1       Running     0          3m\n    aspera-1-aspera-hsts-ascp-swarm-54c98cb6bb-hznw5           2/2       Running     0          3m\n    aspera-1-aspera-hsts-create-access-key-v1-24hdg            0/1       Completed   0          3m\n    aspera-1-aspera-hsts-http-proxy-8b86df4f-8hd6d             1/1       Running     0          3m\n    aspera-1-aspera-hsts-node-api-796f5c8ccc-r9xs2             2/2       Running     0          3m\n    aspera-1-aspera-hsts-node-master-788774bbc7-8sl2s          2/2       Running     0          3m\n    aspera-1-aspera-hsts-noded-loadbalancer-844977799b-f4gd6   1/1       Running     0          3m\n    aspera-1-aspera-hsts-noded-swarm-6b8498fd-slj8g            2/2       Running     0          3m\n    aspera-1-aspera-hsts-prometheus-endpoint-bc5974d79-4fv4t   2/2       Running     0          3m\n    aspera-1-aspera-hsts-prometheus-endpoint-bc5974d79-d426s   2/2       Running     0          3m\n    aspera-1-aspera-hsts-prometheus-endpoint-bc5974d79-t7f8l   2/2       Running     0          3m\n    aspera-1-aspera-hsts-stats-5c5c8cc8fc-c2gbr                2/2       Running     0          3m\n    aspera-1-aspera-hsts-stats-5c5c8cc8fc-lcbxr                2/2       Running     0          3m\n    aspera-1-aspera-hsts-stats-5c5c8cc8fc-qpj5l                2/2       Running     0          3m\n    aspera-1-aspera-hsts-tcp-proxy-748b6bb64-j478m             1/1       Running     0          3m\n    aspera-1-redis-ha-sentinel-0                               1/1       Running     0          3m\n    aspera-1-redis-ha-sentinel-1                               1/1       Running     0          2m\n    aspera-1-redis-ha-sentinel-2                               1/1       Running     0          1m\n    aspera-1-redis-ha-server-0                                 1/1       Running     0          3m\n    aspera-1-redis-ha-server-1                                 1/1       Running     0          2m\n    aspera-1-redis-ha-server-2                                 1/1       Running     0          2m\n    ```\n\n\n\n","fileAbsolutePath":"/Users/davethiessen/github/cloudpak8s/src/pages/content/integration/deploy-fast-file-transfer.md"}}}}