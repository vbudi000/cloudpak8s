(window.webpackJsonp=window.webpackJsonp||[]).push([[47],{"013z":function(e,t,n){"use strict";n("KKXr"),n("pIFo");var o=n("pOBw"),a=n("q1tI"),r=n.n(a),i=n("NmYn"),s=n.n(i),l=n("OKom"),c=n("k4MR"),u=n("TSYQ"),b=n.n(u),p=n("QH2O"),d=n("qKvR"),h=function(e){var t,n=e.title,o=e.tabs,a=void 0===o?[]:o;return Object(d.b)("div",{className:b()(p.pageHeader,(t={},t[p.withTabs]=a.length,t))},Object(d.b)("div",{className:"bx--grid"},Object(d.b)("div",{className:"bx--row"},Object(d.b)("div",{className:"bx--col-lg-12"},Object(d.b)("h1",{id:"page-title",className:p.text},n)))))},m=n("pEPl"),g=n("BAC9"),f=function(e){var t=e.relativePagePath,n=e.repository,o=m.data.site.siteMetadata.repository,a=n||o,r=a.baseUrl,i=a.subDirectory,s=r+"/edit/"+a.branch+i+"/src/pages"+t;return r?Object(d.b)("div",{className:"bx--row "+g.row},Object(d.b)("div",{className:"bx--col"},Object(d.b)("a",{className:g.link,href:s},"Edit this page on GitHub"))):null},O=n("FCXl"),j=(n("Oyvg"),n("Wbzz")),y=n("I8xM");var N=function(e){var t,n;function o(){return e.apply(this,arguments)||this}return n=e,(t=o).prototype=Object.create(n.prototype),t.prototype.constructor=t,t.__proto__=n,o.prototype.render=function(){var e=this.props,t=e.tabs,n=e.slug,o=n.split("/").filter(Boolean).slice(-1)[0],a=t.map((function(e){var t,a=s()(e,{lower:!0}),r=a===o,i=new RegExp(o+"(?!-)"),l=n.replace(i,a);return Object(d.b)("li",{key:e,className:b()((t={},t[y.selectedItem]=r,t),y.listItem)},Object(d.b)(j.Link,{className:y.link,to:""+l},e))}));return Object(d.b)("div",{className:y.tabsContainer},Object(d.b)("div",{className:"bx--grid"},Object(d.b)("div",{className:"bx--row"},Object(d.b)("div",{className:"bx--col-lg-12 bx--col-no-gutter"},Object(d.b)("nav",null,Object(d.b)("ul",{className:y.list},a))))))},o}(r.a.Component),w=n("MjG9");t.a=function(e){var t=e.pageContext,n=e.children,a=e.location,r=e.Title,i=t.frontmatter,u=void 0===i?{}:i,b=t.relativePagePath,p=t.titleType,m=u.tabs,g=u.title,j=u.theme,y=u.description,v=u.keywords,k=o.data.site.pathPrefix,T=k?a.pathname.replace(k,""):a.pathname,S=m?T.split("/").filter(Boolean).slice(-1)[0]||s()(m[0],{lower:!0}):"";return Object(d.b)(c.a,{tabs:m,homepage:!1,theme:j,pageTitle:g,pageDescription:y,pageKeywords:v,titleType:p},Object(d.b)(h,{title:r?Object(d.b)(r,null):g,label:"label",tabs:m}),m&&Object(d.b)(N,{slug:T,tabs:m,currentTab:S}),Object(d.b)(w.a,{padded:!0},n,Object(d.b)(f,{relativePagePath:b})),Object(d.b)(O.a,{pageContext:t,location:a,slug:T,tabs:m,currentTab:S}),Object(d.b)(l.a,null))}},"8s7N":function(e,t,n){"use strict";n.r(t),n.d(t,"_frontmatter",(function(){return i})),n.d(t,"default",(function(){return c}));n("91GP"),n("rGqo"),n("yt8O"),n("Btvt"),n("RW0V"),n("q1tI");var o=n("7ljp"),a=n("013z");n("qKvR");function r(){return(r=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var o in n)Object.prototype.hasOwnProperty.call(n,o)&&(e[o]=n[o])}return e}).apply(this,arguments)}var i={},s={_frontmatter:i},l=a.a;function c(e){var t=e.components,n=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,["components"]);return Object(o.b)(l,r({},s,n,{components:t,mdxType:"MDXLayout"}),Object(o.b)("h2",null,"Node Management Overview"),Object(o.b)("p",null,"All pods in Kubernetes or OpenShift Container Platform run in the nodes.  Because of this, ensuring the health of your nodes is a critical Day 2 activity.  You should be able to replace a node without affecting the service.  As part of Day 2 activities, you need to be able to ensure this."),Object(o.b)("p",null,"When you look at node management, it is also important to refer back to the design of the cluster. Design and sizing your node is a Day 0 activity. By referring to this information, you will know the design principle and the design decision that was made for the cluster."),Object(o.b)("p",null,"This chapter will focus on steps that you can take to ensure the health of your nodes using the standard facility that come out of the box with OpenShift.  The steps presented here can be evolved to make use of other practices presented in the other chapters of this repository, such as performing ",Object(o.b)("a",r({parentName:"p"},{href:"../Logging"}),"logging"),", ",Object(o.b)("a",r({parentName:"p"},{href:"../Monitoring"}),"monitoring"),", and ",Object(o.b)("a",r({parentName:"p"},{href:"../EventManagement"}),"event management"),"."),Object(o.b)("h2",null,"Day 1 Platform"),Object(o.b)("p",null,"When you consider Day 2 node management, you need to have access to the Day 0 design principle and/or design decision.  These are examples of useful information to collect:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"Sizing Considerations"),". How many pods can my cluster handle?  Are there any assumptions made on the application potential loading?  What is the design decision on the number of master and worker nodes?  Is there any scale-out consideration made?"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"Environment Scenarios"),". What is the design for the number of environment types (Dev, Test, Prod) for the clusters?  Is the cluster running as a private cloud, or hosted on a cloud provider, or a hybrid?"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"Installation Method"),".  Terraform, UPI, IPI?"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"Feature"),".  What features am I going to offer my tenants?"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"CI/CD"),". Do I have a strategy for CI/CD?"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"Security"),".  What is supported and not supported by the security department?  Has the cluster design been approved by the security department?"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"Management"),".  What is the design of the management functionality of the cluster?")),Object(o.b)("h2",null,"Day 2 Platform"),Object(o.b)("p",null,"The following shows Day 2 activities on managing the node:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#master-node"}),"Manage Master node health"),Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#mnode-number"}),"Do you increase the number of master nodes?")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#mnode-requirement"}),"Master nodes platform requirement")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#mnode-resiliency"}),"Resiliency of master node")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#mnode-intend"}),"Make sure only the intended pods is running in the master nodes")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#etcd-troubleshoot"}),"Troubleshooting etcd")))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#worker-node"}),"Maintain worker node health"),Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#stop-start"}),"How to stop and start a worker node manually")))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#node-up"}),"Ensure node resiliency")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#co-health"}),"Assure cluster operator health"),Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#mco"}),"Machine Config Operator"))))),Object(o.b)("h2",null,"Day 1 Application"),Object(o.b)("p",null,"For Node Management, you need to ensure that the applications that are already installed on Day 1 are running in the nodes as designed.   You need to ",Object(o.b)("strong",{parentName:"p"},"verify that the application has been deployed in the correct node"),".  For example, you need to define the initial pod placement policy for the applications identified in Day 1. This activity will be extended in Day 2 for applications that will be introduced as part of the steady-state operation."),Object(o.b)("h2",null,"Day 2 Application"),Object(o.b)("p",null,"During Day 2 operations, new applications will be deployed to the OpenShift cluster. You need to ensure that your worker nodes will have enough capacity to support these applications.  One way is to define the project template includes the required Range Limit resources and Quota resources. ",Object(o.b)("em",{parentName:"p"},"The Project template")," is covered in ",Object(o.b)("a",r({parentName:"p"},{href:"../BuildDeploy"}),"Builds and Deploy")," chapter, Range Limit, and Quota are discussed in the ",Object(o.b)("a",r({parentName:"p"},{href:"../Capacity"}),"Capacity")," chapter.   "),Object(o.b)("p",null,"Another important aspect of Day 2 Node Management on Application is to satisfy the applicationsâ€™ workload requirement for performance, security, and manageability through these activities:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#mgmt-node"}),"Configure node for a specialized function"),Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#mgmt-security"}),"Preventing security breach by using Node Restriction")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"#daemonsets"}),"Node placement and DaemonSets"))))),Object(o.b)("h2",null,"Mapping to Personas"),Object(o.b)("table",null,Object(o.b)("thead",{parentName:"table"},Object(o.b)("tr",{parentName:"thead"},Object(o.b)("th",r({parentName:"tr"},{align:null}),"Persona"),Object(o.b)("th",r({parentName:"tr"},{align:null}),"task"))),Object(o.b)("tbody",{parentName:"table"},Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",r({parentName:"tr"},{align:null}),"SRE"),Object(o.b)("td",r({parentName:"tr"},{align:null}),"Manage master node health")),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",r({parentName:"tr"},{align:null}),"SRE"),Object(o.b)("td",r({parentName:"tr"},{align:null}),"Maintain worker node health")),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",r({parentName:"tr"},{align:null}),"SRE"),Object(o.b)("td",r({parentName:"tr"},{align:null}),"Ensure node resiliency")),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",r({parentName:"tr"},{align:null}),"SRE"),Object(o.b)("td",r({parentName:"tr"},{align:null}),"Assure cluster operator health")),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",r({parentName:"tr"},{align:null}),"SRE, DevOps Engineer"),Object(o.b)("td",r({parentName:"tr"},{align:null}),"Configure node for a specialized function")))),Object(o.b)("h2",null,"Platform tasks"),Object(o.b)("a",{name:"master-node"}),Object(o.b)("h2",null,"Manage master node health: ","[ SRE ]"),Object(o.b)("p",null,"As part of the Day 2 operation, you need to ensure the health of your master node.  In particular, there are three pods that you need to ensure to be in good running operation: ",Object(o.b)("strong",{parentName:"p"},"the etcd service provider"),", ",Object(o.b)("strong",{parentName:"p"},"the API Server"),", and ",Object(o.b)("strong",{parentName:"p"},"the Controller and Scheduler"),".  "),Object(o.b)("p",null,"Amongst these three types of pods, etcd requires the persistent volume (storage) as it can generate substantial network traffic, so one way to ensure the master node health is to ensure the health of the etcd services.  The Operators manage the API server, the controller, and the scheduler.  "),Object(o.b)("a",{name:"mnode-number"}),Object(o.b)("h3",null,"Do you increase the number of master nodes?"),Object(o.b)("p",null,"The etcdâ€™s main function is to store Kubernetesâ€™ configuration information; hence its performance is crucial to the efficient performance of your cluster."),Object(o.b)("p",null,"etcd is a distributed key-value store using ",Object(o.b)("a",r({parentName:"p"},{href:"https://raft.github.io"}),"the RAFT consensus algorithm"),". Because of the consensus algorithm, etcd must be deployed in odd numbers of pods to maintain quorum.  This normally translates to three nodes in a Production environment."),Object(o.b)("p",null,"From the operational aspects of etcd, the etcd service is considered an active-active cluster. Meaning, an etcd client can write to any of the etcd nodes and the cluster will replicate the data, and maintain consistency of the data across the instances.  When one etcd pod in one master node performs a write, it needs to synchronize the content with other etcd pods."),Object(o.b)("p",null,"Your Day 0 design document may already specify three etcd pods.  As your cluster size grow, you might think that you need to increase your etcd nodes (Master nodes) beyond 3. However, as general guidance:"),Object(o.b)("blockquote",null,Object(o.b)("p",{parentName:"blockquote"},"For performance reason, you should not have more than 3 etcd nodes (master nodes).")),Object(o.b)("p",null,"The main reason is that as etcd implements a consensus algorithm, each node needs to synchronize with other nodes.  When you have three nodes, each node needs to communicate with two other nodes.  When you have five nodes, then each node needs to talk to four other nodes, causing exponential growth in network traffic that might affect the network capacity of your cluster.  "),Object(o.b)("p",null,"When your cluster grows, if you need to scale etcd, which is unlikely, then rather than adding more etcd pods, it might be better to increase the capacity of each of the etcd pods.  You can also increase the compute (CPU/memory) power of the master node where the etcd runs."),Object(o.b)("a",{name:"mnode-requirement"}),Object(o.b)("h3",null,"Master nodes platform requirement"),Object(o.b)("p",null,"The design of the master nodes platform mostly had been done in Day 0, but the design decision might not have been spelled out in detail in the design document."),Object(o.b)("p",null,"As part of Day 2 activities, you might need to replace a bad master node, or upgrade the CPU/memory capacity of the master node.  The following is a general recommendation on the selection of the master node to host the etcd pod:"),Object(o.b)("blockquote",null,Object(o.b)("ul",{parentName:"blockquote"},Object(o.b)("li",{parentName:"ul"},"It needs storage with fast access disk."),Object(o.b)("li",{parentName:"ul"},"It needs a low latency in communicating with other etcd pods, this means fast networking."),Object(o.b)("li",{parentName:"ul"},"The etcd store should not be located on the same disk as a disk-intensive service such as the database to store logging and metric information."),Object(o.b)("li",{parentName:"ul"},"It should not be spread across data centers or, in the case of public clouds, availability zones"))),Object(o.b)("a",{name:"mnode-resiliency"}),Object(o.b)("h3",null,"Resiliency of master node"),Object(o.b)("p",null,"We have recommended three master nodes as a good number for production.  For etcd, the quorum for three master nodes is two nodes.  Let us consider the failure scenario:"),Object(o.b)("p",null,"When one master node (i.e., etcd pod) is lost, The quorum is still ok.  So the operation can still recover by replacing the lost master node."),Object(o.b)("p",null,"When two master nodes are lost, etcd does not have the quorum to write to disk.  In other words, you can not make changes, no new project or resource, no scaling up or down.  However, the currently running pods will not be terminated."),Object(o.b)("p",null,"When three (all) master nodes are lost, OpenShift stops functioning, and you need to recover the etcd data from backup."),Object(o.b)("blockquote",null,Object(o.b)("p",{parentName:"blockquote"},"To maintain high availability, restore a lost master node as soon as possible.")),Object(o.b)("a",{name:"mnode-intend"}),Object(o.b)("h3",null,"Make sure only the intended pods is running in the master nodes"),Object(o.b)("p",null,"This might sound obvious; however, we have found out customers running the monitoring server in the master node.  Even though OpenShift has put some taint on the master node to discourage this, it still happened."),Object(o.b)("p",null,"Here is an example of how to quickly check them.  The first command list all the nodes in the cluster. The second command list all the pods that are running in one of the master nodes."),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"$ oc get nodes\nNAME                                         STATUS   ROLES    AGE   VERSION\nip-10-0-139-124.us-west-2.compute.internal   Ready    master   13d   v1.16.2\nip-10-0-141-46.us-west-2.compute.internal    Ready    worker   13d   v1.16.2\nip-10-0-147-123.us-west-2.compute.internal   Ready    worker   13d   v1.16.2\nip-10-0-147-175.us-west-2.compute.internal   Ready    master   13d   v1.16.2\nip-10-0-161-65.us-west-2.compute.internal    Ready    worker   13d   v1.16.2\nip-10-0-171-121.us-west-2.compute.internal   Ready    master   13d   v1.16.2\n\n$ oc get pods --all-namespaces --field-selector status.phase=Running,spec.nodeName=ip-10-0-139-124.us-west-2.compute.internal\nNAMESPACE                                      NAME                                                                  READY   STATUS    RESTARTS   AGE\nopenshift-apiserver                            apiserver-g886t                                                       1/1     Running   0          13d\nopenshift-controller-manager                   controller-manager-xk4kw                                              1/1     Running   0          12d\nopenshift-etcd                                 etcd-member-ip-10-0-139-124.us-west-2.compute.internal                2/2     Running   0          13d\nopenshift-kube-apiserver                       kube-apiserver-ip-10-0-139-124.us-west-2.compute.internal             3/3     Running   0          12d\nopenshift-kube-controller-manager              kube-controller-manager-ip-10-0-139-124.us-west-2.compute.internal    3/3     Running   0          12d\nopenshift-kube-scheduler                       openshift-kube-scheduler-ip-10-0-139-124.us-west-2.compute.internal   1/1     Running   0          13d\nopenshift-machine-api                          machine-api-controllers-5d4cbc74bc-6gvtt                              4/4     Running   0          13d\nopenshift-monitoring                           node-exporter-m2pth                                                   2/2     Running   0          13d\nopenshift-monitoring                           thanos-querier-5db59f6b48-hzqh6                                       4/4     Running   0          13d\nopenshift-sdn                                  ovs-v9fjb                                                             1/1     Running   0          13d\nopenshift-sdn                                  sdn-controller-pslzf                                                  1/1     Running   0          13d\nopenshift-sdn                                  sdn-q5ddt                                                             1/1     Running   1          13d\n")),Object(o.b)("p",null,"From here, you can see the API-server, the controller-manager, the scheduler pods.  These are what we expect to run in the master node.  You can also see the networking component OVS and SDN.  There are also two monitoring pods, the Prometheus node-exporter and the Thanos querier running in the Openshift-monitoring namespace.  These are ok as they are the metric collector, not the actual monitoring server itself.  "),Object(o.b)("blockquote",null,Object(o.b)("p",{parentName:"blockquote"},"If you start seeing the pods running in your ",Object(o.b)("em",{parentName:"p"},"user-created project namespace"),", then you know something is not correct.")),Object(o.b)("a",{name:"etcd-troubleshoot"}),Object(o.b)("h3",null,"Troubleshooting etcd"),Object(o.b)("p",null,"The following list some of the error messages on etcd that you might be able to see in the log, with some suggestions on what could be causing it."),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),'Connection error: desc = "transport: Error while dialing dial tcp 0.0.0.0:2379: i/o timeout"; Reconnecting to {0.0.0.0:2379 0 <nil>}\n')),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Most likely this is caused by a misconfigured networking, such as the Master Node firewall or misconfigured SDN.")),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"database space exceeded or applying raft message exceeded backend quota\n")),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"The Kubernetes system might have a space quota for etcd, and one or more members of the etcd is encountering this quota.  This error normally will put the cluster into maintenance mode, and etcd only accepts key reads and deletes.")),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"dial tcp <ip>:2379: getsockopt: connection refused or dial tcp <ip>:2380: getsockopt: connection refused\n")),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"A connection to the etcd endpoint could not be established. Ensure that the etcd container is running on the host with the address shown.")),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"apply entries took too long\n")),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"If the average write duration exceeds 100 milliseconds, etcd will warn that entries are taking too long to write. This issue can have a few causes: Slow disk, CPU starvation, Slow network.  We have described a platform environment suitable to run etcd.  Prevention is preferred to correcting the error.  ")),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"snapshotting is taking more than <num> seconds to finish\n")),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Sending a snapshot took too long and exceeded the expected transfer time.  Most likely due to slow or congested network.")),Object(o.b)("a",{name:"worker-node"}),Object(o.b)("h2",null,"Maintain worker node health: ","[ SRE ]"),Object(o.b)("p",null,"User application and platform management function should be run on the Worker node. A worker node is also called a ",Object(o.b)("em",{parentName:"p"},"Compute")," node. Our recommendation is to run the management function on a special worker node.  This will be covered in the ",Object(o.b)("a",r({parentName:"p"},{href:"#mgmt-node"}),"Node for a specialized function")," section of this chapter."),Object(o.b)("blockquote",null,Object(o.b)("p",{parentName:"blockquote"},"A node is not created by Kubernetes.  Depending on how OpenShift is provisioned, a node is created externally by the platform provisioning, such as cloud providers or through virtual machines.")),Object(o.b)("p",null,"So when Kubernetes creates a node, what it really creates is an object that represents the node. Kubernetes then validates the node.  If there are no pressure conditions (see example below), then the node is eligible to run pods.  "),Object(o.b)("p",null,"Node status and other details about a node can be displayed using the following command:"),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"oc describe node <node-name>\n")),Object(o.b)("p",null,"The following shows a reduced output of the command; some lines have been removed for brevity."),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"JuliusMBP:tmp jwahidin$ oc describe node ip-10-0-161-65.us-west-2.compute.internal\nName:               ip-10-0-161-65.us-west-2.compute.internal\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.8xlarge\nCreationTimestamp:  Wed, 05 Feb 2020 18:36:27 +1100\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 19 Feb 2020 21:42:41 +1100   Wed, 05 Feb 2020 18:36:27 +1100   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 19 Feb 2020 21:42:41 +1100   Wed, 05 Feb 2020 18:36:27 +1100   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 19 Feb 2020 21:42:41 +1100   Wed, 05 Feb 2020 18:36:27 +1100   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 19 Feb 2020 21:42:41 +1100   Wed, 05 Feb 2020 18:37:07 +1100   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.161.65\n  Hostname:     ip-10-0-161-65.us-west-2.compute.internal\n  InternalDNS:  ip-10-0-161-65.us-west-2.compute.internal\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         32\n  ephemeral-storage:           125277164Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      130514256Ki\n  pods:                        250\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         31500m\n  ephemeral-storage:           115455434152\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      129899856Ki\n  pods:                        250\nProviderID:                               aws:///us-west-2c/i-07e6720b3a098deeb\nNon-terminated Pods:                      (20 in total)\n  Namespace                               Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                                               ------------  ----------  ---------------  -------------  ---\n  openshift-cluster-node-tuning-operator  tuned-rjj8m                                                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         14d\n  openshift-dns                           dns-default-22wc4                                                  110m (0%)     0 (0%)      70Mi (0%)        512Mi (0%)     14d\n  openshift-image-registry                node-ca-ld8nq                                                      10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         14d\n\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests       Limits\n  --------                    --------       ------\n  cpu                         5520m (17%)    6 (19%)\n  memory                      15556Mi (12%)  18944Mi (14%)\n  ephemeral-storage           0 (0%)         0 (0%)\n  attachable-volumes-aws-ebs  0              0\nEvents:                       <none>\n\n")),Object(o.b)("p",null,"There is some information worth mentioning from the Node Management point of view."),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"Capacity"),".  As you can see, there are three types of capacity-related listed: ",Object(o.b)("em",{parentName:"li"},"Capacity"),", ",Object(o.b)("em",{parentName:"li"},"Allocatable"),", and ",Object(o.b)("em",{parentName:"li"},"Allocated resources"),".  This will be important to see the capacity of your node/cluster.  As you can see, the capacity of the node is part of the node object.  Capacity definition includes: ",Object(o.b)("em",{parentName:"li"},"cpu"),", ",Object(o.b)("em",{parentName:"li"},"memory"),", ",Object(o.b)("em",{parentName:"li"},"ephemeral-storage"),", and ",Object(o.b)("em",{parentName:"li"},"attachable-volumes"),"."),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"Conditions"),".  You can see that the Nodes currently is not under ",Object(o.b)("em",{parentName:"li"},"Memory"),", ",Object(o.b)("em",{parentName:"li"},"PID"),", and ",Object(o.b)("em",{parentName:"li"},"Disk")," pressure.  So the Status is ",Object(o.b)("em",{parentName:"li"},"Ready"),". Note PID represents the number of processes.")),Object(o.b)("p",null,"Heartbeats, sent by Kubernetes nodes to the node controller running in the master node, help determine the availability of a node."),Object(o.b)("p",null,"The Kubernetes scheduler ensures that there are enough resources for all the pods on a node. It checks that the sum of the requests of containers on the node is no greater than the node capacity."),Object(o.b)("p",null,"When you want to see the capacity of all your nodes in a cluster then you can run ",Object(o.b)("inlineCode",{parentName:"p"},"oc adm top node"),"."),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"$ oc adm top node\nNAME                                         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nip-10-0-139-124.us-west-2.compute.internal   506m         14%    2643Mi          17%       \nip-10-0-141-46.us-west-2.compute.internal    408m         1%     5636Mi          4%        \nip-10-0-147-123.us-west-2.compute.internal   374m         1%     6204Mi          4%        \nip-10-0-147-175.us-west-2.compute.internal   559m         15%    3270Mi          21%       \nip-10-0-161-65.us-west-2.compute.internal    183m         0%     4731Mi          3%        \nip-10-0-171-121.us-west-2.compute.internal   623m         17%    3189Mi          21%       \n")),Object(o.b)("a",{name:"stop-start"}),Object(o.b)("h3",null,"How to stop and start a worker node manually"),Object(o.b)("p",null,"As part of Day 2 activities, you might want to do maintenance on a worker node.  The following describes the process of restarting a worker node:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Disable Scheduling on the node using ",Object(o.b)("inlineCode",{parentName:"li"},"oc adm cordon <node-name>"),".")),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"$ oc get node ip-10-0-147-123.us-west-2.compute.internal\nNAME                                         STATUS   ROLES    AGE   VERSION\nip-10-0-147-123.us-west-2.compute.internal   Ready    worker   14d   v1.16.2\n\n$ oc adm cordon ip-10-0-147-123.us-west-2.compute.internal\nnode/ip-10-0-147-123.us-west-2.compute.internal cordoned\n\n$ oc get node ip-10-0-147-123.us-west-2.compute.internal\nNAME                                         STATUS                     ROLES    AGE   VERSION\nip-10-0-147-123.us-west-2.compute.internal   Ready,SchedulingDisabled   worker   14d   v1.16.2\n")),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Perform a dry-run on evacuating/draining the pod from the node.")),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"$ oc adm drain ip-10-0-147-123.us-west-2.compute.internal --dry-run=true  \nnode/ip-10-0-147-123.us-west-2.compute.internal already cordoned (dry run)\nnode/ip-10-0-147-123.us-west-2.compute.internal drained (dry run)\n")),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Evacuate the current running pod from the node.  If your pod is managed by a ",Object(o.b)("em",{parentName:"li"},"DaemonSet"),", or if your pod is using a local storage such as ",Object(o.b)("em",{parentName:"li"},"EmptyDir")," Storage, the drain will display some message, you then need to specify additional command-line flags to the drain operation.",Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},"If you are sure that you can discard the local data or evacuate the daemon set managed pod, then use the ",Object(o.b)("inlineCode",{parentName:"li"},"--ignore-daemonsets=true")," and ",Object(o.b)("inlineCode",{parentName:"li"},"--delete-local-data=true")," to override the conditions."),Object(o.b)("li",{parentName:"ul"},"You can also use the ",Object(o.b)("inlineCode",{parentName:"li"},"--force=true"),"."),Object(o.b)("li",{parentName:"ul"},"If your node is providing services to the cluster, such as router or master node, there are additional steps that you need to take. These steps is mentioned in the OpenShift documentation on ",Object(o.b)("a",r({parentName:"li"},{href:"https://docs.openshift.com/container-platform/4.3/nodes/nodes/nodes-nodes-viewing.html"}),"Nodes")),Object(o.b)("li",{parentName:"ul"},"You might need to run the drain operation a few times until there is no error message."),Object(o.b)("li",{parentName:"ul"},"Because of this possible iteration, OpenShift 4.3 introduces the ",Object(o.b)("a",r({parentName:"li"},{href:"#mco"}),"Machine Config Operator"),". Other tools to evacuate pods from nodes are available, and an example is ",Object(o.b)("a",r({parentName:"li"},{href:"https://github.com/planetlabs/draino"}),"Draino"),".")))),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),'    $ oc adm drain ip-10-0-147-123.us-west-2.compute.internal\n    node/ip-10-0-147-123.us-west-2.compute.internal already cordoned\n    error: unable to drain node "ip-10-0-147-123.us-west-2.compute.internal", aborting command...\n\n    There are pending nodes to be drained:\n     ip-10-0-147-123.us-west-2.compute.internal\n    cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): openshift-cluster-node-tuning-operator/tuned-67c6c, openshift-dns/dns-default-xltzj, openshift-image-registry/node-ca-lw72z, openshift-machine-config-operator/machine-config-daemon-fkskx, openshift-monitoring/node-exporter-j6pz8, openshift-multus/multus-ln5zt, openshift-sdn/ovs-mqclb, openshift-sdn/sdn-qz497, openshift-storage/csi-cephfsplugin-dzzhw, openshift-storage/csi-rbdplugin-c8q62\n    cannot delete Pods with local storage (use --delete-local-data to override): openshift-monitoring/alertmanager-main-1, openshift-monitoring/prometheus-k8s-0, openshift-storage/csi-cephfsplugin-provisioner-5cdcfcc86b-x6bnb, openshift-storage/csi-rbdplugin-provisioner-8fdc8f955-pxmw5, openshift-storage/noobaa-core-0, openshift-storage/rook-ceph-mgr-a-78c4576db8-cjxxh, openshift-storage/rook-ceph-osd-1-bb9cdcfd6-nrdwc\n')),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Once the drain is completed, you can shutdown the kubelet and the docker(or containerd) before you perform the maintenance action required.  Depending on the infrastructure running your node, the kubelet and the docker shutdown may be different.  For RHEL you can use the following command.",Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},"To shutdown the kubelet.")))),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"    sudo systemctl stop kubelet\n")),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"To shutdown docker")),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"    sudo systemctl stop docker\n")),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"To bring up docker")),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"    sudo systemctl start kubelet\n")),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"To bring up the kubelet")),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"    sudo systemctl start kubelet\n    sudo systemctl status kubelet\n")),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"To view the log of the kubelet on the operating system")),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"    sudo journalctl -e -u kubelet\n")),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Enable Scheduling on the node using ",Object(o.b)("inlineCode",{parentName:"li"},"oc adm uncordon <node-name>"),".")),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"$ oc adm uncordon ip-10-0-147-123.us-west-2.compute.internal\nnode/ip-10-0-147-123.us-west-2.compute.internal uncordoned\n$ oc get node ip-10-0-147-123.us-west-2.compute.internal\nNAME                                         STATUS   ROLES    AGE   VERSION\nip-10-0-147-123.us-west-2.compute.internal   Ready    worker   14m   v1.16.2\n")),Object(o.b)("h4",null,"Use of cordon and uncordon during debugging"),Object(o.b)("p",null,"Earlier we have seen how do you use the cordon and uncordon as part of the restart of a Node.  We can use the cordon and uncordon as a generic approach to isolate a node during troubleshooting."),Object(o.b)("blockquote",null,Object(o.b)("p",{parentName:"blockquote"},"During troubleshooting operation on a Node, you can use the cordon to stop the node from receiving any more pods")),Object(o.b)("a",{name:"node-up"}),Object(o.b)("h2",null,"Ensure node resiliency: ","[ SRE ]"),Object(o.b)("p",null,"You have your OpenShift cluster running. Your management stack is in place. You have done the backup and prove that the backup is good by doing a restore. What is next?  How do you determine that your cluster is resilient as an OpenShift cluster should be?  Well, like backup you test it by doing a restore, with node resiliency you check it by bringing down the node down.  This action is in line with the foundation of Chaos Engineering.  "),Object(o.b)("p",null,"In ",Object(o.b)("a",r({parentName:"p"},{href:"http://principlesofchaos.org/?lang=ENcontent"}),"the Principle of Chaos"),", it defines Chaos Engineering as follow:"),Object(o.b)("blockquote",null,Object(o.b)("p",{parentName:"blockquote"},"Chaos Engineering is the discipline of experimenting on a system in order to build confidence in the systemâ€™s capability to withstand turbulent conditions in production.")),Object(o.b)("p",null,"We recommend the following approach:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Become familiar with the cluster and the steps required to simulate the test.  To do this, you might start with the Development or the Test Environment."),Object(o.b)("li",{parentName:"ul"},"Both the Master and Worker nodes need to be tested; at the beginning, they should be tested separately."),Object(o.b)("li",{parentName:"ul"},"Once you are familiar with the process on the Test environment, then you can do it in the pre-production or even the production environment.  Make sure you plan it first.  "),Object(o.b)("li",{parentName:"ul"},"As part of the planning, you can plan to start the production test during the cluster least busy hour; when you bring down nodes, the loads need to be transferred to other operating nodes.  Based on your capacity information, can your cluster still handle the production loads without the nodes that you are bringing down?  If not, then you have a gap; refer to ",Object(o.b)("a",r({parentName:"li"},{href:"../Capacity"}),"the Capacity")," chapter of this repository for more information on OpenShift capacity."),Object(o.b)("li",{parentName:"ul"},"Another aspect of the plan is during the initial test run on production, and you might want to take the ",Object(o.b)("em",{parentName:"li"},"error budget")," into consideration, i.e., do it when you still have some error budget."),Object(o.b)("li",{parentName:"ul"},"Your goal is to be able to confidently bring down any node unplanned.  "),Object(o.b)("li",{parentName:"ul"},"Once you achieve this goal, set as a standard process to apply the Kubernetes principle of ",Object(o.b)("strong",{parentName:"li"},"Disposable Infrastructure"),"; that is, all your nodes should age no more than a set period (for example, seven days).")),Object(o.b)("a",{name:"co-health"}),Object(o.b)("h2",null,"Assure cluster operator health: ","[ SRE ]"),Object(o.b)("p",null,"In OpenShift 4.x, the master and worker nodes, are managed by operators, amongst them are ",Object(o.b)("inlineCode",{parentName:"p"},"kube-apiserver"),", ",Object(o.b)("inlineCode",{parentName:"p"},"kube-controller-manager")," and ",Object(o.b)("inlineCode",{parentName:"p"},"kube-scheduler")," operators.  To ensure the health of the nodes then you have to ensure that you monitor these operators.  OpenShift 4.x has the Cluster Operator, ",Object(o.b)("inlineCode",{parentName:"p"},"co"),", CRD resources."),Object(o.b)("p",null,"To quickly check the status of your Cluster Operator Status, you can run the following command."),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"$ oc get co\nNAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE\nauthentication                             4.3.0     True        False         False      17d\ncloud-credential                           4.3.0     True        False         False      17d\ncluster-autoscaler                         4.3.0     True        False         False      17d\nconsole                                    4.3.0     True        False         False      17d\ndns                                        4.3.0     True        False         False      17d\nimage-registry                             4.3.0     True        False         False      16d\ningress                                    4.3.0     True        False         False      16d\ninsights                                   4.3.0     True        False         False      17d\nkube-apiserver                             4.3.0     True        False         False      17d\nkube-controller-manager                    4.3.0     True        False         False      17d\nkube-scheduler                             4.3.0     True        False         False      17d\nmachine-api                                4.3.0     True        False         False      17d\nmachine-config                             4.3.0     True        False         False      17d\nmarketplace                                4.3.0     True        False         False      17d\nmonitoring                                 4.3.0     True        False         False      2d1h\nnetwork                                    4.3.0     True        False         False      17d\nnode-tuning                                4.3.0     True        False         False      17d\nopenshift-apiserver                        4.3.0     True        False         False      17d\nopenshift-controller-manager               4.3.0     True        False         False      17d\nopenshift-samples                          4.3.0     True        False         False      17d\noperator-lifecycle-manager                 4.3.0     True        False         False      17d\noperator-lifecycle-manager-catalog         4.3.0     True        False         False      17d\noperator-lifecycle-manager-packageserver   4.3.0     True        False         False      16d\nservice-ca                                 4.3.0     True        False         False      17d\nservice-catalog-apiserver                  4.3.0     True        False         False      17d\nservice-catalog-controller-manager         4.3.0     True        False         False      17d\nstorage                                    4.3.0     True        False         False      17d\n\n")),Object(o.b)("p",null,"If you want to learn more about the specifics of the operator, then you can run the ",Object(o.b)("inlineCode",{parentName:"p"},"oc describe co/<operator-name>"),".\nFor example, the following command describes the ",Object(o.b)("em",{parentName:"p"},"openshift-apiserver")," operator, some of the output had been cut for brevity, but notice the ",Object(o.b)("inlineCode",{parentName:"p"},"Status")," section, and it listed ",Object(o.b)("inlineCode",{parentName:"p"},"Conditions")," as well as ",Object(o.b)("inlineCode",{parentName:"p"},"Related Objects."),"  From this Related Objects, you can learn about all the Related Objects to the apiserver."),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),"$ oc describe co/openshift-apiserver\nName:         openshift-apiserver\nNamespace:    \nLabels:       <none>\nAnnotations:  <none>\nAPI Version:  config.openshift.io/v1\nKind:         ClusterOperator\nMetadata:\n  Creation Timestamp:  2020-02-04T20:12:14Z\n  Generation:          1\n  Resource Version:    11975\n  Self Link:           /apis/config.openshift.io/v1/clusteroperators/openshift-apiserver\n  UID:                 7774b841-583b-4b82-9e88-6edd44dca3d2\nSpec:\nStatus:\n  Conditions:\n    Last Transition Time:  2020-02-04T20:16:34Z\n    Reason:                AsExpected\n    Status:                False\n    Type:                  Degraded\n    Last Transition Time:  2020-02-04T20:18:42Z\n    Reason:                AsExpected\n    Status:                False\n    Type:                  Progressing\n    Last Transition Time:  2020-02-04T20:18:29Z\n    Reason:                AsExpected\n    Status:                True\n    Type:                  Available\n    Last Transition Time:  2020-02-04T20:12:15Z\n    Reason:                AsExpected\n    Status:                True\n    Type:                  Upgradeable\n  Extension:               <nil>\n  Related Objects:\n    Group:     operator.openshift.io\n    Name:      cluster\n    Resource:  openshiftapiservers\n    Group:     \n    Name:      openshift-config\n    Resource:  namespaces\n    Group:     \n    Name:      openshift-config-managed\n    Resource:  namespaces\n    Group:     \n    Name:      openshift-apiserver-operator\n    Resource:  namespaces\n    Group:     \n    Name:      openshift-apiserver\n    Resource:  namespaces\n    Group:     apiregistration.k8s.io\n    Name:      v1.apps.openshift.io\n    Resource:  apiservices\n    . . .\n    Group:     apiregistration.k8s.io\n    Name:      v1.user.openshift.io\n    Resource:  apiservices\n  Versions:\n    Name:     operator\n    Version:  4.3.0\n    Name:     openshift-apiserver\n    Version:  \nEvents:       <none>\n")),Object(o.b)("a",{name:"mco"}),Object(o.b)("h3",null,"Machine Config Operator"),Object(o.b)("p",null,"OpenShift 4.x introduces lots of Operator.  One of them is the Machine Config Operator or MCO for short.  One of the components of the Machine Config Operator is the Machine Config Daemon."),Object(o.b)("p",null,"The Machine Config Daemon runs on every node to manage the configuration changes and updates on the nodes. Machine Config Daemon also provides a set of metrics. Some of the metrics are:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("em",{parentName:"li"},"ssh_accessed"),". Shows the number of successful SSH authentications into the node."),Object(o.b)("li",{parentName:"ul"},Object(o.b)("em",{parentName:"li"},"drain_time"),". Shows how much time the drain took.")),Object(o.b)("p",null,"These metrics can be viewed through the Prometheus Cluster Monitoring stack. The chapter on ",Object(o.b)("a",r({parentName:"p"},{href:"../Monitoring"}),"Monitoring")," will describe the Prometheus Monitoring in more detail."),Object(o.b)("p",null,"More information on MCO can be found in the ",Object(o.b)("a",r({parentName:"p"},{href:"https://github.com/openshift/machine-config-operator"}),"Machine Config Operator")," git pages."),Object(o.b)("h2",null,"Application tasks"),Object(o.b)("a",{name:"mgmt-node"}),Object(o.b)("h2",null,"Configure node for a specialized function: ","[ SRE, DevOps Engineer ]"),Object(o.b)("p",null,"Kubernetes employs a master/worker node architecture. Kubernetes master servers maintain the information about the server cluster, and worker nodes run the actual application workloads.  In practice, you might want to introduce a specialized function node.  "),Object(o.b)("p",null,"A good example of nodes with specialized function is a set of nodes to run the cluster management application. The main reason is to separate the application load and management load.  "),Object(o.b)("blockquote",null,Object(o.b)("p",{parentName:"blockquote"},"If the nodes are not separated, then there is a potential disruption on the application because of the load from the management pods.  You do not want to have the application pods being restricted from scaling up because all the resources are taken by somebody running an analysis on the logs collected for the past three months, for example.")),Object(o.b)("p",null,"There are several methods to ensure the placement of the management function into a specific node.  "),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"using ",Object(o.b)("a",r({parentName:"li"},{href:"https://docs.openshift.com/container-platform/4.3/nodes/pods/nodes-pods-node-selectors.html"}),"labels")),Object(o.b)("li",{parentName:"ul"},"using ",Object(o.b)("a",r({parentName:"li"},{href:"https://docs.openshift.com/container-platform/4.3/nodes/scheduling/nodes-scheduler-node-affinity.html"}),"node affinity")),Object(o.b)("li",{parentName:"ul"},"using ",Object(o.b)("a",r({parentName:"li"},{href:"https://docs.openshift.com/container-platform/4.3/nodes/scheduling/nodes-scheduler-taints-tolerations.html"}),"node taint"))),Object(o.b)("p",null,"These pod placement methods are covered in more detail in the ",Object(o.b)("a",r({parentName:"p"},{href:"https://apps.na.collabserv.com/blogs/7be81ac0-37f8-466b-a858-2525e0b798cf/entry/OCP_day_2_operations_on_Node_Selection?lang=en_us"}),"OCP day-2 Operations: on node selection")," blog."),Object(o.b)("a",{name:"mgmt-security"}),Object(o.b)("h3",null,"Preventing security breach by using Node Restriction"),Object(o.b)("p",null,"Using a ",Object(o.b)("em",{parentName:"p"},"nodeSelector")," is useful as it allows you to direct the pod to only run on a selected node.  For security reasons, it is recommended to use a label that can not be modified by the kubelet process that runs on the node."),Object(o.b)("p",null,"Kubelet is an application that runs on the host operating system that provides the node functionality, so Kubelet is not running in a pod.  As a kubelet is an application running on the operating system, it is possible that it can be infected.  Theoretically, it is possible for an infected kubelet to change the label of the node that it is running.  By changing the node label, then it is possible that the kubelet will direct the scheduler to schedule workloads on the infected node.  "),Object(o.b)("p",null,"To address this, Kubernetes introduced the concept of Node restriction through the ",Object(o.b)("inlineCode",{parentName:"p"},"NodeRestriction admission plugin"),".  The plugin prevents kubelets from setting or modifying labels with a node-restriction ",Object(o.b)("inlineCode",{parentName:"p"},"kubernetes.io/")," prefix.  "),Object(o.b)("blockquote",null,Object(o.b)("p",{parentName:"blockquote"},"Ensure to enable the NodeRestriction admission plugin")),Object(o.b)("p",null,"Even though you are not using a NodeSelector, you should consider enabling the NodeRestriction admission plugin to ensure your node security."),Object(o.b)("p",null,"More information on Node restriction can be found in the Kubernetes site on ",Object(o.b)("a",r({parentName:"p"},{href:"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers"}),"Using Admission Controllers"),"."),Object(o.b)("a",{name:"daemonsets"}),Object(o.b)("h3",null,"Node placement and DaemonSets"),Object(o.b)("p",null,"You use a DaemonSet to ensure that all (or some) Nodes run the pod mentioned in the DaemonSet.  This is useful for implementing common functionality across the whole cluster.  Examples include Cluster storage, such as ",Object(o.b)("em",{parentName:"p"},"ceph"),", Log collection, such as ",Object(o.b)("em",{parentName:"p"},"fluentd"),", and Monitoring collectors, such as the ",Object(o.b)("em",{parentName:"p"},"Prometheus Node Exporter"),".  Using DaemonSet helps you ensuring that monitoring, backup, logging are done across all your nodes."),Object(o.b)("p",null,Object(o.b)("a",r({parentName:"p"},{href:"https://docs.openshift.com/container-platform/4.3/nodes/jobs/nodes-pods-daemonsets.html"}),"The OpenShift Documentation on DaemonSets")," warns on the potential conflict between daemon sets and the default Project Node Selector.  The effect will be in frequent pod recreates that put a heavy load on the cluster."),Object(o.b)("p",null,"In the case of the default Project Node Selector, you can disable them by doing the following."),Object(o.b)("pre",null,Object(o.b)("code",r({parentName:"pre"},{}),'# to change the default project setting\noc patch namespace myproject -p \'{"metadata": {"annotations": {"openshift.io/node-selector": ""}}}\'\n\n# if you do not change the default project setting. Use the --node-selector as you create a new project.\noc adm new-project --node-selector="".\n')),Object(o.b)("p",null,"Daemonsets should ignore the node-placement methods mentioned in this chapter.   "),Object(o.b)("blockquote",null,Object(o.b)("p",{parentName:"blockquote"},"However, as part of your Day 2 activities, should you observe some irregularity in frequent pod recreates as you configure your pod placement policy, then you may want to check the daemon sets.  There is a possibility of configuration conflict between DaemonSets and the Node Placement policy.")),Object(o.b)("h2",null,"Implementing Node Management"),Object(o.b)("h2",null,"Kubernetes"),Object(o.b)("p",null,"Kubernetes site provides more information on ",Object(o.b)("a",r({parentName:"p"},{href:"https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"}),"Managing Computer Resources on Containers")),Object(o.b)("h2",null,"OpenShift"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"OpenShift has sections on its documentation discussing ",Object(o.b)("a",r({parentName:"li"},{href:"https://docs.openshift.com/container-platform/4.3/nodes/nodes/nodes-nodes-viewing.html"}),"Nodes")," and  ",Object(o.b)("a",r({parentName:"li"},{href:"https://docs.openshift.com/container-platform/4.3/nodes/scheduling/nodes-scheduler-about.html"}),"Controlling pod placement using the scheduler")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"https://github.com/openshift/machine-config-operator"}),"The Machine Config Operator"))),Object(o.b)("h2",null,"On IBM Cloud"),Object(o.b)("p",null,"TBD"),Object(o.b)("h2",null,"With IBM Cloud Pak for MCM"),Object(o.b)("p",null,"MCM extends the discussion presented here for multi-cluster environments, as well as creating and enforcing place rule as well as Governance rule.  More information can be found on ",Object(o.b)("a",r({parentName:"p"},{href:"https://www.ibm.com/support/knowledgecenter/SSFC4F_1.2.0/kc_welcome_cloud_pak.html"}),"the IBM Cloudpak for MCM documentation"),"."),Object(o.b)("h2",null,"Others"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",r({parentName:"li"},{href:"https://github.com/planetlabs/draino"}),"Draino")," Automatically cordon and drain Kubernetes nodes based on node conditions.")),Object(o.b)("h2",null,"Other consideration"),Object(o.b)("p",null,"n/a"))}c.isMDXComponent=!0},pEPl:function(e){e.exports=JSON.parse('{"data":{"site":{"siteMetadata":{"repository":{"baseUrl":"https://github.com/ibm-cloud-architecture/cloudpak8s","subDirectory":"/","branch":"master"}}}}}')},pOBw:function(e){e.exports=JSON.parse('{"data":{"site":{"pathPrefix":""}}}')}}]);
//# sourceMappingURL=component---src-pages-day-2-node-index-mdx-601844cc2a29657c01da.js.map