{"componentChunkName":"component---src-pages-mcm-cp-4-mcm-nom-day-2-asm-day-2-from-event-index-mdx","path":"/mcm/cp4mcm_nom_day2/asm_day2_fromEvent/","result":{"pageContext":{"frontmatter":{"title":"MCM - ASM - Day2 - Creating Topology from Events","description":"Day 2 articles on Operating Netcool Ops Manager","keywords":"day2, asm, nom, netcool_ops_manager, event"},"relativePagePath":"/mcm/cp4mcm_nom_day2/asm_day2_fromEvent/index.mdx","titleType":"page","MdxNode":{"id":"429a6d7d-1249-5a24-a371-9316661e6537","children":[],"parent":"680673ff-3e9f-5964-9110-f53c5a93a7d4","internal":{"content":"---\ntitle: MCM - ASM - Day2 - Creating Topology from Events\ndescription: Day 2 articles on Operating Netcool Ops Manager\nkeywords: 'day2, asm, nom, netcool_ops_manager, event'\n---\n\n[Back to NOM Day2](/mcm/cp4mcm_nom_day2)\n\n\n## Creating Topology from Alerts\n\n*I do not have an inventory system, but I want to build the topology of my environment.  How do you do it using ASM?* That is a common question that comes up in a few of my past customer engagement.  \n\nThis article will provide an approach to building a topology without an Inventory application.\n\n<InlineNotification>\n\nThere are solutions such as the [IBM Tivoli Network Manager](https://www.ibm.com/support/knowledgecenter/en/SSSHRK_4.2.0/overview/concept/ovr_product_overview_infocenter.html) that can discover your IP Network topology by querying the network environment using SNMP.  \n\n</InlineNotification>\n\nThis article assumes that you have some basic familiarity with building topology using ASM.\nIf you do not have this familiarity, you might want to read [a tutorial on topology modeling using ASM](https://medium.com/ibm-garage/topology-modelling-using-agile-service-manager-a-tutorial-2e521040ea64)\nand [Building a topology using Agile Service Manager's REST Interface](https://medium.com/ibm-garage/topology-modelling-using-agile-service-managers-rest-interface-7de14a85e333).   \n\nTo build a topology from scratch, you need to come up with some way to model it.  We can use our knowledge of the environment, for example.  Can we derive the topology information from specific knowledge on how the component of the ecosystem interacts?  Can we use a specific naming convention?  In this article, we will start with the output of _a remote ping_.  To make the topology reproducible by you, we are going to use an alerts simulator.\n\n## The model\nA router typically has a feature called remote ping.  You can set up for the router to ping other network devices for availability. You can then either query the router or set up the router to send the results to an external monitoring server. Alerts normally have a field call *Node*.  It represents the object that sends the alerts.  It can be a hostname or IP address of a Server or Management Component of a Network Devices.\n\nLet us use a fictional use case.  You just joined **AFictionalCompany** IT department and was asked your manager to create a topology of the company's main routers and servers.\nAFictionalCompany has two main offices, one in Singapore and another one in Sydney.  The company also has branch offices in Europe and North America.  An Application Server and a database server are running at every office.\n\nAs a member of the IT department, you have access to the Alerts. \nYou noticed that the networking team has set up the *remote ping* from the company's two main routers in Singapore and Sydney to all other routers in their branch offices.  The result of the remote ping is forwarded to your Alert Manager as alerts. You also noticed that each Application Servers and the Database Servers send Heartbeat Availability Alerts as well.  You also found out that the network team and the infrastructure team has labeled the router and the servers with location information.\n\nTo summarize, this is what available:\n-    The remote ping results from 2 main routers in Sydney and Singapore.\n-    Heartbeat Availability alerts from the Application Servers and Database Servers.\n-    Location information in the alerts.\n\n\n## The simulated alerts\nOMNIbus, the Event Management component of NOI, has an alert generator called the Simnet probe.  An OMNIbus probe normally collects from a network, or network management equipment.  A simnet probe, simulate the alerts. It generates simulated alerts based on _a definition file_.  The alerts can then be parsed and transformed using _a probe rule file_.\n\nWe will setup the simnet probe to simulate random link failure and random server failure.\nThe random link failure simulates a failed remote ping, and the random server failure simulates the failed heartbeat.\nThe configuration of the probe, the probe rule file, and the Netcool/Impact policies all can be found from the [GitHub page for this article](https://github.com/jwahidin/topology_modeling/tree/master/Cloudpak8sAlert).  Reading or Working with these configuration files requires certain knowledge in the Netcool Operation Insight.\n\nTo simulate the alerts, we have created all the nodes in the simnet probe definition file. Here is a sample of the Alerts using the out of the box Simnet rules file.\n\n![Simulated Alerts](./images/simulated_alerts.png \"Simulated Alerts\")\n\nNow comes the modeling part.  One way to approach the model building is to extract the alerts and then apply a series of business rules and then push the result to ASM.  Another approach is to _enrich_ the alerts with information that can help our model building before we extract them.  We will be using the second approach.  We enrich the alerts using a probe rule file.\nThe rule file inserts into the alerts the two endpoints involved in the remote ping. \n\nUsing the Server Alerts location information, we create a logic connecting the Server to the router in the same location. For example, we can connect an Alert from an AppServer called `SingaporeAppSrv` or from a DataBase server named `SingaporeDb` to the router responsible for the `Singapore` Office.\n\nHere are the Alerts after being enriched using the new rules file. We have added some tags (not shown), to make the processing of it easier.  Here is the list of alerts after being enriched.\n\n![Enriched Simulated Alerts](./images/simulated_alerts_enrich.png \"Enriched Simulated Alerts\")\n\nThe alerts now have both the resources and the link connecting the resources.  Now we have some data that can be used to create the topology model.  We have a few options for building the topology using this information.  One way is to parse the alerts and call the ASM REST API to create the topology.  Another way is to build a framework that we can reuse.  We populate two tables, one for `resources` and another one for the `relationship.`  By creating the two tables, we can enrich the topology with other information before we build it.  It also provides us with a checkpoint to ensure the extraction logic produces the desired structure. Since we already have Omnibus as the Alerts database, we build the resource and relationship using Omnibus.\n\nWe will be using the following process flow:\n\n![topology creation flow](./images/topology-creation-flow.png \"topology creation flow\")\n\n\n## Building the tables\n\nHere is the logic to build the resource and relationship table\n\n```\nAt Every Period, \n-    scan the new \"link \"simulated alerts inserted into the system since the last scan, and populate the resource and relationship tables if it does not already contain the resource and the relationship.\n-    Scan the new \"server\" simulated alerts inserted into the system since the last scan, and populate the resource and relationship tables if it does not already contain the resource and the relationship.\n```\n\nThe logic has been implemented using Netcool/Impact using `OmnibusEventReader`.\n\n```\n// insertResAndEdgeFromEvent\n// To be called from the OMNIbus Event Reader and hence is expecting an \n// EventContainer to be populated already.\n// Expected to be call when ProcessReq = 101 or 102 and Class is Simnet 3300\n\nfunction createOrUpdateResource(Resource,ResType){\n  key = globalApp + Resource;\n  Res = GetByKey(dtRes, key, 1);\n   \n  if (length(Res) > 0){\n    // Maintaining the update time so state rows can be removed.\n    sql=\"Update custom.a_res set lastUpdate = getdate() where Key = '\" + key + \"'\";\n    Log(\"SQL: \" + sql );\n    DirectSQL('defaultobjectserver', sql, false);\n  } else {\n    nRes = NewObject();\n    nRes.Key = key;\n    nRes.App = globalApp;\n    nRes.UniqueId = Resource;\n    nRes.entityTypes = ResType;\n    nRes.insertTime = GetDate();\n    nRes.lastUpdate = GetDate();\n    nRes.ttl = globalTTL;\n    AddDataItem(dtRes,nRes);\n    Log(\"New Resource Entry: \" + key);\n  }\n} // function\n\nfunction createOrUpdateEdge(From,Rel,To){\n  key = globalApp + From + Rel + To;\n  Edge = GetByKey(dtEdge, key, 1);\n \n  if (length(Edge) > 0){\n    // Maintaining the update time so state rows can be removed.\n    sql=\"Update custom.a_edge set lastUpdate = getdate() where Key = '\" + key +\"'\";\n    Log(\"SQL: \" + sql );\n    DirectSQL('defaultobjectserver', sql, false);\n  } else {\n    nEdge = NewObject();\n    nEdge.Key = key;\n    nEdge.App = globalApp;\n    nEdge.FromRes = From;\n    nEdge.Relationship = Rel;\n    nEdge.ToRes = To;\n    nEdge.insertTime = GetDate();\n    nEdge.lastUpdate = GetDate();\n    nEdge.ttl = globalTTL;\n    AddDataItem(dtEdge,nEdge);\n    Log(\"New Edge Entry: \" + key);\n  }\n} // function\n\nLog(\"vvvvvvvvvvvvvvvvvvvvvvvvvvvvvv Policy started vvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\");\n\n// Called by OmnibusEventReader.\ndtRes = \"asm_Res\";\ndtEdge = \"asm_Edge\";\nglobalTTL = 3600;\nglobalApp = \"Event1\";\n\neNode1 = EventContainer.Node;\neNode2 = EventContainer.NodeAlias;\n\nif (EventContainer.ProcessReq = 101) {\n  createOrUpdateResource(eNode1,\"router\");\n  createOrUpdateResource(eNode2,\"router\"); \n  createOrUpdateEdge(eNode1,\"routesVia\",eNode2);\n} elseif (EventContainer.ProcessReq = 102) {\n  createOrUpdateResource(eNode1,\"server\");\n  createOrUpdateResource(eNode2,\"router\"); \n  createOrUpdateEdge(eNode1,\"connectedTo\",eNode2);\n} else {\n  // Unknown, do nothing.\n}\n\nLog(\"^^^^^^^^^^^^^^^^^^^^^ Policy Completed ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\");\n\n```\nAfter processing, the content of the resource and relationship table is shown below:\n\nResource Table:\n![resource table](./images/resource_table.png \"resource table\")\n\nRelationship Table:\n![relationship table](./images/relationship_table.png \"relationship table\")\n\nWith the two tables created, we can use the same script as written in [the Medium blog to build the topology](https://medium.com/ibm-garage/topology-modelling-using-agile-service-managers-rest-interface-7de14a85e333).  The logic is something like this:\n\n```\nAt Every Period\n    Check if there is at least one updated row in the resource or relationship tables.\n    If there is, then call the script to create the topology from the resource and relationship table.\n```\n\nThis is the code implemented in Netcool/Impact:\n\n```\n// runBulkLoadOnNew\n// To be called periodically by policy activator.\n// lastUpdate is for removal - if required, not currently implemented.\n// \n\ndtRes = \"asm_Res\";\ndtEdge = \"asm_Edge\";\nglobalApp = \"Event1\";\n\n// The insertTime was done less than 60 seconds ago.\n\nfilter = \"((getdate()-insertTime) < 60)\";\n\nrecentRes = GetByFilter(dtRes,filter,false);\nnumRes = length(recentRes);\nrecentEdge = GetByFilter(dtEdge,filter,false);\nnumEdge = length(recentEdge);\nLog(\"Number of Changed Resource: \" + numRes +\", Number of Changed Edge: \" + numEdge);\n\nIf ((numRes>0)or(numEdge>0)){\n   Log(\"Executing Bulk Load at: \"  + GetDate());\n   asmRest.bulkLoad(dtRes,dtEdge,globalApp);\n}\n```\n\nThe `asmRest` Function being called is as follow:\n\n```\n// asmRest\n// Expected to be called by another policy with the dataTypes and App being passed as parameters.\n\nfunction bulkLoad(dtRes,dtEdge,sApp) {\n\n  hasError=\"no\";\n  Handle java.lang.Exception { \n    Log(\"Exception: \" + ErrorMessage);\n    hasError=\"yes\";\n  } \n  \n  Log(\"vvvvvvvvvvvvvvvvvvvvvvvvvvvvvv Policy started vvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\");\n  app_name = sApp;\n  Log(2,\"Parameter received: \" + app_name);\n  paramExist = false;\n  if (app_name != NULL) {\n      if (length(app_name) > 0) {\n          paramExist = true;\n      }\n  }\n  if (!paramExist) {\n      Log(\"Parameter not specified.  Policy will exit!\");\n      Exit();\n  }\n  \n  // Initialize the values of all the parameters and then start the Bulk Job it has not been started.\n  asmFunctions.asm_initalize(asm_protocol,asm_host,asm_port,asm_path,HeadersToSend,HttpProperties);\n  asmFunctions.asm_startBulkJob(asm_protocol,asm_host,asm_port,asm_path,HeadersToSend,HttpProperties,asm_JobId,retError);\n  \n  if ((retError == 'yes') or (hasError == 'yes')){\n     Log(\"Connecting to ASM has failed\");\n     Exit();\n  }\n  \n  // By this point the connection is considered established.\n  app_filter = \"App='\"+ app_name + \"'\";\n  RssTbl = GetByFilter(dtRes,app_filter,false);\n  numRss = length(RssTbl);\n  log(2,\"Number of Resource found = \" + numRss); \n  \n  r = 0;\n  while (r < numRss){\n        asm_name=RssTbl[r].UniqueId;\n        asm_uniqueId=RssTbl[r].UniqueId;\n        asm_matchTokens=RssTbl[r].UniqueId;\n        asm_mergeTokens=RssTbl[r].UniqueId;\n        asm_entityTypes=RssTbl[r].EntityTypes;\n        asmFunctions.asm_createResource(asm_protocol,asm_host,asm_port,asm_path,HeadersToSend,HttpProperties,asm_name,asm_uniqueId,asm_matchTokens,asm_mergeTokens,asm_entityTypes,asm_JobId);\n        \n        Log(2,\"Creating \"+ asm_entityTypes + \", id: \" + asm_uniqueId);\n        r = r + 1;\n  } // End while\n  \n  // Get the Relationship table\n  EdgeTbl = GetByFilter(dtEdge,app_filter,false);\n  numEdge = length(EdgeTbl);\n  log(2,\"Number of Edge found = \" + numEdge); \n  \n  e = 0;\n  while (e < numEdge){\n      asm_fromUniqueId=EdgeTbl[e].FromRes;\n      asm_edgeType=EdgeTbl[e].Relationship;\n      asm_toUniqueId=EdgeTbl[e].ToRes;\n        \n      asmFunctions.asm_createEdge(asm_protocol,asm_host,asm_port,asm_path,HeadersToSend,HttpProperties,asm_fromUniqueId,asm_edgeType,asm_toUniqueId,asm_JobId);\n      Log(2,\"Connecting \"+ asm_fromUniqueId +\" : \" + asm_edgeType + \" : \" + asm_toUniqueId );\n      e = e + 1;\n  }// End while\n  \n  // Synchronize the Bulk Job\n  asmFunctions.asm_syncBulkJob(asm_protocol,asm_host,asm_port,asm_path,HeadersToSend,HttpProperties,asm_JobId);\n  \n  Log(\"^^^^^^^^^^^^^^^^^^^^^ Policy Completed ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\");\n}\n```\n\nThe `asmFunctions` code can be found in the [accompanying github](https://github.com/jwahidin/topology_modeling/tree/master/Cloudpak8sAlert) and not listed here for brevity.\n\n\n## The Topology\n\nBy running the previous Netcool/Impact policy, the content of the resource and relationship tables are pushed to ASM via the ASM's REST Interface.  You can then use **the ASM's Topology Viewer** to view the topology.\n\nThis is the topology being build by the above logic.\n\n![topology from events](./images/topology-from-events.png \"topology from events\")\n\n\n## Summary\nWe have built a topology using alerts, and some location information encoded in the alerts.  We used intermediary resource and relationship tables and periodically check for changes in the table.  If changes were detected, we pushed the tables' content into ASM through ASM's REST Interface. This simple exercise created using a simulator so the reader can recreate it.  It is simple, but hopefully, it gives you some ideas for your project.\n\n[Back to NOM Day2](/mcm/cp4mcm_nom_day2)\n","type":"Mdx","contentDigest":"f93fce8a8f101ccac22de92fef06fafa","counter":537,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"MCM - ASM - Day2 - Creating Topology from Events","description":"Day 2 articles on Operating Netcool Ops Manager","keywords":"day2, asm, nom, netcool_ops_manager, event"},"exports":{},"rawBody":"---\ntitle: MCM - ASM - Day2 - Creating Topology from Events\ndescription: Day 2 articles on Operating Netcool Ops Manager\nkeywords: 'day2, asm, nom, netcool_ops_manager, event'\n---\n\n[Back to NOM Day2](/mcm/cp4mcm_nom_day2)\n\n\n## Creating Topology from Alerts\n\n*I do not have an inventory system, but I want to build the topology of my environment.  How do you do it using ASM?* That is a common question that comes up in a few of my past customer engagement.  \n\nThis article will provide an approach to building a topology without an Inventory application.\n\n<InlineNotification>\n\nThere are solutions such as the [IBM Tivoli Network Manager](https://www.ibm.com/support/knowledgecenter/en/SSSHRK_4.2.0/overview/concept/ovr_product_overview_infocenter.html) that can discover your IP Network topology by querying the network environment using SNMP.  \n\n</InlineNotification>\n\nThis article assumes that you have some basic familiarity with building topology using ASM.\nIf you do not have this familiarity, you might want to read [a tutorial on topology modeling using ASM](https://medium.com/ibm-garage/topology-modelling-using-agile-service-manager-a-tutorial-2e521040ea64)\nand [Building a topology using Agile Service Manager's REST Interface](https://medium.com/ibm-garage/topology-modelling-using-agile-service-managers-rest-interface-7de14a85e333).   \n\nTo build a topology from scratch, you need to come up with some way to model it.  We can use our knowledge of the environment, for example.  Can we derive the topology information from specific knowledge on how the component of the ecosystem interacts?  Can we use a specific naming convention?  In this article, we will start with the output of _a remote ping_.  To make the topology reproducible by you, we are going to use an alerts simulator.\n\n## The model\nA router typically has a feature called remote ping.  You can set up for the router to ping other network devices for availability. You can then either query the router or set up the router to send the results to an external monitoring server. Alerts normally have a field call *Node*.  It represents the object that sends the alerts.  It can be a hostname or IP address of a Server or Management Component of a Network Devices.\n\nLet us use a fictional use case.  You just joined **AFictionalCompany** IT department and was asked your manager to create a topology of the company's main routers and servers.\nAFictionalCompany has two main offices, one in Singapore and another one in Sydney.  The company also has branch offices in Europe and North America.  An Application Server and a database server are running at every office.\n\nAs a member of the IT department, you have access to the Alerts. \nYou noticed that the networking team has set up the *remote ping* from the company's two main routers in Singapore and Sydney to all other routers in their branch offices.  The result of the remote ping is forwarded to your Alert Manager as alerts. You also noticed that each Application Servers and the Database Servers send Heartbeat Availability Alerts as well.  You also found out that the network team and the infrastructure team has labeled the router and the servers with location information.\n\nTo summarize, this is what available:\n-    The remote ping results from 2 main routers in Sydney and Singapore.\n-    Heartbeat Availability alerts from the Application Servers and Database Servers.\n-    Location information in the alerts.\n\n\n## The simulated alerts\nOMNIbus, the Event Management component of NOI, has an alert generator called the Simnet probe.  An OMNIbus probe normally collects from a network, or network management equipment.  A simnet probe, simulate the alerts. It generates simulated alerts based on _a definition file_.  The alerts can then be parsed and transformed using _a probe rule file_.\n\nWe will setup the simnet probe to simulate random link failure and random server failure.\nThe random link failure simulates a failed remote ping, and the random server failure simulates the failed heartbeat.\nThe configuration of the probe, the probe rule file, and the Netcool/Impact policies all can be found from the [GitHub page for this article](https://github.com/jwahidin/topology_modeling/tree/master/Cloudpak8sAlert).  Reading or Working with these configuration files requires certain knowledge in the Netcool Operation Insight.\n\nTo simulate the alerts, we have created all the nodes in the simnet probe definition file. Here is a sample of the Alerts using the out of the box Simnet rules file.\n\n![Simulated Alerts](./images/simulated_alerts.png \"Simulated Alerts\")\n\nNow comes the modeling part.  One way to approach the model building is to extract the alerts and then apply a series of business rules and then push the result to ASM.  Another approach is to _enrich_ the alerts with information that can help our model building before we extract them.  We will be using the second approach.  We enrich the alerts using a probe rule file.\nThe rule file inserts into the alerts the two endpoints involved in the remote ping. \n\nUsing the Server Alerts location information, we create a logic connecting the Server to the router in the same location. For example, we can connect an Alert from an AppServer called `SingaporeAppSrv` or from a DataBase server named `SingaporeDb` to the router responsible for the `Singapore` Office.\n\nHere are the Alerts after being enriched using the new rules file. We have added some tags (not shown), to make the processing of it easier.  Here is the list of alerts after being enriched.\n\n![Enriched Simulated Alerts](./images/simulated_alerts_enrich.png \"Enriched Simulated Alerts\")\n\nThe alerts now have both the resources and the link connecting the resources.  Now we have some data that can be used to create the topology model.  We have a few options for building the topology using this information.  One way is to parse the alerts and call the ASM REST API to create the topology.  Another way is to build a framework that we can reuse.  We populate two tables, one for `resources` and another one for the `relationship.`  By creating the two tables, we can enrich the topology with other information before we build it.  It also provides us with a checkpoint to ensure the extraction logic produces the desired structure. Since we already have Omnibus as the Alerts database, we build the resource and relationship using Omnibus.\n\nWe will be using the following process flow:\n\n![topology creation flow](./images/topology-creation-flow.png \"topology creation flow\")\n\n\n## Building the tables\n\nHere is the logic to build the resource and relationship table\n\n```\nAt Every Period, \n-    scan the new \"link \"simulated alerts inserted into the system since the last scan, and populate the resource and relationship tables if it does not already contain the resource and the relationship.\n-    Scan the new \"server\" simulated alerts inserted into the system since the last scan, and populate the resource and relationship tables if it does not already contain the resource and the relationship.\n```\n\nThe logic has been implemented using Netcool/Impact using `OmnibusEventReader`.\n\n```\n// insertResAndEdgeFromEvent\n// To be called from the OMNIbus Event Reader and hence is expecting an \n// EventContainer to be populated already.\n// Expected to be call when ProcessReq = 101 or 102 and Class is Simnet 3300\n\nfunction createOrUpdateResource(Resource,ResType){\n  key = globalApp + Resource;\n  Res = GetByKey(dtRes, key, 1);\n   \n  if (length(Res) > 0){\n    // Maintaining the update time so state rows can be removed.\n    sql=\"Update custom.a_res set lastUpdate = getdate() where Key = '\" + key + \"'\";\n    Log(\"SQL: \" + sql );\n    DirectSQL('defaultobjectserver', sql, false);\n  } else {\n    nRes = NewObject();\n    nRes.Key = key;\n    nRes.App = globalApp;\n    nRes.UniqueId = Resource;\n    nRes.entityTypes = ResType;\n    nRes.insertTime = GetDate();\n    nRes.lastUpdate = GetDate();\n    nRes.ttl = globalTTL;\n    AddDataItem(dtRes,nRes);\n    Log(\"New Resource Entry: \" + key);\n  }\n} // function\n\nfunction createOrUpdateEdge(From,Rel,To){\n  key = globalApp + From + Rel + To;\n  Edge = GetByKey(dtEdge, key, 1);\n \n  if (length(Edge) > 0){\n    // Maintaining the update time so state rows can be removed.\n    sql=\"Update custom.a_edge set lastUpdate = getdate() where Key = '\" + key +\"'\";\n    Log(\"SQL: \" + sql );\n    DirectSQL('defaultobjectserver', sql, false);\n  } else {\n    nEdge = NewObject();\n    nEdge.Key = key;\n    nEdge.App = globalApp;\n    nEdge.FromRes = From;\n    nEdge.Relationship = Rel;\n    nEdge.ToRes = To;\n    nEdge.insertTime = GetDate();\n    nEdge.lastUpdate = GetDate();\n    nEdge.ttl = globalTTL;\n    AddDataItem(dtEdge,nEdge);\n    Log(\"New Edge Entry: \" + key);\n  }\n} // function\n\nLog(\"vvvvvvvvvvvvvvvvvvvvvvvvvvvvvv Policy started vvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\");\n\n// Called by OmnibusEventReader.\ndtRes = \"asm_Res\";\ndtEdge = \"asm_Edge\";\nglobalTTL = 3600;\nglobalApp = \"Event1\";\n\neNode1 = EventContainer.Node;\neNode2 = EventContainer.NodeAlias;\n\nif (EventContainer.ProcessReq = 101) {\n  createOrUpdateResource(eNode1,\"router\");\n  createOrUpdateResource(eNode2,\"router\"); \n  createOrUpdateEdge(eNode1,\"routesVia\",eNode2);\n} elseif (EventContainer.ProcessReq = 102) {\n  createOrUpdateResource(eNode1,\"server\");\n  createOrUpdateResource(eNode2,\"router\"); \n  createOrUpdateEdge(eNode1,\"connectedTo\",eNode2);\n} else {\n  // Unknown, do nothing.\n}\n\nLog(\"^^^^^^^^^^^^^^^^^^^^^ Policy Completed ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\");\n\n```\nAfter processing, the content of the resource and relationship table is shown below:\n\nResource Table:\n![resource table](./images/resource_table.png \"resource table\")\n\nRelationship Table:\n![relationship table](./images/relationship_table.png \"relationship table\")\n\nWith the two tables created, we can use the same script as written in [the Medium blog to build the topology](https://medium.com/ibm-garage/topology-modelling-using-agile-service-managers-rest-interface-7de14a85e333).  The logic is something like this:\n\n```\nAt Every Period\n    Check if there is at least one updated row in the resource or relationship tables.\n    If there is, then call the script to create the topology from the resource and relationship table.\n```\n\nThis is the code implemented in Netcool/Impact:\n\n```\n// runBulkLoadOnNew\n// To be called periodically by policy activator.\n// lastUpdate is for removal - if required, not currently implemented.\n// \n\ndtRes = \"asm_Res\";\ndtEdge = \"asm_Edge\";\nglobalApp = \"Event1\";\n\n// The insertTime was done less than 60 seconds ago.\n\nfilter = \"((getdate()-insertTime) < 60)\";\n\nrecentRes = GetByFilter(dtRes,filter,false);\nnumRes = length(recentRes);\nrecentEdge = GetByFilter(dtEdge,filter,false);\nnumEdge = length(recentEdge);\nLog(\"Number of Changed Resource: \" + numRes +\", Number of Changed Edge: \" + numEdge);\n\nIf ((numRes>0)or(numEdge>0)){\n   Log(\"Executing Bulk Load at: \"  + GetDate());\n   asmRest.bulkLoad(dtRes,dtEdge,globalApp);\n}\n```\n\nThe `asmRest` Function being called is as follow:\n\n```\n// asmRest\n// Expected to be called by another policy with the dataTypes and App being passed as parameters.\n\nfunction bulkLoad(dtRes,dtEdge,sApp) {\n\n  hasError=\"no\";\n  Handle java.lang.Exception { \n    Log(\"Exception: \" + ErrorMessage);\n    hasError=\"yes\";\n  } \n  \n  Log(\"vvvvvvvvvvvvvvvvvvvvvvvvvvvvvv Policy started vvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\");\n  app_name = sApp;\n  Log(2,\"Parameter received: \" + app_name);\n  paramExist = false;\n  if (app_name != NULL) {\n      if (length(app_name) > 0) {\n          paramExist = true;\n      }\n  }\n  if (!paramExist) {\n      Log(\"Parameter not specified.  Policy will exit!\");\n      Exit();\n  }\n  \n  // Initialize the values of all the parameters and then start the Bulk Job it has not been started.\n  asmFunctions.asm_initalize(asm_protocol,asm_host,asm_port,asm_path,HeadersToSend,HttpProperties);\n  asmFunctions.asm_startBulkJob(asm_protocol,asm_host,asm_port,asm_path,HeadersToSend,HttpProperties,asm_JobId,retError);\n  \n  if ((retError == 'yes') or (hasError == 'yes')){\n     Log(\"Connecting to ASM has failed\");\n     Exit();\n  }\n  \n  // By this point the connection is considered established.\n  app_filter = \"App='\"+ app_name + \"'\";\n  RssTbl = GetByFilter(dtRes,app_filter,false);\n  numRss = length(RssTbl);\n  log(2,\"Number of Resource found = \" + numRss); \n  \n  r = 0;\n  while (r < numRss){\n        asm_name=RssTbl[r].UniqueId;\n        asm_uniqueId=RssTbl[r].UniqueId;\n        asm_matchTokens=RssTbl[r].UniqueId;\n        asm_mergeTokens=RssTbl[r].UniqueId;\n        asm_entityTypes=RssTbl[r].EntityTypes;\n        asmFunctions.asm_createResource(asm_protocol,asm_host,asm_port,asm_path,HeadersToSend,HttpProperties,asm_name,asm_uniqueId,asm_matchTokens,asm_mergeTokens,asm_entityTypes,asm_JobId);\n        \n        Log(2,\"Creating \"+ asm_entityTypes + \", id: \" + asm_uniqueId);\n        r = r + 1;\n  } // End while\n  \n  // Get the Relationship table\n  EdgeTbl = GetByFilter(dtEdge,app_filter,false);\n  numEdge = length(EdgeTbl);\n  log(2,\"Number of Edge found = \" + numEdge); \n  \n  e = 0;\n  while (e < numEdge){\n      asm_fromUniqueId=EdgeTbl[e].FromRes;\n      asm_edgeType=EdgeTbl[e].Relationship;\n      asm_toUniqueId=EdgeTbl[e].ToRes;\n        \n      asmFunctions.asm_createEdge(asm_protocol,asm_host,asm_port,asm_path,HeadersToSend,HttpProperties,asm_fromUniqueId,asm_edgeType,asm_toUniqueId,asm_JobId);\n      Log(2,\"Connecting \"+ asm_fromUniqueId +\" : \" + asm_edgeType + \" : \" + asm_toUniqueId );\n      e = e + 1;\n  }// End while\n  \n  // Synchronize the Bulk Job\n  asmFunctions.asm_syncBulkJob(asm_protocol,asm_host,asm_port,asm_path,HeadersToSend,HttpProperties,asm_JobId);\n  \n  Log(\"^^^^^^^^^^^^^^^^^^^^^ Policy Completed ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\");\n}\n```\n\nThe `asmFunctions` code can be found in the [accompanying github](https://github.com/jwahidin/topology_modeling/tree/master/Cloudpak8sAlert) and not listed here for brevity.\n\n\n## The Topology\n\nBy running the previous Netcool/Impact policy, the content of the resource and relationship tables are pushed to ASM via the ASM's REST Interface.  You can then use **the ASM's Topology Viewer** to view the topology.\n\nThis is the topology being build by the above logic.\n\n![topology from events](./images/topology-from-events.png \"topology from events\")\n\n\n## Summary\nWe have built a topology using alerts, and some location information encoded in the alerts.  We used intermediary resource and relationship tables and periodically check for changes in the table.  If changes were detected, we pushed the tables' content into ASM through ASM's REST Interface. This simple exercise created using a simulator so the reader can recreate it.  It is simple, but hopefully, it gives you some ideas for your project.\n\n[Back to NOM Day2](/mcm/cp4mcm_nom_day2)\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/mcm/cp4mcm_nom_day2/asm_day2_fromEvent/index.mdx"}}}}