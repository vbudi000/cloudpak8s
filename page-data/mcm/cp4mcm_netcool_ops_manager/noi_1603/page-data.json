{"componentChunkName":"component---src-pages-mcm-cp-4-mcm-netcool-ops-manager-noi-1603-index-mdx","path":"/mcm/cp4mcm_netcool_ops_manager/noi_1603/","result":{"pageContext":{"frontmatter":{"title":"MCM - Netcool Ops Manager (NOM) - Installation Guide","description":"Installation guide for installing CP4MCM Netcool Ops Manager optional components","keywords":"ibm,install,mcm, cp4mcm, nom, netcool_ops_manager"},"relativePagePath":"/mcm/cp4mcm_netcool_ops_manager/noi_1603/index.mdx","titleType":"page","MdxNode":{"id":"cb91fb85-a12d-50ff-b3fd-8ff02128c700","children":[],"parent":"8c8ed854-6ccf-5858-a608-770e179c2e3c","internal":{"content":"---\ntitle: MCM - Netcool Ops Manager (NOM) - Installation Guide\ndescription: Installation guide for installing CP4MCM Netcool Ops Manager optional components\nkeywords: 'ibm,install,mcm, cp4mcm, nom, netcool_ops_manager'\n---\n\n[Back to NOI current version - 1.6.1](/mcm/cp4mcm_netcool_ops_manager)\n\n\n## **Solution Overview**\n\nThe following section describe the installation procedure for NOI version 1.6.0.3 and ASM version 1.1.7.\n\nThere are different options that you can choose when installing the \nNetcool Ops Manager.  For example, you can choose an online install or an \noffline install.  \n\nThis playbook is written from the author's experience of installing the product. \nOne of the challenges for the engineer preparing the installation of Netcool \nOps Manager for the first time is that the many different installation options \nthat can lead to confusion.  To address this, we present this playbook as \nprescriptively as possible, following the most common options: ** offline \ninstallation of Agile Service Manager (ASM) and \nNetcool Operations Insight (NOI) on OpenShift Container Platform \nversion 4.3 for \nthe Production environment (size 1) **.  At the time of this writing, OCP version 4.3 is the current supported OCP platform by Netcool Ops Manager.\n\nThe flow of the installation is as follows:\n\n![CP4MCM Netcool Ops Manager Installation Flow](/assets/img/cp4mcm/cp4mcm_noi_installation.png)\n\n\n\n1. [Preparing the Openshift 4.3 environment](#nom-ocp)\n1. [Preparing the installation/bastion workstation](#nom-bastion)\n1. [Preparing the LDAP server](#nom-ldap)\n1. [Downloading the Netcool Ops Manager software](#nom-download)\n1. [Ensuring that IBM Common Platform Common Services is available](#nom-common) \n1. [Exposing OCP External Registry Access](#nom-registry)\n1. [Configuring the icp-inception at the bastion workstation](#nom-inception)\n1. [Running the icp-inception to install IBM Common Services](#nom-run)\n1. [Configuring IBM Common Services Authentication](#nom-auth)\n1. [Getting the cloudctl and helm clients from IBM Common Services](#nom-cli)\n1. [Installing Agile Service Manager](#nom-asm)\n1. [Installing Netcool Operations Insight](#nom-noi)\n1. [Post installation steps](#nom-post)\n\nEach step is detailed as follows:\n\n<a name=\"nom-ocp\"></a>\n\n## Preparing the Openshift 4.3 environment\n\n### Sizing\nThe online documentation (links provided below)\nprovides sizing guidelines.  Separate sizing \nguidelines are available depending on whether you are installing for a \nProof of Concept (PoC) or \nfor a production environment.\n\nBased on the installation that we performed, and choosing the \n\"production size\" option or \"size 1\", you need at least the following \nresources to \nrun the IBM Common Services, ASM and NOI.\n\n#### CPU and Memory\nRecommended minimal worker nodes sizing:\n\nDescription | Quantity\n--- | ---\nNumber of worker nodes | 6\nNumber of vCPUs per worker node | 20 \nMinimum memory per worker node | 24 GB\n\nIn preparing for this playbook, we installed the system into a cluster\nwith as little \nas possible resources.  With 100 vCPUs (5 worker nodes with 20 vCPU each), \nwe saw that some of the pods could not be loaded, because there \nwas not enough CPU \nresource.  The listed CPU and Memory requirements are the minimum. \nThe availability of more worker nodes is recommended.\n\nMore information on sizing can be found at the following sites:\n- [NOI Sizing Guidelines](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/soc/integration/reference/soc_sizing_full.html)\n- [ASM Sizing Guidelines](https://www.ibm.com/support/knowledgecenter/SS9LQB_1.1.7/Reference/r_asm_ocp_sizing.html)\n\n#### Storage Capacity\n\nIf you are installing into OCP 4.3, then **Rook/Ceph** or \n**Openshift Container Storage (OCS)**, with **RADOS Block Device (RBD)** storage \nclass, is the default supported OCP Storage solution.  We recommend Rook/Ceph \nas the dynamic storage solution for Netcool Ops Manager.\n\n**Image Registry Storage**: As you are preparing the OCP cluster, please allow at least 150GB of \nimage-registry storage for the Netcool Ops Manager. This is in addition to \nyour normal image-registry requirements for your other OCP's purpose. \n\n**Persistent Volume Claim**: If you are deploying the Openshift Container Storage (OCS), then OCS \ncreates a default 2 TB Rook/Ceph/RDB block storage.  For an initial \nproduction installation of Netcool Ops Manager, you need about 800 GB of \nstorage (PVC) space, in addition to the image-registry storage.  Please take \nnote of the storage class name. You need this later during the \ninstallation.\n\n<InlineNotification>\n\nPrior to OCP 4.3, storage could be a challenge.  GlusterFS or NFS have been \nfound to not be a recommended storage solution for Netcool Ops Manager. The \ncurrent installation scripts suggested Local Storage.  So if Rook/Ceph is not \navailable, then use Local Storage.\n\n</InlineNotification>\n\nIf you need help with installing your OCP environment, please see this \nplaybook's section on [installing OpenShift](../../ocp/introduction/).\n\n<a name=\"nom-bastion\"></a>\n\n## Preparing the installation/bastion workstation.\n\nYou need one bastion workstation that you can `ssh` into as the machine \nfrom which  \nyou run the installation.  You also need a Docker runtime to run the \ninstallation script.  An RHEL 7.x Virtual Machine with 4vCPU and 16 GB of \nRAM is a good candidate for the bastion workstation.  \n\n<InlineNotification>\n\nRHEL 8 comes with a `podman` package that replaces Docker, so it is not \nrecommended as the bastion workstation platform.\n\n</InlineNotification>\n\nYou need at least 30 GB under /tmp and 60GB under /var/lib/docker to run the \ninstallation scripts.  If you do not have 60GB available in the /var filesystem\nto run the install \nscript, you can configure docker to use a different directory.\n\nYou need to download and unpack the 4 command-line tools:\n- `oc` and `kubectl`\n- `docker`\n- `cloudctl`\n- `helm`\n\nSteps to get the `cloudctl` and `helm` command lines are provided in a \nlater section of this playbook.\n\n### Getting the oc and kubectl command lines\nYou download oc and kubectl from your OCP cluster. The kubectl executable is \na symbolic link of the oc executable.  The following \n[documentation from Red Hat](https://docs.openshift.com/container-platform/3.9/cli_reference/get_started_cli.html) \ndescribes the steps to get started with the `oc` and `kubectl` command line \ninterface.\n\n### Getting the docker command line\nYou can get the docker command line from the RHEL 7 extra repository.  Ensure that your \nyum repository is set up and run the following command.\n\n```\nsudo yum install docker -y\n```\nOnce you have install docker, you need to enable and run the docker daemon.\n\n```\nsudo systemctl enable docker.service\nsudo systemctl start docker.service \n```\n\nVerify that your docker environment is ready by executing:\n\n```\ndocker --version\n```\n\n<a name=\"nom-vardocker\"></a>\n\n#### Reassigning /var/lib/docker\n\nYou need to ensure that you have at least 60GB under /var and 30GB under \n/tmp when \nyou execute the NOI `cloudctl catalog load` later.  If you do not have that \ncapacity on /var then you can configure docker to use a different \ndirectory with the following steps.\n\nIf you are using RHEL 7.x, then you can do the following.\n\n- Stop the docker processes\n\n```\nsudo docker stop $(docker ps -q)\n```\n\n- Stop the docker daemon\n\n```\nsudo systemctl stop docker\n```\n\n- To verify: Unmount all docker mount points, and ensure all docker processes \nare not running\n```\nsudo umount /var/lib/docker/devicemapper/mnt/*\nps aux | grep -i docker | grep -v grep\n```\n\n- Edit the `/etc/sysconfig/docker` file, and add \nthe -g <NEW_DIRECTORY_NAME> to the `OPTIONS` assignement.  For example.\n\n```\n# Modify these options if you want to change the way the docker daemon runs\nOPTIONS='--selinux-enabled --log-driver=journald --signature-verification=false -D -g \"/<NEW/MOUNT/POINT>/\".\n```\n\n- Restart the systemctl daemon\n```\nsudo systemctl daemon-reload\n```\n\n- Copy the existing docker files over from /var/lib/docker to the new \ndirectory. In this example the new directory is /opt/docker, replace it with your path.\n```\nsudo rsync -aqxP /var/lib/docker /opt/docker\n```\n\n- Set SE linux labels\n```\nsudo restorecon /opt/docker\n```\n\n- Restart docker\n```\nsudo systemctl start docker\n```\n\n- Verify that the -g options is listed.\n```\nps aux | grep -i docker | grep -v grep\n```\n\n\n<a name=\"nom-ldap\"></a>\n\n## Preparing the LDAP server\nYou need to provide details of your LDAP server for the following components:\n- OCP Cluster\n- IBM Common Platform (ICP) Console of IBM Common Services.\n- NOI Proxy configuration.\n\nSetting up your LDAP server is a common requirement across all  Cloud Paks, \nso it is not detailed here.  \n\nDuring the installation, you will need to specify the following information, so \nget the information before you start the helm chart configuration:\n\n- Your Base Distinguished name.\n- Your LDAP URL.\n- Your LDAP Bind User Name and Password.\n- The filter to get the user information (User Filter and User ID map).\n- The filter to get the group information (Group Filter and Group ID map).\n- The filter to map a user to a group (Group member ID map).\n\nOne of the pods deployed by the NOI Helm Chart is an OpenLDAP pod. You can \nchoose to set up the OpenLDAP as a standalone repository or as a proxy to an \nexternal LDAP server.  If you choose to use the OpenLDAP pod as a proxy, then \nNetcool Ops Manager expects the external LDAP server to support the \nhierarchical LDAP structure.  In particular, the NOI LDAP configuration wants \nto use an ou (Organizational Unit).  \nMore information on the NOI Proxy LDAP requirement can be found in the \n[IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/soc/admin/reference/managing_users_using_an_external_ldap_server.html).\n\n<InlineNotification>\n\nNote that since RHEL 7.4, the RHEL repository no longer distributes the OpenLDAP \nserver.  The default LDAP server for RHEL 7.5 onwards is the IPA (Identity, Policy, Audit) server. The \nIPA server only supports a flat LDIF structure. It does not support `ou` \nso you can not use the IPA server as the external LDAP server for NOI.  This \ninformation can be found in the \n[Red Hat Solutions](https://access.redhat.com/solutions/4172491) documentation.\n\n\n</InlineNotification>\n\n\n\n<a name=\"nom-download\"></a>\n\n## Downloading the Netcool Ops Manager software\n\n\nThe eAssemby for Netcool Ops Manager is \n\nPartNo  | Description |  Size | Number of images\n--- | --- | --- | ---\nCJ5MZEN  | IBM Netcool Operations Insight v1.6.0.3 Cloud Paks Multiplatform English eAssembly | 30 GB | 7\n\nTo install ASM and NOI, you need to download the following components \nof the above eAssembly. \n\nPartNo  | Description |  Size | File name\n--- | --- | --- | ---\nCC5J3ML     |  Common Services for 3.2.4 IBM Netcool Operations Insight 1.6.0.3 Multilingual | 7.4 GB |COM_SVRCE3.2.4_NOI_1.6.0.3_ML.tar.gz\nCC686ML         | IBM Netcool Agile Service Manager v1.1.7 - Containers for IBM Cloud Private with Red Hat Enterprise Linux OpenShift Multilingual| 3.3 GB| NOI_ASM_V1.1.7_FOR_ICP_ML.tar.gz\nCC5IZML    | IBM Netcool Operations Insight 1.6.0.3 Operations Management - Containers for IBM Cloud Private with Red Hat Enterprise Linux OpenShift Multilingual | 13.1 GB | NOI_V1.6.0.3_OM_FOR_ICP_ML.tar.gz \n\n\n\n\n<a name=\"nom-common\"></a>\n\n## Ensuring that IBM Cloud Platform Common Services is available  \n\nIf you are installing into an Openshift cluster with Cloud Pak for MCM \nalready installed, then skip the next few steps and go to the \n[Installing Agile Service Manager](#nom-asm)\nsection, otherwise install IBM Common Services as described next.\n\n\n<a name=\"nom-registry\"></a>\n\n## Exposing OCP External Registry Access\n\nBy default, OCP does not expose its Image registry to external access. \nYou need to patch the image registry to create a default route. As a route, \nthe default route allows external acess to the OCP image registry. You need \nthis to load the image from your bastion server.\n\n```\noc patch configs.imageregistry.operator.openshift.io/cluster --patch '{\"spec\":{\"defaultRoute\":true}}' --type=merge\n```\nIf your OCP cluster certificate is self signed, then you need the client to \ndownload the certificate to trust.\nCopy the image registry certificate to your bastion server by performing the \nfollowing steps:\n\n\n```\nDOCKER_REGISTRY=\"$(kubectl get route -n openshift-image-registry default-route -o jsonpath='{.spec.host}')\"\nmkdir -p /etc/docker/certs.d/$DOCKER_REGISTRY\nopenssl s_client -showcerts -servername $DOCKER_REGISTRY -connect $DOCKER_REGISTRY:443 2>/dev/null | openssl x509 -inform pem > /etc/docker/certs.d/$DOCKER_REGISTRY/ca.crt\n```\n\nTest the image registry default route by performing a docker login.\n```\ndocker login -u $(oc whoami) -p $(oc whoami -t) ${OCP_REGISTRY}\n```\n\n<InlineNotification>\n\nNote that for the above `docker login` to work, you must first perform an \n`oc login` using a token. You do this by copying \nthe `oc login` with the token from the \nOCP Web Interface and pasting and running it from your command line. \nIf you perform an `oc login` using the certificate defined \nthrough the `KUBECONFIG` environment variable, then the token will be empty, and \nyour docker login will fail. \n\n</InlineNotification>\n\n\n\n\n<a name=\"nom-inception\"></a>\n\n## Configuring the icp-inception at the bastion workstation\n\nYou install IBM Common Services by configuring and running the \nicp-inception on docker.  Start by loading the IBM Common Services downloaded \npackage to the bastion server.\n\n```\ntar xf common-services-boeblingen-2002-x86_64.tar.gz -O | sudo docker load\n```\n\nCreate the cluster configuration from the icp-inception.\n\n```\nmkdir cluster\nsudo docker run --rm -v $(pwd)/cluster:/data -e LICENSE=accept ibmcom/icp-inception-amd64:3.2.4 cp cluster/config.yaml /data/config.yaml\n```\n\nEdit the cluster configuration file. \nYou need to replace the place holder in the section below. Specify the other \noptional components in the configuration file as required.\n\n```\ncluster_nodes:\n  master:\n    - <REPLACE_WITH_ONE_OF_YOUR_WORKER_NODE_NAME>\n  proxy:\n    - <REPLACE_WITH_ONE_OF_YOUR_WORKER_NODE_NAME>\n  management:\n    - <REPLACE_WITH_ONE_OF_YOUR_WORKER_NODE_NAME>\n\nstorage_class: <REPLACE_WITH_YOUR_STORAGE_CLASS>\n\ndefault_admin_user: <REPLACE_WITH_YOUR_ADMIN_USER>\ndefault_admin_password: <REPLACE_WITH_YOUR_ADMIN_USER_PASSWORD>\npassword_rules:\n  - '(.*)'\n```\n\n\nCopy the kubeconfig file that the OCP installation created, to the cluster directory.  Now you are ready to perform the install.\n\n\n\n\n<a name=\"nom-run\"></a>\n\n## Running the icp-inception to install IBM Common Services\n\nChange directory to the parent directory of the `cluster` directory, and run \nthe `icp-inception` playbook.\n\n```\nsudo docker run --net=host -t -e LICENSE=accept -v \"$(pwd)/cluster\":/installer/cluster -v /var/run/docker.sock:/var/run/docker.sock ibmcom/icp-inception-amd64:3.2.4 addon -v\n```\n\n<InlineNotification>\n\n** Note:** If you specify the `default` storage class in the config.yaml file\n(this is the default value), and it does not exist, the pod `icp-mongodb-0` \nwhich is part of `icp-mongodb` stateful set will not be able to continue, as \nit can not create its Persistent Volume Claim (PVC). \nTherefore, it is recommended that you explicitly specify the \nstorage class name in the config.yaml file, rather than using 'default'.\n\n</InlineNotification>\n\n\nMore information on configuring and running the IBM Common Service can be found in the [IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/csd/installer/3.2.2/installation-NOI.html)\n\n\n\n<a name=\"nom-auth\"></a>\n\n## Configuring IBM Common Services Authentication\n\nWhile optional at this stage of the installation, if you use external LDAP, \nwe recommend to verify that the connection is working.\n\nOnce the icp-inception is completed successfully, your IBM Common Services \nshould be up. Get the URL to your IBM Common Services, by checking the OCP \nroute.\n```\noc get route -n kube-system\n```\n\nYou are looking for a route name starting with `icp-console`. The fully \nqualified domain name will look like \n`icp-console.apps.<YOUR_OCP_BASE_DNS_NAMES>`.\n\nLog in to IBM Common Services using the URL and the user name and password \nthat you specified in the config.yaml file.\n\nSet up your LDAP connection, and create the user by following the instructions \nin the \n[MCM Knowledge Center](https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.3.0/iam/3.4.0/configure_ldap.html).\n\n\n\n\n<a name=\"nom-client\"></a>\n\n## Getting the cloudctl and helm clients from IBM Common Services\n\nGet your cloudctl client, log in to the IBM Common Services dashboard, \ncopy the curl command to download cloudctl to your bastion server, paste it \nto your command line, and then run the curl command. Change the mode of the \ncloudctl file to executable, and then move them to a directory in your \ncommand search path.  Verify that cloudctl is available by running it with a \nversion switch.\n\n```\ncurl -kLo cloudctl-linux-amd64-v3.2.4-1675 https://icp-console.apps.csmo-noi.ocp.csplab.local:443/api/cli/cloudctl-linux-amd64\nchmod 755 cloudctl-linux-amd64-v3.2.4-1675 \nmv cloudctl-linux-amd64-v3.2.4-1675 /usr/local/bin/cloudctl\ncloudctl version\n```\n\nDo a similar thing to download the helm client.\n\n```\ncurl -kLo helm-linux-amd64-v2.12.3.tar.gz https://icp-console.apps.csmo-noi.ocp.csplab.local:443/api/cli/helm-linux-amd64.tar.gz\nmkdir helm\ntar xvzf helm-linux-amd64-v2.12.3.tar.gz -C helm\nmv helm/linux-amd64/helm /usr/local/bin\n```\n\nInitialize your helm client. \n\n```\nexport HELM_HOME=~/.helm\nhelm init --client-only\n```\n\nLog in to the IBM Common Services dashboard, copy the login token, and \nlog in to IBM Common Services from the command line using `cloudctl`.  \nYou need to perform a cloudctl login at least once, as it creates the \nnecessary certificate to access helm in the user's `$HOME/.helm` directory. \nVerify that the helm CLI is configured correctly by running a `helm version`\ncommand.  \n\n\n```\ncloudctl login -a https://icp-console.apps.<OCP_CLUSTER> -u <ICP-USER> -n kube-system --skip-ssl-validation\nhelm version --tls\n```\n\nYou might also want to configure the `HELM_HOME` variable\nin your login profile.\n\nMore information on configuring the `cloudctl` and `helm` command line \ninterfaces can be found in the \n[MCM Knowledge Center](https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.3.0/cli/cli_guide_mcm.html)\ndocumentation.\n\n\n\n\n<a name=\"nom-asm\"></a>\n\n## Installing Agile Service Manager and Netcool Operations Insight\n\nThroughout this section the following resource sample values are used:\n\nResource | Sample Value\n--- | ---\nNamespace for ASM and NOI | noins\nRelease name for ASM | asmrel1\nRelease name for NOI | noirel1\nStorage Class name   | rook-ceph-block\nPassword for NOI     | Netcool2020\n\n\nPlease change the sample values if necessary during your installation.  \n\n### Creating the namespace\n\nCreate a namespace (also referred to as project in OCP) for your NOI and ASM \ninstallations.\n\n```\noc new-project noins\n```\n\n<InlineNotification>\n\n** Note**: NOI and ASM must be installed in the same namespace.\n\n</InlineNotification>\n\n\n#### Day 2 Operations: Pod Placement / Node Selection\n\nAs a Day 2 tip: It is recommended to create a pod placement rule to schedule \nall the NOI and ASM pods to run on a selection of worker nodes. One way to \ndo this is to define a **Node Selection** policy by labeling the target worker \nnodes and then telling the scheduler to run the pods only on those worker \nnodes. All NOI and ASM pods are running in the same namespace. Thus, rather \nthan specifying the target label on each pod, it is more manageable to \nspecify the node selection on the namespace.\n\nLabel the target worker nodes. If the name of the worker nodes that run the \nASM and NOI pods is `worker1` to `worker6`, then the worker nodes can be assigned \na label `nodetorun=noiasm` as follow (you can use your name-value pair):\n\n```\nfor i in worker1 worker2 worker3 worker4 worker5 worker6\ndo \noc label nodes $i nodetorun=noiasm\ndone\n```\n\nYou can then tell the scheduler to run all pods in the namespace `noins` by \nadding the label to the annotations sections of the namespace.  You can \nmodify the annotations by executing the following:\n\n```\noc edit ns noins\n```\n\nInsert a line specifying the node selector:\n```\n    openshift.io/node-selector: nodetorun=noiasm\n```\n\nThe following is an example of the edit namespace command, with the \nnode-selector already added at the end of the annotations section:\n```\n[root@bootstrap common]# oc edit ns noins\nnamespace/noins edited\n---------\n# Please edit the object below. Lines beginning with a '#' will be ignored,\n# and an empty file will abort the edit. If an error occurs while saving this file will be\n# reopened with the relevant failures.\n#\napiVersion: v1\nkind: Namespace\nmetadata:\n  annotations:\n    openshift.io/description: NOI Name Space\n    openshift.io/display-name: noins\n    openshift.io/requester: cadmin\n    openshift.io/sa.scc.mcs: s0:c25,c0\n    openshift.io/sa.scc.supplemental-groups: 1000600000/10000\n    openshift.io/sa.scc.uid-range: 1000600000/10000\n    openshift.io/node-selector: nodetorun=noiasm\n  creationTimestamp: \"2020-05-01T07:42:33Z\"\n  name: noins\n  resourceVersion: \"8192651\"\n  selfLink: /api/v1/namespaces/noins\n  uid: 7312be5a-ef8c-42f7-a3df-1e72ecb9b16e\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: Active ~                 \n```\n\n\n\n### Assigning your release name\n\nYou need to specify a release name when you install the NOI and ASM charts. \nNOI and ASM come as separate charts, so you need different release names for \nthe NOI deployment and ASM deployment.  \n\nThe release name has to begin with a lowercase letter and end with an \nalphanumeric character. The release name can only contain hyphens and \nlowercase leters.\n\nDuring the installation of NOI, you need to specify the ASM release name.  \nSimilarly, during the installation of ASM, you need to specify the NOI \nrelease name. Hence it is best to decide both release names before you start \nthe installation.\n\nWe are using `asmrel1` and `noirel1` in the ongoing example.  Please change \nthem to your choice.\n\n\n### Loading the ASM image.\n\nLog in to IBM Common Services from the command line as a `cluster-admin` user. \nLoad the ASM image to the IBM Common Platform Catalog repository, by running \nthe following command from the directory where the \n`NOI_ASM_V1.1.7_FOR_ICP_ML.tar.gz` image is located. \n\n```\nOCP_REGISTRY=$(oc get route -n openshift-image-registry default-route -o jsonpath='{.spec.host}')\ndocker login -u $(oc whoami) -p $(oc whoami -t) ${OCP_REGISTRY}\ncloudctl catalog load-archive --archive NOI_ASM_V1.1.7_FOR_ICP_ML.tar.gz --registry ${OCP_REGISTRY}/noins\n```\n\n<InlineNotification>\n\n** Note**: During the loading of the image to the image registry, you need to \nhave enough space in /tmp and /var/lib/docker. If your /var mount point is \ntoo small, you can tell docker to use a different directory. \nPlease refer to the earlier [section](#nom-vardocker). \n\n</InlineNotification>\n\n\n### Creating the Cluster Role\n\nASM comes with several scripts that you need to execute before you start the \ninstallation.\n\nUnpack the ASM download then execute the scripts to create the cluster role \nand the create Security NameSpace:\n\n```\ntar xvzf NOI_ASM_V1.1.7_FOR_ICP_ML.tar.gz \ncd pak_extensions/pre-install/clusterAdministration\nbash createSecurityClusterPrereqs.sh\nbash createSecurityNamespacePrereqs.sh noins\n```\n\n### Creating the PVC\n\nEdit the `storageConfig.env` file located in the `clusterAdministration` \ndirectory.  Edit the `storageConfig.env`, and specify the 3 worker nodes. \nIf you want to use the Local Storage then run the PVC creation scripts.\n\n```\nbash createStorageVolumes.sh noins asmrel1\n```\n\nThe scripts will print out the additional actions that you need to perform.  \n\n\n#### Creating the  PVC using Rook/Ceph\n\nAs mentioned in the [OCP Storage](#nom-storage) section at the beginning of this \nchapter, you can use the OCS or Rook/Ceph dynamic storage. To do this, you \nneed to change the yaml part of the script.\n\nMake a backup of the `kubhelper.sh` script and then edit it.\n\n```\ncd ../../common\ncopy kubhelper.sh kubhelper.sh.bak\nvi kubhelper.sh\n```\n\nSearch for the line starting with EOPV, and change the content between the \ntwo EOPVs, as follows:\n(Change the `rook-ceph-block` to your storage class name).\n\n\n```\ncat <<EOPV | ${CMD} apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ${4}\n  namespace: ${3}\n  labels:\n    release: ${2}\n  finalizers:\n  - kubernetes.io/pvc-protection\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: ${5}Gi\n  storageClassName: rook-ceph-block\nEOPV\n```\n\nRun the script as before.\n\n```\ncd ../pre-install/clusterAdministration\nbash createStorageVolumes.sh noins asmrel1\n```\n\nVerify that the PV and PVC are created:\n\n\n```\noc project noins\noc get pv\noc get pvc\n```\n\n<InlineNotification>\n\n**Note**: The prefered PVC reclaim policy of ASM is the **Retain** option.  If your default storage class reclaim policy is not \"Retain\" then you can patch the PV by using this command: \n\n`oc patch pv <PV_NAME>  -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}'`\n\n</InlineNotification>\n\n\n\n### Installing Agile Service Manager from the catalog\n\nThe product documentation recommends installing Agile Service Manager before installing Netcool Operation Insight.  \n\nBrowse the IBM Common Services URL as a `cluster-admin` user, \nselect the catalog, and enter \"netcool\" in the search bar. The ASM chart \nshould be shown.  Select the chart and go through the chart configuration.\n\nThere are 2 configuration options that you need to be careful with:\n\nConfiguration Options   |   Values\n--- | ---\nRepository | image-registry.openshift-image-registry.svc:5000/noins\nRouter domain | apps.<YOUR_OCP_CLUSTER_NAME>\n\n\n<InlineNotification>\n\n**Note**: If you use the cloudctl catalog load command to load the ASM image, \nthen the repository is pre-populated with the external docker registry name. \nYou need to change this to the internal docker registry name. If you do not \nchange this to the internal registry name and you configured OCP using a \nself-signed certificate, then the installation of ASM will not even start, as the \nhelm process can not pull the image from the registry.\n\n</InlineNotification>\n\n\n\n#### ASM File Observer\n\nIf you choose to install the ASM File Observer during the initial installation of ASM, you may notice that the pod (`asmrel1-file-observer-*`) is in *Pending* mode.  This may be caused by the PVC for the pod being released and since the reclaim policy is \"Retain\" it can not be bound again. Should this happen, then delete and recreate the PVC `data-noirel1-file-observer`.  The following yaml file might be helpful.  Remember to change the `namespace`, `release`, and `storageClassName` to your configuration.  Note that deleting and recreating the PVC will wipe out the content of the file system in the PVC.  Do not do this after the initial install of ASM, unless you are ok with the removal of the content of the file system.\n\n```\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-asmrel1-file-observer \n  namespace: noins\n  labels:\n    release: asmrel1\n  finalizers:\n  - kubernetes.io/pvc-protection\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: rook-ceph-block\n```\n\nMore information on installing Agile Service Manager can be found in the [IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SS9LQB_1.1.7/welcome_page/kc_welcome-444.html).\n\n\n\n\n<a name=\"nom-noi\"></a>\n\n## Installing Netcool Operations Insight\n\n\n### Loading the NOI Catalog\nYou start the installation by loading the NOI image.  Unlike in ASM, for NOI, \nyou have to unpack the downloaded image first.\n\n```\ntar xvzf /home/netcool/NOI_V1.6.0.3_OM_FOR_ICP_ML.tar.gz \ncloudctl catalog load-archive --archive ibm-netcool-prod-2.1.3-x86_64.tar.gz --registry default-route-openshift-image-registry.apps.<OCP_CLUSTER_BASE_DNS>/noins\n```\n\n### Creating the Service Account\n\nDepending whether you will be running the helm chart as cluster admin or not, \nyou need to create the service account and assign the permissions to it. \nIf you are unsure, perform the following. \n(Change `noins` to your namespace):\n\n```\noc create serviceaccount noi-service-account -n noins\noc adm policy add-scc-to-user ibm-privileged-scc system:serviceaccount:noins:noi-service-account\noc create role noiservice-reader --verb=update --verb=patch --verb=get --verb=list --verb=watch --resource=services -n noins\noc create rolebinding noiservice-reader --role noiservice-reader --serviceaccount noins:noi-service-account -n noins\noc create role noiconfigmap-reader --verb=update --verb=patch --verb=get --verb=list --verb=watch --resource=configmaps -n noins\noc create rolebinding noiconfigmap-reader --role noiconfigmap-reader --serviceaccount noins:noi-service-account -n noins\noc create role noisecret-creater --verb=list --verb=create --verb=get --resource=secrets -n noins\noc create rolebinding noisecret-creater --serviceaccount noins:noi-service-account --role noisecret-creater -n noins\noc create role releasename-redis --verb=get --verb=list --verb=update --verb=patch --verb=watch --resource=pods -n noins\noc create rolebinding releasename-redis --serviceaccount noins:noi-service-account --role releasename-redis -n noins\noc create role noiroute-reader --verb=update --verb=patch --verb=get --verb=list --verb=watch --resource=routes --resource=routes/custom-host -n noins\noc create rolebinding noiroute-reader --role noiroute-reader --serviceaccount noins:noi-service-account -n noins\n```\n\nVerify that you get a **yes** to all of the following commands:\n\n```\nkubectl auth can-i patch services --namespace noins --as system:serviceaccount:noins:noi-service-account \nkubectl auth can-i watch configmaps --namespace noins --as system:serviceaccount:noins:noi-service-account\nkubectl auth can-i create secrets --namespace noins --as system:serviceaccount:noins:noi-service-account\nkubectl auth can-i create secrets --namespace noins --as system:serviceaccount:noins:noi-service-account\nkubectl auth can-i watch pods --namespace noins --as system:serviceaccount:noins:noi-service-account\nkubectl auth can-i patch routes --namespace noins --as system:serviceaccount:noins:noi-service-account\nkubectl auth can-i patch routes/custom-host --namespace noins --as system:serviceaccount:noins:noi-service-account\n```\n\n### Creating the passwords for the different NOI components.  \n\nPerform the following steps if you want to choose the passwords for the NOI installation.\nIf you do not do this, then the passwords are created for you. \nThe password for Cassandra is already created during ASM install, so you will \nsee a warning when you perform the following. \nThe creation of the Cassandra secret is still included here for completeness. \nChange the `noins` to your namespace, and change the `Netcool2020` sample \npassword to your preferred password.  \n\n```\noc create secret generic noirel1-icpadmin-secret --from-literal=ICP_ADMIN_PASSWORD=Netcool2020 --namespace noins\noc create secret generic noirel1-impact-secret --from-literal=IMPACT_ADMIN_PASSWORD=Netcool2020 --namespace noins\noc create secret generic noirel1-la-secret --from-literal=UNITY_ADMIN_PASSWORD=Netcool2020 --namespace noins\noc create secret generic noirel1-ldap-secret --from-literal=LDAP_BIND_PASSWORD=Netcool2020 --namespace noins\noc create secret generic noirel1-omni-secret --from-literal=OMNIBUS_ROOT_PASSWORD=Netcool2020 --namespace noins\noc create secret generic noirel1-was-secret --from-literal=WAS_PASSWORD=Netcool2020 --namespace noins\noc create secret generic noirel1-couchdb-secret --from-literal=password=Netcool2020 --from-literal=secret=couchdb --from-literal=username=root --namespace noins\noc create secret generic noirel1-systemauth-secret --from-literal=password=Netcool2020 --from-literal=username=system --namespace noins\noc create secret generic noirel1-ibm-hdm-common-ui-session-secret --from-literal=session=Netcool2020 --namespace noins\noc create secret generic noirel1-cassandra-auth-secret --from-literal=username=hdm --from-literal=password=Netcool2020 --namespace noins\noc create secret generic noirel1-ibm-redis-authsecret --from-literal=username=redis --from-literal=password=Netcool2020 --namespace noins\noc create secret generic noirel1-kafka-admin-secret --from-literal=username=kafka --from-literal=password=Netcool2020 --namespace noins\noc create secret generic noirel1-kafka-client-secret --from-literal=username=admin --from-literal=password=Netcool2020 --namespace noins\n```\n\n<InlineNotification>\n\n**Note**: At the time of this writing, the online manual mistakenly creates a \npassword for `noirel1-impactadmin-secret` rather than `noirel1-impact-secret`. \nThe name has been corrected in the above command.\n\n</InlineNotification>\n\n\n### Deploying the NOI helm chart\n\nLog in to the IBM Common Services GUI, select Catalog, type \"netcool\" in the \nsearch fields, and select the NOI Chart.\n\n#### Specifying the configuration.\n\nThe following are some notes on the configuration.\n\nConfiguration  | Suggested Values  | Description\n--- | --- | ---\nHelm release name |    noirel1 | Replace this with your NOI release name, it can not be the same with the ASM release name.\nTarget namespace |    noins | The namespace where you did all the previous service account and password creation.  It should be the same as the ASM namespace.\nTarget Cluster     | local-cluster | Unless you use MCM to deploy to a remote cluster.\nPod Security    | ibm-priviledged-psp | You have configured this earlier. \nMaster Node    | `apps.<THE-OCP-CLUSTER-BASE-DOMAIN-NAME>` | The OCP service base name.\nHttps Port    | 443 | The default 443 should be good unless you have a specific network config.\nImage repository |    `image-registry.openshift-image-registry.svc:5000/noins` | Change this from the default external repository.  Change noins to your namespace.\nDocker image repository secret    | Leave Blank | Leave it blank unless you want to use a secret that you have created previously.\nEnvironment Size |    Size1 | For POC change to Size0\nServiceAccount under which your pods run |    noi-service-account | Pre-filled, you have created this earlier.\nCreate required RBAC RoleBinding |    check | The default is not checked.\nUse existing TLS certificate secrets |    check | The default is not checked.\nIndicate that all password secrets have been created prior to install |    check | Unless you want the installer to generate the secrets.\nEnable sub-chart resource requests |    check | default. Enabling this will use more resources. You might want to uncheck this for POC.\nEnable anti-affinity |    check | default.  For resilience, always check this.\nEnable data persistence    | check | Unless you are doing a POC and do not want to persist the data.\nUse dynamic provisioning |    check |  If you do not use the dynamic provisioning, then you need to create the PVC first.\nNumber of Impact server instance    | 2 | (default is 1).  \nLDAP Mode |    standalone | ** Note:** If you specify standalone, **do not change** any of the LDAP configurations after this.  Only specify the details of the LDAP for proxy.\nEnable ASM Integration | check | Check, unless you want to install only NOI.\n\n\n#### Monitoring the deployment.\n\nWatch for the pods. \n\n```\nwatch -n15 oc get pods -n noins\n```\n\nOnce the pods are all up, then you have completed the installation.\n\n### **A Few Common Issues**\n\nSome common issues during the deployment include:\n\n- **Not enough compute (CPU, Memory) resources**.  The impact pods (`*-nciserver-*`) are initialized last. \nIt also requires a comparatively larger compute resource. If your impact \npods are not running, then run the `oc describe pods <POD_NAME>` command to check what caused it.\n- **Not enough storage**.  If you do not have enough Disk Storage, then the \nPVCs might not all be successfully created.  You should be able to see this \nalso using the `oc describe pods <POD_NAME>` command.\n\n\nMore information on installing Netcool Operation Insight can be found in the [IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/soc/integration/task/int_installing-opsmg-rhocp_ui.html).\n\n\n<a name=\"nom-post\"></a>\n\n## Post-installation steps\n\nIf you install Agile Service Manager after Netcool Operation Insight, you must restart some services. If the Agile Service Manager secrets are changed, you must also restart some services.  This is described in [the IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/soc/integration/task/int_installing-asm-rhocp.html)\n\n\n### Helm release details\n\nGet the helm details of the ASM release.\n\n```\nhelm status asmrel1 --tls\n```\n\nFrom the output, you can see useful information about the resources created \nby the Helm install as well as some useful commands.\n\nSimilarly, you can check the helm status of the NOI Release.\n\n```\nhelm status noirel1 --tls\n```\n\nThe following lists some of the information provided by the above helm status \ncommands.\n\nComponent   | URL\n--- | ---\nWebGUI | `https://netcool.noirel1.apps.<OCP_CLUSTER>:443/ibm/console`\nWAS | `https://was.noirel1.apps.<OCP_CLUSTER>:443/ibm/console`\nLog Analytics | `https://scala.noirel1.apps.<OCP_CLUSTER>:443/Unity`\nImpact | `https://impact.noirel1.apps.<OCP_CLUSTER>:443/ibm/console`\n\n\n\n### Assigning roles\n\nLog in to DASH and assign the user or group roles. \nYour ASM and NOI base system is now installed and ready.\n\nMore information on administering users can be found in the [IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/soc/admin/task/adm_administering-users.html)\n\n\n[Back to NOI current version - 1.6.1](/mcm/cp4mcm_netcool_ops_manager)","type":"Mdx","contentDigest":"b08bfd644c6b8dddfa9e0fcfbb4bb82a","counter":573,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"MCM - Netcool Ops Manager (NOM) - Installation Guide","description":"Installation guide for installing CP4MCM Netcool Ops Manager optional components","keywords":"ibm,install,mcm, cp4mcm, nom, netcool_ops_manager"},"exports":{},"rawBody":"---\ntitle: MCM - Netcool Ops Manager (NOM) - Installation Guide\ndescription: Installation guide for installing CP4MCM Netcool Ops Manager optional components\nkeywords: 'ibm,install,mcm, cp4mcm, nom, netcool_ops_manager'\n---\n\n[Back to NOI current version - 1.6.1](/mcm/cp4mcm_netcool_ops_manager)\n\n\n## **Solution Overview**\n\nThe following section describe the installation procedure for NOI version 1.6.0.3 and ASM version 1.1.7.\n\nThere are different options that you can choose when installing the \nNetcool Ops Manager.  For example, you can choose an online install or an \noffline install.  \n\nThis playbook is written from the author's experience of installing the product. \nOne of the challenges for the engineer preparing the installation of Netcool \nOps Manager for the first time is that the many different installation options \nthat can lead to confusion.  To address this, we present this playbook as \nprescriptively as possible, following the most common options: ** offline \ninstallation of Agile Service Manager (ASM) and \nNetcool Operations Insight (NOI) on OpenShift Container Platform \nversion 4.3 for \nthe Production environment (size 1) **.  At the time of this writing, OCP version 4.3 is the current supported OCP platform by Netcool Ops Manager.\n\nThe flow of the installation is as follows:\n\n![CP4MCM Netcool Ops Manager Installation Flow](/assets/img/cp4mcm/cp4mcm_noi_installation.png)\n\n\n\n1. [Preparing the Openshift 4.3 environment](#nom-ocp)\n1. [Preparing the installation/bastion workstation](#nom-bastion)\n1. [Preparing the LDAP server](#nom-ldap)\n1. [Downloading the Netcool Ops Manager software](#nom-download)\n1. [Ensuring that IBM Common Platform Common Services is available](#nom-common) \n1. [Exposing OCP External Registry Access](#nom-registry)\n1. [Configuring the icp-inception at the bastion workstation](#nom-inception)\n1. [Running the icp-inception to install IBM Common Services](#nom-run)\n1. [Configuring IBM Common Services Authentication](#nom-auth)\n1. [Getting the cloudctl and helm clients from IBM Common Services](#nom-cli)\n1. [Installing Agile Service Manager](#nom-asm)\n1. [Installing Netcool Operations Insight](#nom-noi)\n1. [Post installation steps](#nom-post)\n\nEach step is detailed as follows:\n\n<a name=\"nom-ocp\"></a>\n\n## Preparing the Openshift 4.3 environment\n\n### Sizing\nThe online documentation (links provided below)\nprovides sizing guidelines.  Separate sizing \nguidelines are available depending on whether you are installing for a \nProof of Concept (PoC) or \nfor a production environment.\n\nBased on the installation that we performed, and choosing the \n\"production size\" option or \"size 1\", you need at least the following \nresources to \nrun the IBM Common Services, ASM and NOI.\n\n#### CPU and Memory\nRecommended minimal worker nodes sizing:\n\nDescription | Quantity\n--- | ---\nNumber of worker nodes | 6\nNumber of vCPUs per worker node | 20 \nMinimum memory per worker node | 24 GB\n\nIn preparing for this playbook, we installed the system into a cluster\nwith as little \nas possible resources.  With 100 vCPUs (5 worker nodes with 20 vCPU each), \nwe saw that some of the pods could not be loaded, because there \nwas not enough CPU \nresource.  The listed CPU and Memory requirements are the minimum. \nThe availability of more worker nodes is recommended.\n\nMore information on sizing can be found at the following sites:\n- [NOI Sizing Guidelines](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/soc/integration/reference/soc_sizing_full.html)\n- [ASM Sizing Guidelines](https://www.ibm.com/support/knowledgecenter/SS9LQB_1.1.7/Reference/r_asm_ocp_sizing.html)\n\n#### Storage Capacity\n\nIf you are installing into OCP 4.3, then **Rook/Ceph** or \n**Openshift Container Storage (OCS)**, with **RADOS Block Device (RBD)** storage \nclass, is the default supported OCP Storage solution.  We recommend Rook/Ceph \nas the dynamic storage solution for Netcool Ops Manager.\n\n**Image Registry Storage**: As you are preparing the OCP cluster, please allow at least 150GB of \nimage-registry storage for the Netcool Ops Manager. This is in addition to \nyour normal image-registry requirements for your other OCP's purpose. \n\n**Persistent Volume Claim**: If you are deploying the Openshift Container Storage (OCS), then OCS \ncreates a default 2 TB Rook/Ceph/RDB block storage.  For an initial \nproduction installation of Netcool Ops Manager, you need about 800 GB of \nstorage (PVC) space, in addition to the image-registry storage.  Please take \nnote of the storage class name. You need this later during the \ninstallation.\n\n<InlineNotification>\n\nPrior to OCP 4.3, storage could be a challenge.  GlusterFS or NFS have been \nfound to not be a recommended storage solution for Netcool Ops Manager. The \ncurrent installation scripts suggested Local Storage.  So if Rook/Ceph is not \navailable, then use Local Storage.\n\n</InlineNotification>\n\nIf you need help with installing your OCP environment, please see this \nplaybook's section on [installing OpenShift](../../ocp/introduction/).\n\n<a name=\"nom-bastion\"></a>\n\n## Preparing the installation/bastion workstation.\n\nYou need one bastion workstation that you can `ssh` into as the machine \nfrom which  \nyou run the installation.  You also need a Docker runtime to run the \ninstallation script.  An RHEL 7.x Virtual Machine with 4vCPU and 16 GB of \nRAM is a good candidate for the bastion workstation.  \n\n<InlineNotification>\n\nRHEL 8 comes with a `podman` package that replaces Docker, so it is not \nrecommended as the bastion workstation platform.\n\n</InlineNotification>\n\nYou need at least 30 GB under /tmp and 60GB under /var/lib/docker to run the \ninstallation scripts.  If you do not have 60GB available in the /var filesystem\nto run the install \nscript, you can configure docker to use a different directory.\n\nYou need to download and unpack the 4 command-line tools:\n- `oc` and `kubectl`\n- `docker`\n- `cloudctl`\n- `helm`\n\nSteps to get the `cloudctl` and `helm` command lines are provided in a \nlater section of this playbook.\n\n### Getting the oc and kubectl command lines\nYou download oc and kubectl from your OCP cluster. The kubectl executable is \na symbolic link of the oc executable.  The following \n[documentation from Red Hat](https://docs.openshift.com/container-platform/3.9/cli_reference/get_started_cli.html) \ndescribes the steps to get started with the `oc` and `kubectl` command line \ninterface.\n\n### Getting the docker command line\nYou can get the docker command line from the RHEL 7 extra repository.  Ensure that your \nyum repository is set up and run the following command.\n\n```\nsudo yum install docker -y\n```\nOnce you have install docker, you need to enable and run the docker daemon.\n\n```\nsudo systemctl enable docker.service\nsudo systemctl start docker.service \n```\n\nVerify that your docker environment is ready by executing:\n\n```\ndocker --version\n```\n\n<a name=\"nom-vardocker\"></a>\n\n#### Reassigning /var/lib/docker\n\nYou need to ensure that you have at least 60GB under /var and 30GB under \n/tmp when \nyou execute the NOI `cloudctl catalog load` later.  If you do not have that \ncapacity on /var then you can configure docker to use a different \ndirectory with the following steps.\n\nIf you are using RHEL 7.x, then you can do the following.\n\n- Stop the docker processes\n\n```\nsudo docker stop $(docker ps -q)\n```\n\n- Stop the docker daemon\n\n```\nsudo systemctl stop docker\n```\n\n- To verify: Unmount all docker mount points, and ensure all docker processes \nare not running\n```\nsudo umount /var/lib/docker/devicemapper/mnt/*\nps aux | grep -i docker | grep -v grep\n```\n\n- Edit the `/etc/sysconfig/docker` file, and add \nthe -g <NEW_DIRECTORY_NAME> to the `OPTIONS` assignement.  For example.\n\n```\n# Modify these options if you want to change the way the docker daemon runs\nOPTIONS='--selinux-enabled --log-driver=journald --signature-verification=false -D -g \"/<NEW/MOUNT/POINT>/\".\n```\n\n- Restart the systemctl daemon\n```\nsudo systemctl daemon-reload\n```\n\n- Copy the existing docker files over from /var/lib/docker to the new \ndirectory. In this example the new directory is /opt/docker, replace it with your path.\n```\nsudo rsync -aqxP /var/lib/docker /opt/docker\n```\n\n- Set SE linux labels\n```\nsudo restorecon /opt/docker\n```\n\n- Restart docker\n```\nsudo systemctl start docker\n```\n\n- Verify that the -g options is listed.\n```\nps aux | grep -i docker | grep -v grep\n```\n\n\n<a name=\"nom-ldap\"></a>\n\n## Preparing the LDAP server\nYou need to provide details of your LDAP server for the following components:\n- OCP Cluster\n- IBM Common Platform (ICP) Console of IBM Common Services.\n- NOI Proxy configuration.\n\nSetting up your LDAP server is a common requirement across all  Cloud Paks, \nso it is not detailed here.  \n\nDuring the installation, you will need to specify the following information, so \nget the information before you start the helm chart configuration:\n\n- Your Base Distinguished name.\n- Your LDAP URL.\n- Your LDAP Bind User Name and Password.\n- The filter to get the user information (User Filter and User ID map).\n- The filter to get the group information (Group Filter and Group ID map).\n- The filter to map a user to a group (Group member ID map).\n\nOne of the pods deployed by the NOI Helm Chart is an OpenLDAP pod. You can \nchoose to set up the OpenLDAP as a standalone repository or as a proxy to an \nexternal LDAP server.  If you choose to use the OpenLDAP pod as a proxy, then \nNetcool Ops Manager expects the external LDAP server to support the \nhierarchical LDAP structure.  In particular, the NOI LDAP configuration wants \nto use an ou (Organizational Unit).  \nMore information on the NOI Proxy LDAP requirement can be found in the \n[IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/soc/admin/reference/managing_users_using_an_external_ldap_server.html).\n\n<InlineNotification>\n\nNote that since RHEL 7.4, the RHEL repository no longer distributes the OpenLDAP \nserver.  The default LDAP server for RHEL 7.5 onwards is the IPA (Identity, Policy, Audit) server. The \nIPA server only supports a flat LDIF structure. It does not support `ou` \nso you can not use the IPA server as the external LDAP server for NOI.  This \ninformation can be found in the \n[Red Hat Solutions](https://access.redhat.com/solutions/4172491) documentation.\n\n\n</InlineNotification>\n\n\n\n<a name=\"nom-download\"></a>\n\n## Downloading the Netcool Ops Manager software\n\n\nThe eAssemby for Netcool Ops Manager is \n\nPartNo  | Description |  Size | Number of images\n--- | --- | --- | ---\nCJ5MZEN  | IBM Netcool Operations Insight v1.6.0.3 Cloud Paks Multiplatform English eAssembly | 30 GB | 7\n\nTo install ASM and NOI, you need to download the following components \nof the above eAssembly. \n\nPartNo  | Description |  Size | File name\n--- | --- | --- | ---\nCC5J3ML     |  Common Services for 3.2.4 IBM Netcool Operations Insight 1.6.0.3 Multilingual | 7.4 GB |COM_SVRCE3.2.4_NOI_1.6.0.3_ML.tar.gz\nCC686ML         | IBM Netcool Agile Service Manager v1.1.7 - Containers for IBM Cloud Private with Red Hat Enterprise Linux OpenShift Multilingual| 3.3 GB| NOI_ASM_V1.1.7_FOR_ICP_ML.tar.gz\nCC5IZML    | IBM Netcool Operations Insight 1.6.0.3 Operations Management - Containers for IBM Cloud Private with Red Hat Enterprise Linux OpenShift Multilingual | 13.1 GB | NOI_V1.6.0.3_OM_FOR_ICP_ML.tar.gz \n\n\n\n\n<a name=\"nom-common\"></a>\n\n## Ensuring that IBM Cloud Platform Common Services is available  \n\nIf you are installing into an Openshift cluster with Cloud Pak for MCM \nalready installed, then skip the next few steps and go to the \n[Installing Agile Service Manager](#nom-asm)\nsection, otherwise install IBM Common Services as described next.\n\n\n<a name=\"nom-registry\"></a>\n\n## Exposing OCP External Registry Access\n\nBy default, OCP does not expose its Image registry to external access. \nYou need to patch the image registry to create a default route. As a route, \nthe default route allows external acess to the OCP image registry. You need \nthis to load the image from your bastion server.\n\n```\noc patch configs.imageregistry.operator.openshift.io/cluster --patch '{\"spec\":{\"defaultRoute\":true}}' --type=merge\n```\nIf your OCP cluster certificate is self signed, then you need the client to \ndownload the certificate to trust.\nCopy the image registry certificate to your bastion server by performing the \nfollowing steps:\n\n\n```\nDOCKER_REGISTRY=\"$(kubectl get route -n openshift-image-registry default-route -o jsonpath='{.spec.host}')\"\nmkdir -p /etc/docker/certs.d/$DOCKER_REGISTRY\nopenssl s_client -showcerts -servername $DOCKER_REGISTRY -connect $DOCKER_REGISTRY:443 2>/dev/null | openssl x509 -inform pem > /etc/docker/certs.d/$DOCKER_REGISTRY/ca.crt\n```\n\nTest the image registry default route by performing a docker login.\n```\ndocker login -u $(oc whoami) -p $(oc whoami -t) ${OCP_REGISTRY}\n```\n\n<InlineNotification>\n\nNote that for the above `docker login` to work, you must first perform an \n`oc login` using a token. You do this by copying \nthe `oc login` with the token from the \nOCP Web Interface and pasting and running it from your command line. \nIf you perform an `oc login` using the certificate defined \nthrough the `KUBECONFIG` environment variable, then the token will be empty, and \nyour docker login will fail. \n\n</InlineNotification>\n\n\n\n\n<a name=\"nom-inception\"></a>\n\n## Configuring the icp-inception at the bastion workstation\n\nYou install IBM Common Services by configuring and running the \nicp-inception on docker.  Start by loading the IBM Common Services downloaded \npackage to the bastion server.\n\n```\ntar xf common-services-boeblingen-2002-x86_64.tar.gz -O | sudo docker load\n```\n\nCreate the cluster configuration from the icp-inception.\n\n```\nmkdir cluster\nsudo docker run --rm -v $(pwd)/cluster:/data -e LICENSE=accept ibmcom/icp-inception-amd64:3.2.4 cp cluster/config.yaml /data/config.yaml\n```\n\nEdit the cluster configuration file. \nYou need to replace the place holder in the section below. Specify the other \noptional components in the configuration file as required.\n\n```\ncluster_nodes:\n  master:\n    - <REPLACE_WITH_ONE_OF_YOUR_WORKER_NODE_NAME>\n  proxy:\n    - <REPLACE_WITH_ONE_OF_YOUR_WORKER_NODE_NAME>\n  management:\n    - <REPLACE_WITH_ONE_OF_YOUR_WORKER_NODE_NAME>\n\nstorage_class: <REPLACE_WITH_YOUR_STORAGE_CLASS>\n\ndefault_admin_user: <REPLACE_WITH_YOUR_ADMIN_USER>\ndefault_admin_password: <REPLACE_WITH_YOUR_ADMIN_USER_PASSWORD>\npassword_rules:\n  - '(.*)'\n```\n\n\nCopy the kubeconfig file that the OCP installation created, to the cluster directory.  Now you are ready to perform the install.\n\n\n\n\n<a name=\"nom-run\"></a>\n\n## Running the icp-inception to install IBM Common Services\n\nChange directory to the parent directory of the `cluster` directory, and run \nthe `icp-inception` playbook.\n\n```\nsudo docker run --net=host -t -e LICENSE=accept -v \"$(pwd)/cluster\":/installer/cluster -v /var/run/docker.sock:/var/run/docker.sock ibmcom/icp-inception-amd64:3.2.4 addon -v\n```\n\n<InlineNotification>\n\n** Note:** If you specify the `default` storage class in the config.yaml file\n(this is the default value), and it does not exist, the pod `icp-mongodb-0` \nwhich is part of `icp-mongodb` stateful set will not be able to continue, as \nit can not create its Persistent Volume Claim (PVC). \nTherefore, it is recommended that you explicitly specify the \nstorage class name in the config.yaml file, rather than using 'default'.\n\n</InlineNotification>\n\n\nMore information on configuring and running the IBM Common Service can be found in the [IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/csd/installer/3.2.2/installation-NOI.html)\n\n\n\n<a name=\"nom-auth\"></a>\n\n## Configuring IBM Common Services Authentication\n\nWhile optional at this stage of the installation, if you use external LDAP, \nwe recommend to verify that the connection is working.\n\nOnce the icp-inception is completed successfully, your IBM Common Services \nshould be up. Get the URL to your IBM Common Services, by checking the OCP \nroute.\n```\noc get route -n kube-system\n```\n\nYou are looking for a route name starting with `icp-console`. The fully \nqualified domain name will look like \n`icp-console.apps.<YOUR_OCP_BASE_DNS_NAMES>`.\n\nLog in to IBM Common Services using the URL and the user name and password \nthat you specified in the config.yaml file.\n\nSet up your LDAP connection, and create the user by following the instructions \nin the \n[MCM Knowledge Center](https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.3.0/iam/3.4.0/configure_ldap.html).\n\n\n\n\n<a name=\"nom-client\"></a>\n\n## Getting the cloudctl and helm clients from IBM Common Services\n\nGet your cloudctl client, log in to the IBM Common Services dashboard, \ncopy the curl command to download cloudctl to your bastion server, paste it \nto your command line, and then run the curl command. Change the mode of the \ncloudctl file to executable, and then move them to a directory in your \ncommand search path.  Verify that cloudctl is available by running it with a \nversion switch.\n\n```\ncurl -kLo cloudctl-linux-amd64-v3.2.4-1675 https://icp-console.apps.csmo-noi.ocp.csplab.local:443/api/cli/cloudctl-linux-amd64\nchmod 755 cloudctl-linux-amd64-v3.2.4-1675 \nmv cloudctl-linux-amd64-v3.2.4-1675 /usr/local/bin/cloudctl\ncloudctl version\n```\n\nDo a similar thing to download the helm client.\n\n```\ncurl -kLo helm-linux-amd64-v2.12.3.tar.gz https://icp-console.apps.csmo-noi.ocp.csplab.local:443/api/cli/helm-linux-amd64.tar.gz\nmkdir helm\ntar xvzf helm-linux-amd64-v2.12.3.tar.gz -C helm\nmv helm/linux-amd64/helm /usr/local/bin\n```\n\nInitialize your helm client. \n\n```\nexport HELM_HOME=~/.helm\nhelm init --client-only\n```\n\nLog in to the IBM Common Services dashboard, copy the login token, and \nlog in to IBM Common Services from the command line using `cloudctl`.  \nYou need to perform a cloudctl login at least once, as it creates the \nnecessary certificate to access helm in the user's `$HOME/.helm` directory. \nVerify that the helm CLI is configured correctly by running a `helm version`\ncommand.  \n\n\n```\ncloudctl login -a https://icp-console.apps.<OCP_CLUSTER> -u <ICP-USER> -n kube-system --skip-ssl-validation\nhelm version --tls\n```\n\nYou might also want to configure the `HELM_HOME` variable\nin your login profile.\n\nMore information on configuring the `cloudctl` and `helm` command line \ninterfaces can be found in the \n[MCM Knowledge Center](https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.3.0/cli/cli_guide_mcm.html)\ndocumentation.\n\n\n\n\n<a name=\"nom-asm\"></a>\n\n## Installing Agile Service Manager and Netcool Operations Insight\n\nThroughout this section the following resource sample values are used:\n\nResource | Sample Value\n--- | ---\nNamespace for ASM and NOI | noins\nRelease name for ASM | asmrel1\nRelease name for NOI | noirel1\nStorage Class name   | rook-ceph-block\nPassword for NOI     | Netcool2020\n\n\nPlease change the sample values if necessary during your installation.  \n\n### Creating the namespace\n\nCreate a namespace (also referred to as project in OCP) for your NOI and ASM \ninstallations.\n\n```\noc new-project noins\n```\n\n<InlineNotification>\n\n** Note**: NOI and ASM must be installed in the same namespace.\n\n</InlineNotification>\n\n\n#### Day 2 Operations: Pod Placement / Node Selection\n\nAs a Day 2 tip: It is recommended to create a pod placement rule to schedule \nall the NOI and ASM pods to run on a selection of worker nodes. One way to \ndo this is to define a **Node Selection** policy by labeling the target worker \nnodes and then telling the scheduler to run the pods only on those worker \nnodes. All NOI and ASM pods are running in the same namespace. Thus, rather \nthan specifying the target label on each pod, it is more manageable to \nspecify the node selection on the namespace.\n\nLabel the target worker nodes. If the name of the worker nodes that run the \nASM and NOI pods is `worker1` to `worker6`, then the worker nodes can be assigned \na label `nodetorun=noiasm` as follow (you can use your name-value pair):\n\n```\nfor i in worker1 worker2 worker3 worker4 worker5 worker6\ndo \noc label nodes $i nodetorun=noiasm\ndone\n```\n\nYou can then tell the scheduler to run all pods in the namespace `noins` by \nadding the label to the annotations sections of the namespace.  You can \nmodify the annotations by executing the following:\n\n```\noc edit ns noins\n```\n\nInsert a line specifying the node selector:\n```\n    openshift.io/node-selector: nodetorun=noiasm\n```\n\nThe following is an example of the edit namespace command, with the \nnode-selector already added at the end of the annotations section:\n```\n[root@bootstrap common]# oc edit ns noins\nnamespace/noins edited\n---------\n# Please edit the object below. Lines beginning with a '#' will be ignored,\n# and an empty file will abort the edit. If an error occurs while saving this file will be\n# reopened with the relevant failures.\n#\napiVersion: v1\nkind: Namespace\nmetadata:\n  annotations:\n    openshift.io/description: NOI Name Space\n    openshift.io/display-name: noins\n    openshift.io/requester: cadmin\n    openshift.io/sa.scc.mcs: s0:c25,c0\n    openshift.io/sa.scc.supplemental-groups: 1000600000/10000\n    openshift.io/sa.scc.uid-range: 1000600000/10000\n    openshift.io/node-selector: nodetorun=noiasm\n  creationTimestamp: \"2020-05-01T07:42:33Z\"\n  name: noins\n  resourceVersion: \"8192651\"\n  selfLink: /api/v1/namespaces/noins\n  uid: 7312be5a-ef8c-42f7-a3df-1e72ecb9b16e\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: Active ~                 \n```\n\n\n\n### Assigning your release name\n\nYou need to specify a release name when you install the NOI and ASM charts. \nNOI and ASM come as separate charts, so you need different release names for \nthe NOI deployment and ASM deployment.  \n\nThe release name has to begin with a lowercase letter and end with an \nalphanumeric character. The release name can only contain hyphens and \nlowercase leters.\n\nDuring the installation of NOI, you need to specify the ASM release name.  \nSimilarly, during the installation of ASM, you need to specify the NOI \nrelease name. Hence it is best to decide both release names before you start \nthe installation.\n\nWe are using `asmrel1` and `noirel1` in the ongoing example.  Please change \nthem to your choice.\n\n\n### Loading the ASM image.\n\nLog in to IBM Common Services from the command line as a `cluster-admin` user. \nLoad the ASM image to the IBM Common Platform Catalog repository, by running \nthe following command from the directory where the \n`NOI_ASM_V1.1.7_FOR_ICP_ML.tar.gz` image is located. \n\n```\nOCP_REGISTRY=$(oc get route -n openshift-image-registry default-route -o jsonpath='{.spec.host}')\ndocker login -u $(oc whoami) -p $(oc whoami -t) ${OCP_REGISTRY}\ncloudctl catalog load-archive --archive NOI_ASM_V1.1.7_FOR_ICP_ML.tar.gz --registry ${OCP_REGISTRY}/noins\n```\n\n<InlineNotification>\n\n** Note**: During the loading of the image to the image registry, you need to \nhave enough space in /tmp and /var/lib/docker. If your /var mount point is \ntoo small, you can tell docker to use a different directory. \nPlease refer to the earlier [section](#nom-vardocker). \n\n</InlineNotification>\n\n\n### Creating the Cluster Role\n\nASM comes with several scripts that you need to execute before you start the \ninstallation.\n\nUnpack the ASM download then execute the scripts to create the cluster role \nand the create Security NameSpace:\n\n```\ntar xvzf NOI_ASM_V1.1.7_FOR_ICP_ML.tar.gz \ncd pak_extensions/pre-install/clusterAdministration\nbash createSecurityClusterPrereqs.sh\nbash createSecurityNamespacePrereqs.sh noins\n```\n\n### Creating the PVC\n\nEdit the `storageConfig.env` file located in the `clusterAdministration` \ndirectory.  Edit the `storageConfig.env`, and specify the 3 worker nodes. \nIf you want to use the Local Storage then run the PVC creation scripts.\n\n```\nbash createStorageVolumes.sh noins asmrel1\n```\n\nThe scripts will print out the additional actions that you need to perform.  \n\n\n#### Creating the  PVC using Rook/Ceph\n\nAs mentioned in the [OCP Storage](#nom-storage) section at the beginning of this \nchapter, you can use the OCS or Rook/Ceph dynamic storage. To do this, you \nneed to change the yaml part of the script.\n\nMake a backup of the `kubhelper.sh` script and then edit it.\n\n```\ncd ../../common\ncopy kubhelper.sh kubhelper.sh.bak\nvi kubhelper.sh\n```\n\nSearch for the line starting with EOPV, and change the content between the \ntwo EOPVs, as follows:\n(Change the `rook-ceph-block` to your storage class name).\n\n\n```\ncat <<EOPV | ${CMD} apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ${4}\n  namespace: ${3}\n  labels:\n    release: ${2}\n  finalizers:\n  - kubernetes.io/pvc-protection\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: ${5}Gi\n  storageClassName: rook-ceph-block\nEOPV\n```\n\nRun the script as before.\n\n```\ncd ../pre-install/clusterAdministration\nbash createStorageVolumes.sh noins asmrel1\n```\n\nVerify that the PV and PVC are created:\n\n\n```\noc project noins\noc get pv\noc get pvc\n```\n\n<InlineNotification>\n\n**Note**: The prefered PVC reclaim policy of ASM is the **Retain** option.  If your default storage class reclaim policy is not \"Retain\" then you can patch the PV by using this command: \n\n`oc patch pv <PV_NAME>  -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}'`\n\n</InlineNotification>\n\n\n\n### Installing Agile Service Manager from the catalog\n\nThe product documentation recommends installing Agile Service Manager before installing Netcool Operation Insight.  \n\nBrowse the IBM Common Services URL as a `cluster-admin` user, \nselect the catalog, and enter \"netcool\" in the search bar. The ASM chart \nshould be shown.  Select the chart and go through the chart configuration.\n\nThere are 2 configuration options that you need to be careful with:\n\nConfiguration Options   |   Values\n--- | ---\nRepository | image-registry.openshift-image-registry.svc:5000/noins\nRouter domain | apps.<YOUR_OCP_CLUSTER_NAME>\n\n\n<InlineNotification>\n\n**Note**: If you use the cloudctl catalog load command to load the ASM image, \nthen the repository is pre-populated with the external docker registry name. \nYou need to change this to the internal docker registry name. If you do not \nchange this to the internal registry name and you configured OCP using a \nself-signed certificate, then the installation of ASM will not even start, as the \nhelm process can not pull the image from the registry.\n\n</InlineNotification>\n\n\n\n#### ASM File Observer\n\nIf you choose to install the ASM File Observer during the initial installation of ASM, you may notice that the pod (`asmrel1-file-observer-*`) is in *Pending* mode.  This may be caused by the PVC for the pod being released and since the reclaim policy is \"Retain\" it can not be bound again. Should this happen, then delete and recreate the PVC `data-noirel1-file-observer`.  The following yaml file might be helpful.  Remember to change the `namespace`, `release`, and `storageClassName` to your configuration.  Note that deleting and recreating the PVC will wipe out the content of the file system in the PVC.  Do not do this after the initial install of ASM, unless you are ok with the removal of the content of the file system.\n\n```\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-asmrel1-file-observer \n  namespace: noins\n  labels:\n    release: asmrel1\n  finalizers:\n  - kubernetes.io/pvc-protection\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: rook-ceph-block\n```\n\nMore information on installing Agile Service Manager can be found in the [IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SS9LQB_1.1.7/welcome_page/kc_welcome-444.html).\n\n\n\n\n<a name=\"nom-noi\"></a>\n\n## Installing Netcool Operations Insight\n\n\n### Loading the NOI Catalog\nYou start the installation by loading the NOI image.  Unlike in ASM, for NOI, \nyou have to unpack the downloaded image first.\n\n```\ntar xvzf /home/netcool/NOI_V1.6.0.3_OM_FOR_ICP_ML.tar.gz \ncloudctl catalog load-archive --archive ibm-netcool-prod-2.1.3-x86_64.tar.gz --registry default-route-openshift-image-registry.apps.<OCP_CLUSTER_BASE_DNS>/noins\n```\n\n### Creating the Service Account\n\nDepending whether you will be running the helm chart as cluster admin or not, \nyou need to create the service account and assign the permissions to it. \nIf you are unsure, perform the following. \n(Change `noins` to your namespace):\n\n```\noc create serviceaccount noi-service-account -n noins\noc adm policy add-scc-to-user ibm-privileged-scc system:serviceaccount:noins:noi-service-account\noc create role noiservice-reader --verb=update --verb=patch --verb=get --verb=list --verb=watch --resource=services -n noins\noc create rolebinding noiservice-reader --role noiservice-reader --serviceaccount noins:noi-service-account -n noins\noc create role noiconfigmap-reader --verb=update --verb=patch --verb=get --verb=list --verb=watch --resource=configmaps -n noins\noc create rolebinding noiconfigmap-reader --role noiconfigmap-reader --serviceaccount noins:noi-service-account -n noins\noc create role noisecret-creater --verb=list --verb=create --verb=get --resource=secrets -n noins\noc create rolebinding noisecret-creater --serviceaccount noins:noi-service-account --role noisecret-creater -n noins\noc create role releasename-redis --verb=get --verb=list --verb=update --verb=patch --verb=watch --resource=pods -n noins\noc create rolebinding releasename-redis --serviceaccount noins:noi-service-account --role releasename-redis -n noins\noc create role noiroute-reader --verb=update --verb=patch --verb=get --verb=list --verb=watch --resource=routes --resource=routes/custom-host -n noins\noc create rolebinding noiroute-reader --role noiroute-reader --serviceaccount noins:noi-service-account -n noins\n```\n\nVerify that you get a **yes** to all of the following commands:\n\n```\nkubectl auth can-i patch services --namespace noins --as system:serviceaccount:noins:noi-service-account \nkubectl auth can-i watch configmaps --namespace noins --as system:serviceaccount:noins:noi-service-account\nkubectl auth can-i create secrets --namespace noins --as system:serviceaccount:noins:noi-service-account\nkubectl auth can-i create secrets --namespace noins --as system:serviceaccount:noins:noi-service-account\nkubectl auth can-i watch pods --namespace noins --as system:serviceaccount:noins:noi-service-account\nkubectl auth can-i patch routes --namespace noins --as system:serviceaccount:noins:noi-service-account\nkubectl auth can-i patch routes/custom-host --namespace noins --as system:serviceaccount:noins:noi-service-account\n```\n\n### Creating the passwords for the different NOI components.  \n\nPerform the following steps if you want to choose the passwords for the NOI installation.\nIf you do not do this, then the passwords are created for you. \nThe password for Cassandra is already created during ASM install, so you will \nsee a warning when you perform the following. \nThe creation of the Cassandra secret is still included here for completeness. \nChange the `noins` to your namespace, and change the `Netcool2020` sample \npassword to your preferred password.  \n\n```\noc create secret generic noirel1-icpadmin-secret --from-literal=ICP_ADMIN_PASSWORD=Netcool2020 --namespace noins\noc create secret generic noirel1-impact-secret --from-literal=IMPACT_ADMIN_PASSWORD=Netcool2020 --namespace noins\noc create secret generic noirel1-la-secret --from-literal=UNITY_ADMIN_PASSWORD=Netcool2020 --namespace noins\noc create secret generic noirel1-ldap-secret --from-literal=LDAP_BIND_PASSWORD=Netcool2020 --namespace noins\noc create secret generic noirel1-omni-secret --from-literal=OMNIBUS_ROOT_PASSWORD=Netcool2020 --namespace noins\noc create secret generic noirel1-was-secret --from-literal=WAS_PASSWORD=Netcool2020 --namespace noins\noc create secret generic noirel1-couchdb-secret --from-literal=password=Netcool2020 --from-literal=secret=couchdb --from-literal=username=root --namespace noins\noc create secret generic noirel1-systemauth-secret --from-literal=password=Netcool2020 --from-literal=username=system --namespace noins\noc create secret generic noirel1-ibm-hdm-common-ui-session-secret --from-literal=session=Netcool2020 --namespace noins\noc create secret generic noirel1-cassandra-auth-secret --from-literal=username=hdm --from-literal=password=Netcool2020 --namespace noins\noc create secret generic noirel1-ibm-redis-authsecret --from-literal=username=redis --from-literal=password=Netcool2020 --namespace noins\noc create secret generic noirel1-kafka-admin-secret --from-literal=username=kafka --from-literal=password=Netcool2020 --namespace noins\noc create secret generic noirel1-kafka-client-secret --from-literal=username=admin --from-literal=password=Netcool2020 --namespace noins\n```\n\n<InlineNotification>\n\n**Note**: At the time of this writing, the online manual mistakenly creates a \npassword for `noirel1-impactadmin-secret` rather than `noirel1-impact-secret`. \nThe name has been corrected in the above command.\n\n</InlineNotification>\n\n\n### Deploying the NOI helm chart\n\nLog in to the IBM Common Services GUI, select Catalog, type \"netcool\" in the \nsearch fields, and select the NOI Chart.\n\n#### Specifying the configuration.\n\nThe following are some notes on the configuration.\n\nConfiguration  | Suggested Values  | Description\n--- | --- | ---\nHelm release name |    noirel1 | Replace this with your NOI release name, it can not be the same with the ASM release name.\nTarget namespace |    noins | The namespace where you did all the previous service account and password creation.  It should be the same as the ASM namespace.\nTarget Cluster     | local-cluster | Unless you use MCM to deploy to a remote cluster.\nPod Security    | ibm-priviledged-psp | You have configured this earlier. \nMaster Node    | `apps.<THE-OCP-CLUSTER-BASE-DOMAIN-NAME>` | The OCP service base name.\nHttps Port    | 443 | The default 443 should be good unless you have a specific network config.\nImage repository |    `image-registry.openshift-image-registry.svc:5000/noins` | Change this from the default external repository.  Change noins to your namespace.\nDocker image repository secret    | Leave Blank | Leave it blank unless you want to use a secret that you have created previously.\nEnvironment Size |    Size1 | For POC change to Size0\nServiceAccount under which your pods run |    noi-service-account | Pre-filled, you have created this earlier.\nCreate required RBAC RoleBinding |    check | The default is not checked.\nUse existing TLS certificate secrets |    check | The default is not checked.\nIndicate that all password secrets have been created prior to install |    check | Unless you want the installer to generate the secrets.\nEnable sub-chart resource requests |    check | default. Enabling this will use more resources. You might want to uncheck this for POC.\nEnable anti-affinity |    check | default.  For resilience, always check this.\nEnable data persistence    | check | Unless you are doing a POC and do not want to persist the data.\nUse dynamic provisioning |    check |  If you do not use the dynamic provisioning, then you need to create the PVC first.\nNumber of Impact server instance    | 2 | (default is 1).  \nLDAP Mode |    standalone | ** Note:** If you specify standalone, **do not change** any of the LDAP configurations after this.  Only specify the details of the LDAP for proxy.\nEnable ASM Integration | check | Check, unless you want to install only NOI.\n\n\n#### Monitoring the deployment.\n\nWatch for the pods. \n\n```\nwatch -n15 oc get pods -n noins\n```\n\nOnce the pods are all up, then you have completed the installation.\n\n### **A Few Common Issues**\n\nSome common issues during the deployment include:\n\n- **Not enough compute (CPU, Memory) resources**.  The impact pods (`*-nciserver-*`) are initialized last. \nIt also requires a comparatively larger compute resource. If your impact \npods are not running, then run the `oc describe pods <POD_NAME>` command to check what caused it.\n- **Not enough storage**.  If you do not have enough Disk Storage, then the \nPVCs might not all be successfully created.  You should be able to see this \nalso using the `oc describe pods <POD_NAME>` command.\n\n\nMore information on installing Netcool Operation Insight can be found in the [IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/soc/integration/task/int_installing-opsmg-rhocp_ui.html).\n\n\n<a name=\"nom-post\"></a>\n\n## Post-installation steps\n\nIf you install Agile Service Manager after Netcool Operation Insight, you must restart some services. If the Agile Service Manager secrets are changed, you must also restart some services.  This is described in [the IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/soc/integration/task/int_installing-asm-rhocp.html)\n\n\n### Helm release details\n\nGet the helm details of the ASM release.\n\n```\nhelm status asmrel1 --tls\n```\n\nFrom the output, you can see useful information about the resources created \nby the Helm install as well as some useful commands.\n\nSimilarly, you can check the helm status of the NOI Release.\n\n```\nhelm status noirel1 --tls\n```\n\nThe following lists some of the information provided by the above helm status \ncommands.\n\nComponent   | URL\n--- | ---\nWebGUI | `https://netcool.noirel1.apps.<OCP_CLUSTER>:443/ibm/console`\nWAS | `https://was.noirel1.apps.<OCP_CLUSTER>:443/ibm/console`\nLog Analytics | `https://scala.noirel1.apps.<OCP_CLUSTER>:443/Unity`\nImpact | `https://impact.noirel1.apps.<OCP_CLUSTER>:443/ibm/console`\n\n\n\n### Assigning roles\n\nLog in to DASH and assign the user or group roles. \nYour ASM and NOI base system is now installed and ready.\n\nMore information on administering users can be found in the [IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/soc/admin/task/adm_administering-users.html)\n\n\n[Back to NOI current version - 1.6.1](/mcm/cp4mcm_netcool_ops_manager)","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/mcm/cp4mcm_netcool_ops_manager/noi_1603/index.mdx"}}}}