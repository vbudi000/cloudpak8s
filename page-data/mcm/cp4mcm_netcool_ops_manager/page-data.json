{"componentChunkName":"component---src-pages-mcm-cp-4-mcm-netcool-ops-manager-index-mdx","path":"/mcm/cp4mcm_netcool_ops_manager/","result":{"pageContext":{"frontmatter":{"title":"MCM - Netcool Ops Manager (NOM) - Installation Guide","description":"Installation guide for installing CP4MCM Netcool Ops Manager optional components","keywords":"ibm,install,mcm, cp4mcm, nom, netcool_ops_manager"},"relativePagePath":"/mcm/cp4mcm_netcool_ops_manager/index.mdx","titleType":"page","MdxNode":{"id":"dfcd4b37-004c-5a02-b278-9c6e932f0a7e","children":[],"parent":"4592ee25-e758-5698-878e-18768c2db549","internal":{"content":"---\ntitle: MCM - Netcool Ops Manager (NOM) - Installation Guide\ndescription: Installation guide for installing CP4MCM Netcool Ops Manager optional components\nkeywords: 'ibm,install,mcm, cp4mcm, nom, netcool_ops_manager'\n---\n\n## **Solution Overview**\n\nNetcool Ops Manager (NOM) is an optional component of Cloud Pak for Multicloud \nManagement. Netcool Ops Manager consists of multiple parts. The components are:\n- IBM Netcool Operations Insight (NOI)\n- IBM Agile Service Manager (ASM)\n- IBM Cloud Event Management (CEM)\n- IBM Predictive Insights (PI)\n\nThe components can be installed on Prem, on the cloud, or in a \nhybrid installation.  This playbook is \nwritten for the Cloud Pak for Multicloud Management (MCM), \nso the focus is on installing Netcool Ops Manager on \nOpenShift Container Platform (OCP), which can run on Prem or in the cloud. \n\nThe targetted audience for this section of the playbook is technical \nsales or technical services engineers who need a unified guide to install \nNetcool Ops Manager.\n\nThe current version 1.6.1 of NOI changes the installation method to use the `Operator.`  It no longer requires the IBM Common Platform (ICP) Common Service.  NOI 1.6.0.3 uses ICP.  Because of this, you can not perform an upgrade from 1.6.0.3 to 1.6.1.\n\n## Previous Version\n\nThe previous version installation instruction is desribed in \n[The installation instruction for NOI 1.6.0.3 including ASM 1.1.7](/mcm/cp4mcm_netcool_ops_manager/noi_1603) section.\n\n## The Operator Lifecycle Management\n\nThe NOI 1.6.1 can be installed online by using the Operator Lifecycle Management(OLM) or offline by downloading the container software first and then pushing it to the OpenShift Cluster.  The latter method is typically referred to as the CLI (Command Line Interface) method, as you need to use the CLI to perform the installation.\n\n## Installation Steps.\n\nThe following are the steps to install NOI 1.6.1:\n\n![NOI 1.6.1 OLM installation flow](./images/noi161_olm_flow.png \"NOI 1.6.1 OLM installation flow\")\n\n\n1. [Obtain your ICR entitlement key](#nom-icr)\n1. [Prepare the Openshift cluster](#nom-ocp)\n1. [Prepare the installation workstation](#nom-bastion)\n1. [Prepare the LDAP server](#nom-ldap)\n1. [Create the Openshift resources](#nom-rss)\n1. [Create the application secret](#nom-pwd)\n1. [Install the Operator](#nom-op)\n1. [Create the NOI Instance](#nom-ins)\n1. [Post installation steps](#nom-post)\n\nEach step is detailed as follows:\n\n\n<a name=\"nom-icr\"></a>\n\n## Obtain your ICR entitlement key\n\nWhen installing through the Operator Lifecycle Management, the container will be downloaded directly from the `cp.icr.io` (IBM Cloud Container Registry).   You first need to get your entitlement key from the IBM Cloud Container Registry. Once you got your entitlement key, you can create a Kubernetes secret.  You specify the secret name in the Operator.  This step is described later.\n\nFor IBM Staff, please go to `My IBM Entitlement` site to get your internal entitlement key.\n\n<a name=\"nom-ocp\"></a>\n\n## Preparing the Openshift cluster\n\nThe [Official documentation](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.1/com.ibm.netcool_ops.doc/soc/integration/concept/deploying_on_rhocp.html) of NOI 1.6.1 mentioned that NOI 1.6.1 is supported to run on:\n- OpenShift Container Platform version 4.3\n- OpenShift Container Platform version 4.4.3 \n\nA later version of 4.4.x seems to work; in fact, Specifying Custom Resource using a form is only supported on 4.4.6 or newer.\n\nIf you install on OCP 4.3 or 4.4.5 (or earlier), you need to specify `the Custom Resource (CR)` using the YAML.  On OCP 4.4.6 or above, you have the option to specify the CR using a form.  In this playbook, we will describe the custom resource using YAML.\n\n### Sizing\nThe online documentation (links provided below)\nprovides sizing guidelines.  Separate sizing \nguidelines are available depending on whether you are installing for a \n__trial__ (PoC) or \nfor a __production__ environment.\n\n#### CPU and Memory\nRecommended minimal worker nodes sizing:\n\nDescription | Quantity\n--- | ---\nNumber of worker nodes | 5\nNumber of vCPUs per worker node | 16 \nMinimum memory per worker node | 32 GB\n\nInformation on sizing can be found at the following sites:\n[Sizing Guidelines](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.1/com.ibm.netcool_ops.doc/soc/integration/reference/soc_sizing_full.html)\n\n#### Storage Capacity\n\nIf you are installing into OCP, then **Rook/Ceph** or \n**Openshift Container Storage (OCS)**, with **RADOS Block Device (RBD)** storage class, is the default supported OCP Storage solution.  We recommend Rook/Ceph \nas the dynamic storage solution for Netcool Ops Manager.\n\n**Image Registry Storage**: As you are preparing the OCP cluster, If you are going to install through CLI, please allow at least 150GB of image-registry storage for the Netcool Ops Manager. This capacity is in addition to your image-registry requirements for your other OCP's purpose. \n\n**Persistent Volume Claim**: If you are deploying the Openshift Container Storage (OCS), then OCS \ncreates a default 2 TB Rook/Ceph/RDB block storage.  For an initial production installation of Netcool Ops Manager, you need about 800 GB of storage (PVC) space and image-registry storage.  Please take note of the storage class name. You need this later during the installation.\n\nIf you need help with installing your OCP environment, please see this \nplaybook's section on [installing OpenShift](../../ocp/introduction/).\n\nIf other team members configured your OCP cluster, then please ensure that they provide you with an account with a cluster administrator role.\n\n\n<a name=\"nom-bastion\"></a>\n\n## Preparing the installation workstation\n\nYou will need to use your local workstation web browser to access the OCP Web Interface, install the Operator, and create the NOI Custom Resource.\n\nYou will also need the OCP client to help you with the installation.\n\n### Getting the oc and kubectl command lines\nYou download oc and kubectl from your OCP cluster. The kubectl executable is \na symbolic link of the oc executable.  The following \n[documentation from Red Hat](https://docs.openshift.com/container-platform/4.3/cli_reference/openshift_cli/getting-started-cli.html) \ndescribes the steps to get started with the `oc` and `kubectl` command line \ninterface.\n\n<a name=\"nom-ldap\"></a>\n\n## Preparing the LDAP server\nYou need to provide details of your LDAP server for the following components:\n- OCP Cluster\n- NOI Proxy configuration.\n\nSetting up your LDAP server is a common requirement across all  Cloud Paks, \nso it is not detailed here.  \n\nDuring the installation, you will need to specify the following information, so \nget the information before you start the helm chart configuration:\n\n- Your Base Distinguished name.\n- Your LDAP URL.\n- Your LDAP Bind User Name and Password.\n- The filter to get the user information (User Filter and User ID map).\n- The filter to get the group information (Group Filter and Group ID map).\n- The filter to map a user to a group (Group member ID map).\n\nOne of the pods deployed by the NOI Helm Chart is an OpenLDAP pod. You can choose to set up the OpenLDAP as a standalone repository or as a proxy to an external LDAP server.  If you choose to use the OpenLDAP pod as a proxy, then Netcool Ops Manager expects the external LDAP server to support the hierarchical LDAP structure.  In particular, the NOI LDAP configuration wants to use ou (Organizational Unit).  \nMore information on the NOI Proxy LDAP requirement can be found in the \n[IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/soc/admin/reference/managing_users_using_an_external_ldap_server.html).\n\n<InlineNotification>\n\nNote that since RHEL 7.4, the RHEL repository no longer distributes the OpenLDAP server.  The default LDAP server for RHEL 7.5 onwards is the IPA (Identity, Policy, Audit) server. The \nIPA server only supports a flat LDIF structure. It does not support `ou`, so you can not use the IPA server as the external LDAP server for NOI.  This information can be found in the \n[Red Hat Solutions](https://access.redhat.com/solutions/4172491) documentation.\n\n\n</InlineNotification>\n\n### User in LDAP.\n\nIf you are using an external LDAP server, then create the following user in the external LDAP:\n- __smadmin__ - The administrative user for the dashboard.\n- __impactadmin__ - The administrative user for Netcool/Impact.\n- __icpadmin__ - The default ICP admin user\n- __icpuser__ - The default ICP standard user\n\n\n<a name=\"nom-rss\"></a>\n\n## Create the Openshift resources\n\nYou need to create the following Openshift resource for the Operator.\n- namespace\n- Custom Resource name\n- Service Account Registry Secret.\n- Service Account\n\n### namespace\nCreate the namespace for the NOI installation.\nIf you decide your namespace to be `noi161ns` then perform the following:\n\n```\noc new-project noi161ns\n```\n\n### Custom Resource name\n\nAll your pods will be prefixed with your custom resource name, so choose something short.  For example, `noicr`.\n\n### Service Account Registry Secret.\n\nYou create a secret containing your entitlement key described earlier.  \n\n```\noc create secret docker-registry noi-registry-secret --docker-server=cp.icr.io --docker-username=cp --docker-password=\"your entitlement key from the first step above\"\n```\n\nYou specify the `noi-registry-secret` into the service account (next step) and into a custom resource (later).\n\n\n### Service Account\n\nIt is recommended to use the suggested `noi-service-account`, perform the following:\n\n```\noc create serviceaccount noi-service-account -n noi161ns\noc adm policy add-scc-to-user privileged system:serviceaccount:noi161ns:noi-service-account\noc patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"noi-registry-secret\"}]}'\n```\n\n<a name=\"nom-pwd\"></a>\n\n## Create the application secret\n\nIf you are using the internal OpenLDAP, then this step is optional.  If you do not specify the secret, then the password will be created for you.  You can get the password post-install from the Kubernetes secret.\n\nIf you are going to use the external LDAP, then you need to specify the password for:\n- LDAP user\n- smadmin\n- impactadmin\n- icpadmin\n\nThe password should match each user's password in the external LDAP.\n\nIf you want to use a friendly password, you can either create the secret before installing or change the password after the installation.\n\nThe details of specifying the password through Kubernetes secret are described in the [Configuring Authentication](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.1/com.ibm.netcool_ops.doc/soc/integration/task/int-creating_passwords_and_secrets-rhocp.html) section of the online document.\n\nFor your convenience, the following list the command to specify the password `Netcool2020` to everything, including the internal LDAP pod.  Note there is an additional line for impact as the secret name should be `custom-resource-impact-secret` rather than the documented `custom-resource-impactadmin-secret`.  Copy and paste this snippet and change the _password_, _custom resource name_, and _namespace_ to your preferred value.\n\n```\noc create secret generic noicr-icpadmin-secret --from-literal=ICP_ADMIN_PASSWORD=Netcool2020 --namespace noi161ns\noc create secret generic noicr-impactadmin-secret --from-literal=IMPACT_ADMIN_PASSWORD=Netcool2020 --namespace noi161ns\noc create secret generic noicr-ldap-secret --from-literal=LDAP_BIND_PASSWORD=Netcool2020 --namespace noi161ns\noc create secret generic noicr-omni-secret --from-literal=OMNIBUS_ROOT_PASSWORD=Netcool2020 --namespace noi161ns\noc create secret generic noicr-was-secret --from-literal=WAS_PASSWORD=Netcool2020 --namespace noi161ns\noc create secret generic noicr-couchdb-secret --from-literal=password=Netcool2020 --from-literal=secret=couchdb --from-literal=username=root --namespace noi161ns\noc create secret generic noicr-systemauth-secret --from-literal=password=Netcool2020 --from-literal=username=system --namespace noi161ns\noc create secret generic noicr-ibm-hdm-common-ui-session-secret --from-literal=session=Netcool2020 --namespace noi161ns\noc create secret generic noicr-cassandra-auth-secret --from-literal=username=hdm --from-literal=password=Netcool2020 --namespace noi161ns\noc create secret generic noicr-ibm-redis-authsecret --from-literal=username=redis --from-literal=password=Netcool2020 --namespace noi161ns\noc create secret generic noicr-kafka-admin-secret --from-literal=username=kafka --from-literal=password=Netcool2020 --namespace noi161ns\noc create secret generic noicr-kafka-client-secret --from-literal=username=admin --from-literal=password=Netcool2020 --namespace noi161ns\noc create secret generic noicr-impact-secret --from-literal=IMPACT_ADMIN_PASSWORD=Netcool2020 --namespace noi161ns\n```\n\n<a name=\"nom-op\"></a>\n\n## Install the Operator\n\n- Using a browser login to the OCP Web Interface as a user with a cluster-admin role. \n- choose the following menu / sub-menu:\n`Administration > Cluster Settings > Global Configurations > OperatorHub > Sources`.\n- Click `Create Catalog Source`.  \n- Specify the image URL as `docker.io/ibmcom/noi-operator-catalog:1.0.0-20200620093846`.  Specify other details to your preference.\n- Click `Create`\n- After a few minutes in the `Sources` tab, you should see that the _# of Operators_ should turn to 1, as per the diagram below\n\n![NOI 1.6.1 OperatorHub Sources](./images/operator_hub_sources.png \"NOI 1.6.1 OperatorHub Sources\")\n\n\n- Go to the Main Menu, and select `Operators > OperatorHub`.\n- In the search text box, enter Netcool, and the NOI Operator should be listed, click on it and select `Install.`\n- Select the namespace that you have created earlier, do **not** specify the Approval Strategy > Manual, and select `Install.`\n- From the main menu, select `Operators > Installed Operators`, wait until the status says `Succeeded`.\n- You can use your workstation, perform an `oc login`, ensure that you are in the correct namespace perform the `oc project noi161ns` otherwise, do the `oc get pods` and you should see the noi-operator pods is running.\n\n<a name=\"nom-ins\"></a>\n\n## Create the NOI instances.\n\n- Continue from the `Operators > Installed Operators` select the `Netcool Operations Insight` Operator.  You should see the following screen:\n\n![NOI 1.6.1 provided API](./images/provided_api.png \"NOI 1.6.1 provided API\")\n\n- Select the `Create Instances` under the Cloud Deployment.\n- You will be presented with a YAML editor. An example of the YAML file is provided, in the example, the following are the options that had been selected:\n    - Custom Resource name: `noicr`\n    - Namespace: `noi161ns`\n    - antiAffinity: true\n    - clusterDomain: `apps.yourdomain.com`\n    - deploymenttype: `trial` (Enter `production` for production use)\n    - entitlementSecret: `noi-registry-secret`\n    - for internal LDAP, do not change any of the LDAP entry.\n    - storageClass: `rook-ceph-block`  (the name of your ceph storage class) There are multiple locations where the storage class information is required; in the example, all pods are assigned the same storage class.\n    - Enable ASM\n    - Enable a selection of ASM Observer: Kubernetes, Docker, REST, File, vCentre.\n    - Disable the Topology netDisco and appDisco \n    \nNote that at any time after the installation, you can change most of the configuration by editing the custom resource `noicr`.    \n    \nThe following is the example YAML specification:  \n\n```\n# Please edit the object below. Lines beginning with a '#' will be ignored,\n# and an empty file will abort the edit. If an error occurs while saving, this file will be\n# reopened with the relevant failures.\n#\napiVersion: noi.ibm.com/v1beta1\nkind: NOI\nmetadata:\n  creationTimestamp: \"2020-07-18T20:25:52Z\"\n  generation: 2\n  name: noicr\n  namespace: noi161ns\n  resourceVersion: \"53759204\"\n  selfLink: /apis/noi.ibm.com/v1beta1/namespaces/noi161ns/nois/noicr\n  uid: 05b565fd-e774-4895-8239-c1702db8fdbe\nspec:\n  advanced:\n    antiAffinity: true\n    imagePullPolicy: IfNotPresent\n    imagePullRepository: cp.icr.io/cp/noi\n  clusterDomain: apps.youdomain.com\n  deploymentType: trial\n  entitlementSecret: noi-registry-secret\n  ldap:\n    baseDN: dc=mycluster,dc=icp\n    bindDN: cn=admin,dc=mycluster,dc=icp\n    mode: standalone\n    port: \"389\"\n    sslPort: \"636\"\n    storageClass: rook-ceph-block\n    storageSize: 1Gi\n    suffix: dc=mycluster,dc=icp\n    url: ldap://localhost:389\n  license:\n    accept: true\n  persistence:\n    enabled: true\n    storageClassCassandraBackup: rook-ceph-block\n    storageClassCassandraData: rook-ceph-block\n    storageClassCouchdb: rook-ceph-block\n    storageClassDB2: rook-ceph-block\n    storageClassElastic: rook-ceph-block\n    storageClassImpactGUI: rook-ceph-block\n    storageClassImpactServer: rook-ceph-block\n    storageClassKafka: rook-ceph-block\n    storageClassNCOBackup: rook-ceph-block\n    storageClassNCOPrimary: rook-ceph-block\n    storageClassZookeeper: rook-ceph-block\n    storageSizeCassandraBackup: 50Gi\n    storageSizeCassandraData: 50Gi\n    storageSizeCouchdb: 5Gi\n    storageSizeDB2: 5Gi\n    storageSizeElastic: 75Gi\n    storageSizeImpactGUI: 5Gi\n    storageSizeImpactServer: 5Gi\n    storageSizeKafka: 50Gi\n    storageSizeNCOBackup: 5Gi\n    storageSizeNCOPrimary: 5Gi\n    storageSizeZookeeper: 5Gi\n  topology:\n    appDisco:\n      certSecret: \"\"\n      dbsecret: \"\"\n      dburl: \"\"\n      dsDefaultExistingClaim: \"\"\n      dsStorageClass: rook-ceph-block\n      dsStorageSize: 5Gi\n      enabled: false\n      pssExistingClaim: \"\"\n      pssStorageClass: rook-ceph-block\n      pssStorageSize: 5Gi\n      secure: false\n      sssStorageClass: rook-ceph-block\n      sssStorageSize: 5Gi\n    enabled: true\n    netDisco: false\n    observers:\n      alm: false\n      appdynamics: false\n      aws: false\n      azure: false\n      bigfixinventory: false\n      cienablueplanet: false\n      ciscoaci: false\n      contrail: false\n      dns: false\n      docker: true\n      dynatrace: false\n      file: true\n      googlecloud: false\n      ibmcloud: false\n      itnm: false\n      jenkins: false\n      junipercso: false\n      kubernetes: true\n      newrelic: false\n      openstack: false\n      rancher: false\n      rest: true\n      servicenow: false\n      taddm: false\n      vmvcenter: true\n      vmwarensx: false\n      zabbix: false\n    storageClassCassandraBackupTopology: rook-ceph-block\n    storageClassCassandraDataTopology: rook-ceph-block\n    storageClassElasticTopology: rook-ceph-block\n    storageClassFileObserver: rook-ceph-block\n    storageSizeCassandraBackupTopology: 50Gi\n    storageSizeCassandraDataTopology: 50Gi\n    storageSizeElasticTopology: 75Gi\n    storageSizeFileObserver: 5Gi\n  version: 1.6.1\n```\n\nOnce you are ready to initialize, select the **Create** button.\n\nThe Operator starts by running the pod `noicr-verifysecrets-*`  you can check using the `oc get pods` commands.  If the `verifysecrets` do not complete, then you have some authorization configuration errors, otherwise, the Operator starts deploying the pods in stages.\n\nIf everything is running successfully you should be able to see the list of pods similar to the following:\n\n```\n[jwahidin@workstation noi-operator-1.0.0]$ oc get pods\nNAME                                                              READY   STATUS      RESTARTS   AGE\nasm-operator-86d7867886-jtqrf                                     1/1     Running     0          37h\ncem-operator-bc5bb4ff9-6jhcv                                      1/1     Running     0          37h\nnoi-operator-6d5786bcf4-55xw2                                     1/1     Running     0          38h\nnoicr-alert-action-service-alertactionservice-b988bcb76-mwb66     1/1     Running     0          37h\nnoicr-alert-trigger-service-alerttriggerservice-68c9f56d-5jv8k    1/1     Running     0          37h\nnoicr-cassandra-0                                                 1/1     Running     0          38h\nnoicr-common-dash-auth-im-repo-dashauth-5785bff598-p2kl7          1/1     Running     0          37h\nnoicr-couchdb-0                                                   1/1     Running     0          38h\nnoicr-db2ese-0                                                    2/2     Running     0          38h\nnoicr-ea-noi-layer-eanoiactionservice-55cbcf84dd-fptss            1/1     Running     0          37h\nnoicr-ea-noi-layer-eanoigateway-57986b54b8-9tjxn                  1/1     Running     0          37h\nnoicr-ea-noi-layer-easetupomnibus-h5xch                           0/2     Completed   0          38h\nnoicr-elasticsearch-0                                             1/1     Running     0          38h\nnoicr-grafana-6cb8c5ff6c-4g477                                    1/1     Running     0          37h\nnoicr-ibm-cem-akora-app-cem-78dd8895dd-69mgx                      1/1     Running     0          37h\nnoicr-ibm-cem-brokers-846456587c-x98db                            1/1     Running     0          37h\nnoicr-ibm-cem-cem-users-6fd585c65b-z8vfd                          1/1     Running     0          37h\nnoicr-ibm-cem-channelservices-6857fc4bc4-xtjsn                    1/1     Running     0          37h\nnoicr-ibm-cem-event-analytics-ui-fd5b9f67c-ktms9                  1/1     Running     0          37h\nnoicr-ibm-cem-eventpreprocessor-75bdffd8cf-kdjvc                  1/1     Running     0          37h\nnoicr-ibm-cem-incidentprocessor-6c687548fd-lqfrh                  1/1     Running     1          37h\nnoicr-ibm-cem-integration-controller-b8c6d9d54-55h9z              1/1     Running     0          37h\nnoicr-ibm-cem-normalizer-6b6cb779c6-kbdnl                         1/1     Running     0          37h\nnoicr-ibm-cem-notificationprocessor-764c68d99d-qcn2r              1/1     Running     0          37h\nnoicr-ibm-cem-rba-as-86f446f5cd-ldcpk                             1/1     Running     0          37h\nnoicr-ibm-cem-rba-rbs-6759b5d7cf-569ck                            1/1     Running     0          37h\nnoicr-ibm-cem-scheduling-ui-867579b575-lvppj                      1/1     Running     1          37h\nnoicr-ibm-ea-asm-mime-eaasmmime-5cb5f6d7d7-g6c89                  1/1     Running     1          37h\nnoicr-ibm-ea-asm-normalizer-normalizerstreams-678d7fb4cc-mcbsp    1/1     Running     0          37h\nnoicr-ibm-ea-mime-classification-eaasmmimecls-8687645759-h6s4v    1/1     Running     0          37h\nnoicr-ibm-ea-ui-api-graphql-8657c6dc6d-cmc8q                      1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-archivingservice-567b749b6b-tbvxz     1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-collater-aggregationservice-58dtx46   1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-dedup-aggregationservice-69b76qcr9b   1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-eventsqueryservice-79df4f6f67-n8zxp   1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-inferenceservice-84bbcccd8-bb6s7      1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-ingestionservice-794b854dc-lrnbb      1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-normalizer-aggregationservice-wrlhz   1/1     Running     1          37h\nnoicr-ibm-hdm-analytics-dev-policyregistryservice-664c694cdp6p4   1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-servicemonitorservice-7d7679f899x4r   1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-setup-6f8gc                           0/2     Completed   0          37h\nnoicr-ibm-hdm-analytics-dev-trainer-c9f868b7b-n9776               1/1     Running     0          37h\nnoicr-ibm-hdm-common-ui-uiserver-5588cbf6cc-sfvgk                 1/1     Running     0          37h\nnoicr-ibm-noi-alert-details-service-64bf9f7-rbgwf                 1/1     Running     0          37h\nnoicr-ibm-noi-alert-details-setup-znrrl                           0/1     Completed   0          37h\nnoicr-ibm-redis-server-0                                          2/2     Running     0          38h\nnoicr-ibm-redis-server-1                                          2/2     Running     0          37h\nnoicr-ibm-redis-server-2                                          2/2     Running     0          37h\nnoicr-impactgui-0                                                 2/2     Running     0          38h\nnoicr-kafka-0                                                     2/2     Running     0          38h\nnoicr-kafka-1                                                     2/2     Running     0          38h\nnoicr-kafka-2                                                     2/2     Running     0          38h\nnoicr-logstash-587495ddf5-8ntpw                                   1/1     Running     0          37h\nnoicr-nciserver-0                                                 2/2     Running     0          5h54m\nnoicr-ncobackup-0                                                 2/2     Running     1          38h\nnoicr-ncodatalayer-agg-ir-75c9749469-9jcrv                        1/1     Running     0          37h\nnoicr-ncodatalayer-agg-irf-7b7b8bfb89-tqpcx                       1/1     Running     0          37h\nnoicr-ncodatalayer-agg-setup-sj6bx                                0/1     Completed   0          37h\nnoicr-ncodatalayer-agg-std-7b5cf79884-99cbg                       1/1     Running     1          37h\nnoicr-ncodatalayer-agg-std-7b5cf79884-bdd56                       1/1     Running     0          37h\nnoicr-ncoprimary-0                                                1/1     Running     2          38h\nnoicr-openldap-0                                                  1/1     Running     0          38h\nnoicr-proxy-5d9f77b864-dzns9                                      1/1     Running     0          37h\nnoicr-register-cnea-mgmt-artifact-1595241600-8bpn2                0/1     Completed   0          11m\nnoicr-register-cnea-mgmt-artifact-1595241900-tr49h                0/1     Completed   0          6m32s\nnoicr-register-cnea-mgmt-artifact-1595242200-8vct9                0/1     Completed   0          90s\nnoicr-spark-master-655cf4666d-79shf                               1/1     Running     0          37h\nnoicr-spark-slave-5f85db758d-tnscr                                1/1     Running     0          37h\nnoicr-spark-slave-5f85db758d-wzt4t                                1/1     Running     0          37h\nnoicr-topology-docker-observer-77bc9699d7-ptngj                   1/1     Running     0          37h\nnoicr-topology-elasticsearch-0                                    1/1     Running     0          37h\nnoicr-topology-kubernetes-observer-5fdcd8b8d-b69lm                1/1     Running     0          37h\nnoicr-topology-layout-648f8d6d7d-nz68k                            1/1     Running     0          37h\nnoicr-topology-merge-f5f69c4bc-r8sgf                              1/1     Running     0          37h\nnoicr-topology-noi-gateway-6c655fff94-gx7tc                       1/1     Running     3          37h\nnoicr-topology-noi-probe-578b797567-rhvsl                         1/1     Running     2        37h\nnoicr-topology-observer-service-84c8bbd9d6-vlknz                  1/1     Running     0          37h\nnoicr-topology-rest-observer-5667f7b85c-8hwxz                     1/1     Running     0          37h\nnoicr-topology-search-79875cb5bb-v2br8                            1/1     Running     0          23h\nnoicr-topology-secret-manager-7bj4q                               0/3     Completed   2          37h\nnoicr-topology-status-6b677bf47f-68wg2                            1/1     Running     0          37h\nnoicr-topology-system-health-cronjob-1595241600-n6nfq             0/1     Completed   0          11m\nnoicr-topology-system-health-cronjob-1595241900-x88qp             0/1     Completed   0          6m31s\nnoicr-topology-system-health-cronjob-1595242200-z72l5             0/1     Completed   0          90s\nnoicr-topology-topology-696d996d9c-lhhn7                          1/1     Running     0          37h\nnoicr-topology-ui-api-cf6644744-785qp                             1/1     Running     0          23h\nnoicr-topology-vmvcenter-observer-db47dbf77-v2tjc                 1/1     Running     0          37h\nnoicr-verifysecrets-9fnsc                                         0/1     Completed   0          38h\nnoicr-webgui-0                                                    2/2     Running     0          38h\nnoicr-zookeeper-0                                                 1/1     Running     0          38h\n```\n\n\n\n\n<a name=\"nom-post\"></a>\n\n## Post-installation steps\n\n\nIf you check on the status of the noicr (custom resource), you can see the next steps that you can perform.  It is listed here for convenience.\n\n```\nstatus:\n  message: >-\n    This deployment of Netcool Operation Insight is now complete. You can now\n\n    access to the following services:\n\n    Identify the public IP of the cluster:\n      export NODE_IP=<Public IP of the ICp cluster>\n\n    WebGUI:\n      Update your hosts file( On the machine you are running your Browser) or your DNS settings with this mapping\n      $NODE_IP netcool.noicr.apps.yourdomain.com\n\n      firefox https://netcool.noicr.apps.yourdomain.com:443/ibm/console\n\n      Default credentials are: icpadmin/password you can get from the secret noicr-icpadmin-secret using the following\n      kubectl get secret noicr-icpadmin-secret -o json -n noi161ns| grep ICP_ADMIN_PASSWORD  | cut -d : -f2 | cut -d '\"' -f2 | base64 -d;echo\n\n    WAS Console:\n      Update your hosts file( On the machine you are running your Browser) or your DNS settings with this mapping\n      $NODE_IP was.noicr.apps.yourdomain.com\n\n      firefox https://was.noicr.apps.yourdomain.com:443/ibm/console\n\n      Default credentials are: smadmin/password you can get from the secret noicr-was-secret using the following\n      kubectl get secret noicr-was-secret -o json -n noi161ns| grep WAS_PASSWORD | cut -d : -f2 | cut -d '\"' -f2 | base64 -d;echo\n\n    Impact GUI:\n      Update your hosts file( On the machine you are running your Browser) or your DNS settings with this mapping\n      $NODE_IP impact.noicr.apps.yourdomain.com\n\n      firefox https://impact.noicr.apps.yourdomain.com:443/ibm/console\n\n      Credentials are: impactadmin/password you can get from the secret noicr-impact-secret using the following\n      kubectl get secret noicr-impact-secret -o json -n noi161ns| grep IMPACT_ADMIN_PASSWORD | cut -d : -f2 | cut -d '\"' -f2 | base64 -d;echo\n\n    Impact Servers:\n      Update your hosts file( On the machine you are running your Browser) or your DNS settings with this mapping\n      $NODE_IP nci-0.noicr.apps.yourdomain.com\n\n      firefox https://nci-0.noicr.apps.yourdomain.com:443/nameserver/services\n\n      Default credentials are: impactadmin//password you can get from the secret noicr-impact-secret using the following\n      kubectl get secret noicr-impact-secret -o json -n noi161ns| grep IMPACT_ADMIN_PASSWORD | cut -d : -f2 | cut -d '\"' -f2 | base64 -d;echo\n\n    AIOPS:\n      Update your hosts file( On the machine you are running your Browser) or your DNS settings with this mapping\n      $NODE_IP netcool.noicr.apps.yourdomain.com\n\n      firefox https://netcool.noicr.apps.yourdomain.com:443/\n    Default credentials are: icpadmin/password you can get from the secret\n    noicr-icpadmin-secret using the following\n      kubectl get secret noicr-icpadmin-secret -o json -n noi161ns| grep ICP_ADMIN_PASSWORD  | cut -d : -f2 | cut -d '\"' -f2 | base64 -d;echo\n\n    Sample Data:\n      To create a job that installs and trains the analytics system with sample data, run the following commands:\n\n      kubectl run ingesnoi3 -i --restart=Never --env=LICENSE=accept  --namespace noi161ns\\\n            --env=CONTAINER_IMAGE=cp.icr.io/cp/noi/ea-events-tooling:4.0.11-20200616105630BST \\\n            --overrides='{ \"apiVersion\": \"v1\", \"spec\": { \"imagePullSecrets\": [{\"name\": \"noi-registry-secret\"}] } }' \\\n            --image=cp.icr.io/cp/noi/ea-events-tooling:4.0.11-20200616105630BST \\\n            loadSampleData.sh -- -r noicr  -t cfd95b7e-3bc7-4006-a4a8-a73a79c71255 \\\n            -a noi-service-account   -s noi-registry-secret  \\\n            -j > loadSampleData.yaml\n\n      kubectl create -f loadSampleData.yaml -n noi161ns\n\n      If the default service account does not have access to the image repository, uncomment the image pull secrets\n      section in the loadSampleData.yaml file and set noi-registry-secret  as the imagePullSecrets.name\n      value before running the kubectl create command.\n\n      To see how the sample events have been grouped, connect your browser to WebGUI, choose 'Event Viewer' from\n      the 'Incidents' menu, and change the view from 'Default' to 'Example_IBM_CloudAnalytics'.\n```\n\n\n### Assigning roles\n\nLog in to the Netcool Dashboard and assign the user or group roles. \nYour NOI 1.6.1 system is now installed and ready.\n\nMore information on administering users can be found in the [IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.1/com.ibm.netcool_ops.doc/soc/admin/task/adm_administering-users.html)\n\n[Installation instruction for NOI 1.6.0.3 including ASM 1.1.7](/mcm/cp4mcm_netcool_ops_manager/noi_1603) \n","type":"Mdx","contentDigest":"0aa565ce778f8d3199bf61df32a21fd0","counter":572,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"MCM - Netcool Ops Manager (NOM) - Installation Guide","description":"Installation guide for installing CP4MCM Netcool Ops Manager optional components","keywords":"ibm,install,mcm, cp4mcm, nom, netcool_ops_manager"},"exports":{},"rawBody":"---\ntitle: MCM - Netcool Ops Manager (NOM) - Installation Guide\ndescription: Installation guide for installing CP4MCM Netcool Ops Manager optional components\nkeywords: 'ibm,install,mcm, cp4mcm, nom, netcool_ops_manager'\n---\n\n## **Solution Overview**\n\nNetcool Ops Manager (NOM) is an optional component of Cloud Pak for Multicloud \nManagement. Netcool Ops Manager consists of multiple parts. The components are:\n- IBM Netcool Operations Insight (NOI)\n- IBM Agile Service Manager (ASM)\n- IBM Cloud Event Management (CEM)\n- IBM Predictive Insights (PI)\n\nThe components can be installed on Prem, on the cloud, or in a \nhybrid installation.  This playbook is \nwritten for the Cloud Pak for Multicloud Management (MCM), \nso the focus is on installing Netcool Ops Manager on \nOpenShift Container Platform (OCP), which can run on Prem or in the cloud. \n\nThe targetted audience for this section of the playbook is technical \nsales or technical services engineers who need a unified guide to install \nNetcool Ops Manager.\n\nThe current version 1.6.1 of NOI changes the installation method to use the `Operator.`  It no longer requires the IBM Common Platform (ICP) Common Service.  NOI 1.6.0.3 uses ICP.  Because of this, you can not perform an upgrade from 1.6.0.3 to 1.6.1.\n\n## Previous Version\n\nThe previous version installation instruction is desribed in \n[The installation instruction for NOI 1.6.0.3 including ASM 1.1.7](/mcm/cp4mcm_netcool_ops_manager/noi_1603) section.\n\n## The Operator Lifecycle Management\n\nThe NOI 1.6.1 can be installed online by using the Operator Lifecycle Management(OLM) or offline by downloading the container software first and then pushing it to the OpenShift Cluster.  The latter method is typically referred to as the CLI (Command Line Interface) method, as you need to use the CLI to perform the installation.\n\n## Installation Steps.\n\nThe following are the steps to install NOI 1.6.1:\n\n![NOI 1.6.1 OLM installation flow](./images/noi161_olm_flow.png \"NOI 1.6.1 OLM installation flow\")\n\n\n1. [Obtain your ICR entitlement key](#nom-icr)\n1. [Prepare the Openshift cluster](#nom-ocp)\n1. [Prepare the installation workstation](#nom-bastion)\n1. [Prepare the LDAP server](#nom-ldap)\n1. [Create the Openshift resources](#nom-rss)\n1. [Create the application secret](#nom-pwd)\n1. [Install the Operator](#nom-op)\n1. [Create the NOI Instance](#nom-ins)\n1. [Post installation steps](#nom-post)\n\nEach step is detailed as follows:\n\n\n<a name=\"nom-icr\"></a>\n\n## Obtain your ICR entitlement key\n\nWhen installing through the Operator Lifecycle Management, the container will be downloaded directly from the `cp.icr.io` (IBM Cloud Container Registry).   You first need to get your entitlement key from the IBM Cloud Container Registry. Once you got your entitlement key, you can create a Kubernetes secret.  You specify the secret name in the Operator.  This step is described later.\n\nFor IBM Staff, please go to `My IBM Entitlement` site to get your internal entitlement key.\n\n<a name=\"nom-ocp\"></a>\n\n## Preparing the Openshift cluster\n\nThe [Official documentation](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.1/com.ibm.netcool_ops.doc/soc/integration/concept/deploying_on_rhocp.html) of NOI 1.6.1 mentioned that NOI 1.6.1 is supported to run on:\n- OpenShift Container Platform version 4.3\n- OpenShift Container Platform version 4.4.3 \n\nA later version of 4.4.x seems to work; in fact, Specifying Custom Resource using a form is only supported on 4.4.6 or newer.\n\nIf you install on OCP 4.3 or 4.4.5 (or earlier), you need to specify `the Custom Resource (CR)` using the YAML.  On OCP 4.4.6 or above, you have the option to specify the CR using a form.  In this playbook, we will describe the custom resource using YAML.\n\n### Sizing\nThe online documentation (links provided below)\nprovides sizing guidelines.  Separate sizing \nguidelines are available depending on whether you are installing for a \n__trial__ (PoC) or \nfor a __production__ environment.\n\n#### CPU and Memory\nRecommended minimal worker nodes sizing:\n\nDescription | Quantity\n--- | ---\nNumber of worker nodes | 5\nNumber of vCPUs per worker node | 16 \nMinimum memory per worker node | 32 GB\n\nInformation on sizing can be found at the following sites:\n[Sizing Guidelines](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.1/com.ibm.netcool_ops.doc/soc/integration/reference/soc_sizing_full.html)\n\n#### Storage Capacity\n\nIf you are installing into OCP, then **Rook/Ceph** or \n**Openshift Container Storage (OCS)**, with **RADOS Block Device (RBD)** storage class, is the default supported OCP Storage solution.  We recommend Rook/Ceph \nas the dynamic storage solution for Netcool Ops Manager.\n\n**Image Registry Storage**: As you are preparing the OCP cluster, If you are going to install through CLI, please allow at least 150GB of image-registry storage for the Netcool Ops Manager. This capacity is in addition to your image-registry requirements for your other OCP's purpose. \n\n**Persistent Volume Claim**: If you are deploying the Openshift Container Storage (OCS), then OCS \ncreates a default 2 TB Rook/Ceph/RDB block storage.  For an initial production installation of Netcool Ops Manager, you need about 800 GB of storage (PVC) space and image-registry storage.  Please take note of the storage class name. You need this later during the installation.\n\nIf you need help with installing your OCP environment, please see this \nplaybook's section on [installing OpenShift](../../ocp/introduction/).\n\nIf other team members configured your OCP cluster, then please ensure that they provide you with an account with a cluster administrator role.\n\n\n<a name=\"nom-bastion\"></a>\n\n## Preparing the installation workstation\n\nYou will need to use your local workstation web browser to access the OCP Web Interface, install the Operator, and create the NOI Custom Resource.\n\nYou will also need the OCP client to help you with the installation.\n\n### Getting the oc and kubectl command lines\nYou download oc and kubectl from your OCP cluster. The kubectl executable is \na symbolic link of the oc executable.  The following \n[documentation from Red Hat](https://docs.openshift.com/container-platform/4.3/cli_reference/openshift_cli/getting-started-cli.html) \ndescribes the steps to get started with the `oc` and `kubectl` command line \ninterface.\n\n<a name=\"nom-ldap\"></a>\n\n## Preparing the LDAP server\nYou need to provide details of your LDAP server for the following components:\n- OCP Cluster\n- NOI Proxy configuration.\n\nSetting up your LDAP server is a common requirement across all  Cloud Paks, \nso it is not detailed here.  \n\nDuring the installation, you will need to specify the following information, so \nget the information before you start the helm chart configuration:\n\n- Your Base Distinguished name.\n- Your LDAP URL.\n- Your LDAP Bind User Name and Password.\n- The filter to get the user information (User Filter and User ID map).\n- The filter to get the group information (Group Filter and Group ID map).\n- The filter to map a user to a group (Group member ID map).\n\nOne of the pods deployed by the NOI Helm Chart is an OpenLDAP pod. You can choose to set up the OpenLDAP as a standalone repository or as a proxy to an external LDAP server.  If you choose to use the OpenLDAP pod as a proxy, then Netcool Ops Manager expects the external LDAP server to support the hierarchical LDAP structure.  In particular, the NOI LDAP configuration wants to use ou (Organizational Unit).  \nMore information on the NOI Proxy LDAP requirement can be found in the \n[IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.0/com.ibm.netcool_ops.doc/soc/admin/reference/managing_users_using_an_external_ldap_server.html).\n\n<InlineNotification>\n\nNote that since RHEL 7.4, the RHEL repository no longer distributes the OpenLDAP server.  The default LDAP server for RHEL 7.5 onwards is the IPA (Identity, Policy, Audit) server. The \nIPA server only supports a flat LDIF structure. It does not support `ou`, so you can not use the IPA server as the external LDAP server for NOI.  This information can be found in the \n[Red Hat Solutions](https://access.redhat.com/solutions/4172491) documentation.\n\n\n</InlineNotification>\n\n### User in LDAP.\n\nIf you are using an external LDAP server, then create the following user in the external LDAP:\n- __smadmin__ - The administrative user for the dashboard.\n- __impactadmin__ - The administrative user for Netcool/Impact.\n- __icpadmin__ - The default ICP admin user\n- __icpuser__ - The default ICP standard user\n\n\n<a name=\"nom-rss\"></a>\n\n## Create the Openshift resources\n\nYou need to create the following Openshift resource for the Operator.\n- namespace\n- Custom Resource name\n- Service Account Registry Secret.\n- Service Account\n\n### namespace\nCreate the namespace for the NOI installation.\nIf you decide your namespace to be `noi161ns` then perform the following:\n\n```\noc new-project noi161ns\n```\n\n### Custom Resource name\n\nAll your pods will be prefixed with your custom resource name, so choose something short.  For example, `noicr`.\n\n### Service Account Registry Secret.\n\nYou create a secret containing your entitlement key described earlier.  \n\n```\noc create secret docker-registry noi-registry-secret --docker-server=cp.icr.io --docker-username=cp --docker-password=\"your entitlement key from the first step above\"\n```\n\nYou specify the `noi-registry-secret` into the service account (next step) and into a custom resource (later).\n\n\n### Service Account\n\nIt is recommended to use the suggested `noi-service-account`, perform the following:\n\n```\noc create serviceaccount noi-service-account -n noi161ns\noc adm policy add-scc-to-user privileged system:serviceaccount:noi161ns:noi-service-account\noc patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"noi-registry-secret\"}]}'\n```\n\n<a name=\"nom-pwd\"></a>\n\n## Create the application secret\n\nIf you are using the internal OpenLDAP, then this step is optional.  If you do not specify the secret, then the password will be created for you.  You can get the password post-install from the Kubernetes secret.\n\nIf you are going to use the external LDAP, then you need to specify the password for:\n- LDAP user\n- smadmin\n- impactadmin\n- icpadmin\n\nThe password should match each user's password in the external LDAP.\n\nIf you want to use a friendly password, you can either create the secret before installing or change the password after the installation.\n\nThe details of specifying the password through Kubernetes secret are described in the [Configuring Authentication](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.1/com.ibm.netcool_ops.doc/soc/integration/task/int-creating_passwords_and_secrets-rhocp.html) section of the online document.\n\nFor your convenience, the following list the command to specify the password `Netcool2020` to everything, including the internal LDAP pod.  Note there is an additional line for impact as the secret name should be `custom-resource-impact-secret` rather than the documented `custom-resource-impactadmin-secret`.  Copy and paste this snippet and change the _password_, _custom resource name_, and _namespace_ to your preferred value.\n\n```\noc create secret generic noicr-icpadmin-secret --from-literal=ICP_ADMIN_PASSWORD=Netcool2020 --namespace noi161ns\noc create secret generic noicr-impactadmin-secret --from-literal=IMPACT_ADMIN_PASSWORD=Netcool2020 --namespace noi161ns\noc create secret generic noicr-ldap-secret --from-literal=LDAP_BIND_PASSWORD=Netcool2020 --namespace noi161ns\noc create secret generic noicr-omni-secret --from-literal=OMNIBUS_ROOT_PASSWORD=Netcool2020 --namespace noi161ns\noc create secret generic noicr-was-secret --from-literal=WAS_PASSWORD=Netcool2020 --namespace noi161ns\noc create secret generic noicr-couchdb-secret --from-literal=password=Netcool2020 --from-literal=secret=couchdb --from-literal=username=root --namespace noi161ns\noc create secret generic noicr-systemauth-secret --from-literal=password=Netcool2020 --from-literal=username=system --namespace noi161ns\noc create secret generic noicr-ibm-hdm-common-ui-session-secret --from-literal=session=Netcool2020 --namespace noi161ns\noc create secret generic noicr-cassandra-auth-secret --from-literal=username=hdm --from-literal=password=Netcool2020 --namespace noi161ns\noc create secret generic noicr-ibm-redis-authsecret --from-literal=username=redis --from-literal=password=Netcool2020 --namespace noi161ns\noc create secret generic noicr-kafka-admin-secret --from-literal=username=kafka --from-literal=password=Netcool2020 --namespace noi161ns\noc create secret generic noicr-kafka-client-secret --from-literal=username=admin --from-literal=password=Netcool2020 --namespace noi161ns\noc create secret generic noicr-impact-secret --from-literal=IMPACT_ADMIN_PASSWORD=Netcool2020 --namespace noi161ns\n```\n\n<a name=\"nom-op\"></a>\n\n## Install the Operator\n\n- Using a browser login to the OCP Web Interface as a user with a cluster-admin role. \n- choose the following menu / sub-menu:\n`Administration > Cluster Settings > Global Configurations > OperatorHub > Sources`.\n- Click `Create Catalog Source`.  \n- Specify the image URL as `docker.io/ibmcom/noi-operator-catalog:1.0.0-20200620093846`.  Specify other details to your preference.\n- Click `Create`\n- After a few minutes in the `Sources` tab, you should see that the _# of Operators_ should turn to 1, as per the diagram below\n\n![NOI 1.6.1 OperatorHub Sources](./images/operator_hub_sources.png \"NOI 1.6.1 OperatorHub Sources\")\n\n\n- Go to the Main Menu, and select `Operators > OperatorHub`.\n- In the search text box, enter Netcool, and the NOI Operator should be listed, click on it and select `Install.`\n- Select the namespace that you have created earlier, do **not** specify the Approval Strategy > Manual, and select `Install.`\n- From the main menu, select `Operators > Installed Operators`, wait until the status says `Succeeded`.\n- You can use your workstation, perform an `oc login`, ensure that you are in the correct namespace perform the `oc project noi161ns` otherwise, do the `oc get pods` and you should see the noi-operator pods is running.\n\n<a name=\"nom-ins\"></a>\n\n## Create the NOI instances.\n\n- Continue from the `Operators > Installed Operators` select the `Netcool Operations Insight` Operator.  You should see the following screen:\n\n![NOI 1.6.1 provided API](./images/provided_api.png \"NOI 1.6.1 provided API\")\n\n- Select the `Create Instances` under the Cloud Deployment.\n- You will be presented with a YAML editor. An example of the YAML file is provided, in the example, the following are the options that had been selected:\n    - Custom Resource name: `noicr`\n    - Namespace: `noi161ns`\n    - antiAffinity: true\n    - clusterDomain: `apps.yourdomain.com`\n    - deploymenttype: `trial` (Enter `production` for production use)\n    - entitlementSecret: `noi-registry-secret`\n    - for internal LDAP, do not change any of the LDAP entry.\n    - storageClass: `rook-ceph-block`  (the name of your ceph storage class) There are multiple locations where the storage class information is required; in the example, all pods are assigned the same storage class.\n    - Enable ASM\n    - Enable a selection of ASM Observer: Kubernetes, Docker, REST, File, vCentre.\n    - Disable the Topology netDisco and appDisco \n    \nNote that at any time after the installation, you can change most of the configuration by editing the custom resource `noicr`.    \n    \nThe following is the example YAML specification:  \n\n```\n# Please edit the object below. Lines beginning with a '#' will be ignored,\n# and an empty file will abort the edit. If an error occurs while saving, this file will be\n# reopened with the relevant failures.\n#\napiVersion: noi.ibm.com/v1beta1\nkind: NOI\nmetadata:\n  creationTimestamp: \"2020-07-18T20:25:52Z\"\n  generation: 2\n  name: noicr\n  namespace: noi161ns\n  resourceVersion: \"53759204\"\n  selfLink: /apis/noi.ibm.com/v1beta1/namespaces/noi161ns/nois/noicr\n  uid: 05b565fd-e774-4895-8239-c1702db8fdbe\nspec:\n  advanced:\n    antiAffinity: true\n    imagePullPolicy: IfNotPresent\n    imagePullRepository: cp.icr.io/cp/noi\n  clusterDomain: apps.youdomain.com\n  deploymentType: trial\n  entitlementSecret: noi-registry-secret\n  ldap:\n    baseDN: dc=mycluster,dc=icp\n    bindDN: cn=admin,dc=mycluster,dc=icp\n    mode: standalone\n    port: \"389\"\n    sslPort: \"636\"\n    storageClass: rook-ceph-block\n    storageSize: 1Gi\n    suffix: dc=mycluster,dc=icp\n    url: ldap://localhost:389\n  license:\n    accept: true\n  persistence:\n    enabled: true\n    storageClassCassandraBackup: rook-ceph-block\n    storageClassCassandraData: rook-ceph-block\n    storageClassCouchdb: rook-ceph-block\n    storageClassDB2: rook-ceph-block\n    storageClassElastic: rook-ceph-block\n    storageClassImpactGUI: rook-ceph-block\n    storageClassImpactServer: rook-ceph-block\n    storageClassKafka: rook-ceph-block\n    storageClassNCOBackup: rook-ceph-block\n    storageClassNCOPrimary: rook-ceph-block\n    storageClassZookeeper: rook-ceph-block\n    storageSizeCassandraBackup: 50Gi\n    storageSizeCassandraData: 50Gi\n    storageSizeCouchdb: 5Gi\n    storageSizeDB2: 5Gi\n    storageSizeElastic: 75Gi\n    storageSizeImpactGUI: 5Gi\n    storageSizeImpactServer: 5Gi\n    storageSizeKafka: 50Gi\n    storageSizeNCOBackup: 5Gi\n    storageSizeNCOPrimary: 5Gi\n    storageSizeZookeeper: 5Gi\n  topology:\n    appDisco:\n      certSecret: \"\"\n      dbsecret: \"\"\n      dburl: \"\"\n      dsDefaultExistingClaim: \"\"\n      dsStorageClass: rook-ceph-block\n      dsStorageSize: 5Gi\n      enabled: false\n      pssExistingClaim: \"\"\n      pssStorageClass: rook-ceph-block\n      pssStorageSize: 5Gi\n      secure: false\n      sssStorageClass: rook-ceph-block\n      sssStorageSize: 5Gi\n    enabled: true\n    netDisco: false\n    observers:\n      alm: false\n      appdynamics: false\n      aws: false\n      azure: false\n      bigfixinventory: false\n      cienablueplanet: false\n      ciscoaci: false\n      contrail: false\n      dns: false\n      docker: true\n      dynatrace: false\n      file: true\n      googlecloud: false\n      ibmcloud: false\n      itnm: false\n      jenkins: false\n      junipercso: false\n      kubernetes: true\n      newrelic: false\n      openstack: false\n      rancher: false\n      rest: true\n      servicenow: false\n      taddm: false\n      vmvcenter: true\n      vmwarensx: false\n      zabbix: false\n    storageClassCassandraBackupTopology: rook-ceph-block\n    storageClassCassandraDataTopology: rook-ceph-block\n    storageClassElasticTopology: rook-ceph-block\n    storageClassFileObserver: rook-ceph-block\n    storageSizeCassandraBackupTopology: 50Gi\n    storageSizeCassandraDataTopology: 50Gi\n    storageSizeElasticTopology: 75Gi\n    storageSizeFileObserver: 5Gi\n  version: 1.6.1\n```\n\nOnce you are ready to initialize, select the **Create** button.\n\nThe Operator starts by running the pod `noicr-verifysecrets-*`  you can check using the `oc get pods` commands.  If the `verifysecrets` do not complete, then you have some authorization configuration errors, otherwise, the Operator starts deploying the pods in stages.\n\nIf everything is running successfully you should be able to see the list of pods similar to the following:\n\n```\n[jwahidin@workstation noi-operator-1.0.0]$ oc get pods\nNAME                                                              READY   STATUS      RESTARTS   AGE\nasm-operator-86d7867886-jtqrf                                     1/1     Running     0          37h\ncem-operator-bc5bb4ff9-6jhcv                                      1/1     Running     0          37h\nnoi-operator-6d5786bcf4-55xw2                                     1/1     Running     0          38h\nnoicr-alert-action-service-alertactionservice-b988bcb76-mwb66     1/1     Running     0          37h\nnoicr-alert-trigger-service-alerttriggerservice-68c9f56d-5jv8k    1/1     Running     0          37h\nnoicr-cassandra-0                                                 1/1     Running     0          38h\nnoicr-common-dash-auth-im-repo-dashauth-5785bff598-p2kl7          1/1     Running     0          37h\nnoicr-couchdb-0                                                   1/1     Running     0          38h\nnoicr-db2ese-0                                                    2/2     Running     0          38h\nnoicr-ea-noi-layer-eanoiactionservice-55cbcf84dd-fptss            1/1     Running     0          37h\nnoicr-ea-noi-layer-eanoigateway-57986b54b8-9tjxn                  1/1     Running     0          37h\nnoicr-ea-noi-layer-easetupomnibus-h5xch                           0/2     Completed   0          38h\nnoicr-elasticsearch-0                                             1/1     Running     0          38h\nnoicr-grafana-6cb8c5ff6c-4g477                                    1/1     Running     0          37h\nnoicr-ibm-cem-akora-app-cem-78dd8895dd-69mgx                      1/1     Running     0          37h\nnoicr-ibm-cem-brokers-846456587c-x98db                            1/1     Running     0          37h\nnoicr-ibm-cem-cem-users-6fd585c65b-z8vfd                          1/1     Running     0          37h\nnoicr-ibm-cem-channelservices-6857fc4bc4-xtjsn                    1/1     Running     0          37h\nnoicr-ibm-cem-event-analytics-ui-fd5b9f67c-ktms9                  1/1     Running     0          37h\nnoicr-ibm-cem-eventpreprocessor-75bdffd8cf-kdjvc                  1/1     Running     0          37h\nnoicr-ibm-cem-incidentprocessor-6c687548fd-lqfrh                  1/1     Running     1          37h\nnoicr-ibm-cem-integration-controller-b8c6d9d54-55h9z              1/1     Running     0          37h\nnoicr-ibm-cem-normalizer-6b6cb779c6-kbdnl                         1/1     Running     0          37h\nnoicr-ibm-cem-notificationprocessor-764c68d99d-qcn2r              1/1     Running     0          37h\nnoicr-ibm-cem-rba-as-86f446f5cd-ldcpk                             1/1     Running     0          37h\nnoicr-ibm-cem-rba-rbs-6759b5d7cf-569ck                            1/1     Running     0          37h\nnoicr-ibm-cem-scheduling-ui-867579b575-lvppj                      1/1     Running     1          37h\nnoicr-ibm-ea-asm-mime-eaasmmime-5cb5f6d7d7-g6c89                  1/1     Running     1          37h\nnoicr-ibm-ea-asm-normalizer-normalizerstreams-678d7fb4cc-mcbsp    1/1     Running     0          37h\nnoicr-ibm-ea-mime-classification-eaasmmimecls-8687645759-h6s4v    1/1     Running     0          37h\nnoicr-ibm-ea-ui-api-graphql-8657c6dc6d-cmc8q                      1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-archivingservice-567b749b6b-tbvxz     1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-collater-aggregationservice-58dtx46   1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-dedup-aggregationservice-69b76qcr9b   1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-eventsqueryservice-79df4f6f67-n8zxp   1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-inferenceservice-84bbcccd8-bb6s7      1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-ingestionservice-794b854dc-lrnbb      1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-normalizer-aggregationservice-wrlhz   1/1     Running     1          37h\nnoicr-ibm-hdm-analytics-dev-policyregistryservice-664c694cdp6p4   1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-servicemonitorservice-7d7679f899x4r   1/1     Running     0          37h\nnoicr-ibm-hdm-analytics-dev-setup-6f8gc                           0/2     Completed   0          37h\nnoicr-ibm-hdm-analytics-dev-trainer-c9f868b7b-n9776               1/1     Running     0          37h\nnoicr-ibm-hdm-common-ui-uiserver-5588cbf6cc-sfvgk                 1/1     Running     0          37h\nnoicr-ibm-noi-alert-details-service-64bf9f7-rbgwf                 1/1     Running     0          37h\nnoicr-ibm-noi-alert-details-setup-znrrl                           0/1     Completed   0          37h\nnoicr-ibm-redis-server-0                                          2/2     Running     0          38h\nnoicr-ibm-redis-server-1                                          2/2     Running     0          37h\nnoicr-ibm-redis-server-2                                          2/2     Running     0          37h\nnoicr-impactgui-0                                                 2/2     Running     0          38h\nnoicr-kafka-0                                                     2/2     Running     0          38h\nnoicr-kafka-1                                                     2/2     Running     0          38h\nnoicr-kafka-2                                                     2/2     Running     0          38h\nnoicr-logstash-587495ddf5-8ntpw                                   1/1     Running     0          37h\nnoicr-nciserver-0                                                 2/2     Running     0          5h54m\nnoicr-ncobackup-0                                                 2/2     Running     1          38h\nnoicr-ncodatalayer-agg-ir-75c9749469-9jcrv                        1/1     Running     0          37h\nnoicr-ncodatalayer-agg-irf-7b7b8bfb89-tqpcx                       1/1     Running     0          37h\nnoicr-ncodatalayer-agg-setup-sj6bx                                0/1     Completed   0          37h\nnoicr-ncodatalayer-agg-std-7b5cf79884-99cbg                       1/1     Running     1          37h\nnoicr-ncodatalayer-agg-std-7b5cf79884-bdd56                       1/1     Running     0          37h\nnoicr-ncoprimary-0                                                1/1     Running     2          38h\nnoicr-openldap-0                                                  1/1     Running     0          38h\nnoicr-proxy-5d9f77b864-dzns9                                      1/1     Running     0          37h\nnoicr-register-cnea-mgmt-artifact-1595241600-8bpn2                0/1     Completed   0          11m\nnoicr-register-cnea-mgmt-artifact-1595241900-tr49h                0/1     Completed   0          6m32s\nnoicr-register-cnea-mgmt-artifact-1595242200-8vct9                0/1     Completed   0          90s\nnoicr-spark-master-655cf4666d-79shf                               1/1     Running     0          37h\nnoicr-spark-slave-5f85db758d-tnscr                                1/1     Running     0          37h\nnoicr-spark-slave-5f85db758d-wzt4t                                1/1     Running     0          37h\nnoicr-topology-docker-observer-77bc9699d7-ptngj                   1/1     Running     0          37h\nnoicr-topology-elasticsearch-0                                    1/1     Running     0          37h\nnoicr-topology-kubernetes-observer-5fdcd8b8d-b69lm                1/1     Running     0          37h\nnoicr-topology-layout-648f8d6d7d-nz68k                            1/1     Running     0          37h\nnoicr-topology-merge-f5f69c4bc-r8sgf                              1/1     Running     0          37h\nnoicr-topology-noi-gateway-6c655fff94-gx7tc                       1/1     Running     3          37h\nnoicr-topology-noi-probe-578b797567-rhvsl                         1/1     Running     2        37h\nnoicr-topology-observer-service-84c8bbd9d6-vlknz                  1/1     Running     0          37h\nnoicr-topology-rest-observer-5667f7b85c-8hwxz                     1/1     Running     0          37h\nnoicr-topology-search-79875cb5bb-v2br8                            1/1     Running     0          23h\nnoicr-topology-secret-manager-7bj4q                               0/3     Completed   2          37h\nnoicr-topology-status-6b677bf47f-68wg2                            1/1     Running     0          37h\nnoicr-topology-system-health-cronjob-1595241600-n6nfq             0/1     Completed   0          11m\nnoicr-topology-system-health-cronjob-1595241900-x88qp             0/1     Completed   0          6m31s\nnoicr-topology-system-health-cronjob-1595242200-z72l5             0/1     Completed   0          90s\nnoicr-topology-topology-696d996d9c-lhhn7                          1/1     Running     0          37h\nnoicr-topology-ui-api-cf6644744-785qp                             1/1     Running     0          23h\nnoicr-topology-vmvcenter-observer-db47dbf77-v2tjc                 1/1     Running     0          37h\nnoicr-verifysecrets-9fnsc                                         0/1     Completed   0          38h\nnoicr-webgui-0                                                    2/2     Running     0          38h\nnoicr-zookeeper-0                                                 1/1     Running     0          38h\n```\n\n\n\n\n<a name=\"nom-post\"></a>\n\n## Post-installation steps\n\n\nIf you check on the status of the noicr (custom resource), you can see the next steps that you can perform.  It is listed here for convenience.\n\n```\nstatus:\n  message: >-\n    This deployment of Netcool Operation Insight is now complete. You can now\n\n    access to the following services:\n\n    Identify the public IP of the cluster:\n      export NODE_IP=<Public IP of the ICp cluster>\n\n    WebGUI:\n      Update your hosts file( On the machine you are running your Browser) or your DNS settings with this mapping\n      $NODE_IP netcool.noicr.apps.yourdomain.com\n\n      firefox https://netcool.noicr.apps.yourdomain.com:443/ibm/console\n\n      Default credentials are: icpadmin/password you can get from the secret noicr-icpadmin-secret using the following\n      kubectl get secret noicr-icpadmin-secret -o json -n noi161ns| grep ICP_ADMIN_PASSWORD  | cut -d : -f2 | cut -d '\"' -f2 | base64 -d;echo\n\n    WAS Console:\n      Update your hosts file( On the machine you are running your Browser) or your DNS settings with this mapping\n      $NODE_IP was.noicr.apps.yourdomain.com\n\n      firefox https://was.noicr.apps.yourdomain.com:443/ibm/console\n\n      Default credentials are: smadmin/password you can get from the secret noicr-was-secret using the following\n      kubectl get secret noicr-was-secret -o json -n noi161ns| grep WAS_PASSWORD | cut -d : -f2 | cut -d '\"' -f2 | base64 -d;echo\n\n    Impact GUI:\n      Update your hosts file( On the machine you are running your Browser) or your DNS settings with this mapping\n      $NODE_IP impact.noicr.apps.yourdomain.com\n\n      firefox https://impact.noicr.apps.yourdomain.com:443/ibm/console\n\n      Credentials are: impactadmin/password you can get from the secret noicr-impact-secret using the following\n      kubectl get secret noicr-impact-secret -o json -n noi161ns| grep IMPACT_ADMIN_PASSWORD | cut -d : -f2 | cut -d '\"' -f2 | base64 -d;echo\n\n    Impact Servers:\n      Update your hosts file( On the machine you are running your Browser) or your DNS settings with this mapping\n      $NODE_IP nci-0.noicr.apps.yourdomain.com\n\n      firefox https://nci-0.noicr.apps.yourdomain.com:443/nameserver/services\n\n      Default credentials are: impactadmin//password you can get from the secret noicr-impact-secret using the following\n      kubectl get secret noicr-impact-secret -o json -n noi161ns| grep IMPACT_ADMIN_PASSWORD | cut -d : -f2 | cut -d '\"' -f2 | base64 -d;echo\n\n    AIOPS:\n      Update your hosts file( On the machine you are running your Browser) or your DNS settings with this mapping\n      $NODE_IP netcool.noicr.apps.yourdomain.com\n\n      firefox https://netcool.noicr.apps.yourdomain.com:443/\n    Default credentials are: icpadmin/password you can get from the secret\n    noicr-icpadmin-secret using the following\n      kubectl get secret noicr-icpadmin-secret -o json -n noi161ns| grep ICP_ADMIN_PASSWORD  | cut -d : -f2 | cut -d '\"' -f2 | base64 -d;echo\n\n    Sample Data:\n      To create a job that installs and trains the analytics system with sample data, run the following commands:\n\n      kubectl run ingesnoi3 -i --restart=Never --env=LICENSE=accept  --namespace noi161ns\\\n            --env=CONTAINER_IMAGE=cp.icr.io/cp/noi/ea-events-tooling:4.0.11-20200616105630BST \\\n            --overrides='{ \"apiVersion\": \"v1\", \"spec\": { \"imagePullSecrets\": [{\"name\": \"noi-registry-secret\"}] } }' \\\n            --image=cp.icr.io/cp/noi/ea-events-tooling:4.0.11-20200616105630BST \\\n            loadSampleData.sh -- -r noicr  -t cfd95b7e-3bc7-4006-a4a8-a73a79c71255 \\\n            -a noi-service-account   -s noi-registry-secret  \\\n            -j > loadSampleData.yaml\n\n      kubectl create -f loadSampleData.yaml -n noi161ns\n\n      If the default service account does not have access to the image repository, uncomment the image pull secrets\n      section in the loadSampleData.yaml file and set noi-registry-secret  as the imagePullSecrets.name\n      value before running the kubectl create command.\n\n      To see how the sample events have been grouped, connect your browser to WebGUI, choose 'Event Viewer' from\n      the 'Incidents' menu, and change the view from 'Default' to 'Example_IBM_CloudAnalytics'.\n```\n\n\n### Assigning roles\n\nLog in to the Netcool Dashboard and assign the user or group roles. \nYour NOI 1.6.1 system is now installed and ready.\n\nMore information on administering users can be found in the [IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/SSTPTP_1.6.1/com.ibm.netcool_ops.doc/soc/admin/task/adm_administering-users.html)\n\n[Installation instruction for NOI 1.6.0.3 including ASM 1.1.7](/mcm/cp4mcm_netcool_ops_manager/noi_1603) \n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/mcm/cp4mcm_netcool_ops_manager/index.mdx"}}}}