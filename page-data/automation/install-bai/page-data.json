{"componentChunkName":"component---src-pages-automation-install-bai-index-mdx","path":"/automation/install-bai/","result":{"pageContext":{"frontmatter":{"title":"Business Automation Insights","weight":700},"relativePagePath":"/automation/install-bai/index.mdx","titleType":"page","MdxNode":{"id":"889ad09e-90cf-5f54-9e25-7b8ae452c352","children":[],"parent":"e8e22376-aef6-5471-9e19-5f7f20ccbcaa","internal":{"content":"---\ntitle: Business Automation Insights\nweight: 700\n---\n\n### Log in to you OCP cluster\nSee the [Prerequisites](/content/automation/pre-requisites) chapter for details on logging in to your OCP cluster.\n\n## Install Kafka\n\n### Add the Helm repository\n- Execute:\n```\nhelm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator\n```\n\n### Create the Kafka project\n- Execute:\n```\noc new-project kafka\n```\n\n- Make sure you are working from the `kafka` project, then grant the tiller server `edit` access to current project:\n```\noc project kafka\noc adm policy add-role-to-user edit \"system:serviceaccount:tiller:tiller\"\n```\n\n- Add the Security Context Constraints (CSS) `anyuid` to the Service Account `default` of the project:\n```\noc adm policy add-scc-to-user anyuid -z default\n```\n### Create PVs if necessary\nIf your cluster does not support dynamic persistence volume, download [kafka-pv.yaml](/assets/automation/bai/kafka-pv.yaml) and execute:\n```\noc apply -f kafka-pv.yaml\n```\n\n### Install the Helm chart\n\nDownload the [kafka-values.yaml](/assets/automation/bai/kafka-values.yaml) file to your working directory and run the following command:\n```\nhelm install incubator/kafka --name my-kafka --namespace kafka -f kafka-values.yaml\n```\nThis will deploy [Apache Kafka](https://github.com/helm/charts/tree/master/incubator/kafka) and [Zookeeper](https://github.com/helm/charts/tree/master/incubator/zookeeper).\n\nKeep `my-kafka` as a release name because the rest of the installation assumes it. \n\n### Set-up the Kafka bootstrap server\n\nIf access is from inside the OpenShift cluster environment, then the bootstrap server is `my-kafka-headless.kafka.svc.cluster.local:9092`.\n\nIf access is from an external system, you need to perform the following steps:\n\n1 - Retrieve the OpenShift ingress address with the following command: \n![Kafka ingress](/assets/automation//images/rhos-kafka1.png)\n\n2 - Use this ingress address to set the bootstrap server to `{rhos-ingress-ip}:31090,{rhos-ingress-ip}:31091,{rhos-ingress-ip}:31092`.\n\n3 - Add the line `{rhos-ingress-ip} kafka.cluster.local` to the `/etc/hosts` file.\n\n\n## Install Business Automation Insights (BAI)\n\n### Download the BAI PPA \nDownload the following PPA from [IBM Passport Advantage](https://www.ibm.com/software/passportadvantage) to your working-directory:\n\n- *IBM Cloud Pak for Automation v19.0.1 - Business Operation Insights for Certified Kubernetes Multiplatform English (CC222EN)*\n\nThe downloaded archive should be `ICP4A19.0.1-bai.tgz`.\n\n### Create the BAI project\n- Create a new OpenShift project for BAI:\n```\noc new-project baiproject\n```\n- Make sure you are working from your newly created BAI project, then grant the tiller server `edit` access to current project:\n```\noc project baiproject\noc adm policy add-role-to-user edit \"system:serviceaccount:tiller:tiller\"\n```\n\n### Create a ServiceAccount and update the SCCs\n\nCheck if `ibm-anyuid-scc` and `ibm-privileged-scc` exist in your cluster:\n```\noc get scc ibm-anyuid-scc\noc get scc ibm-privileged-scc\n```\n\nIf they don't exist, download [ibm-anyuid-scc.yaml](/assets/automation/bai/ibm-anyuid-scc.yaml) and [ibm-privileged-scc.yaml](/assets/automation/bai/ibm-privileged-scc.yaml) files to your working directory and execute the following commands:\n\n```\noc apply -f ibm-anyuid-scc.yaml\noc apply -f ibm-privileged-scc.yaml\n\n```\n\nDownload [bai-psp.yaml](/assets/automation/bai/bai-psp.yaml) file to your working directory.\n```\noc apply -f bai-psp.yaml\n\noc adm policy add-scc-to-user ibm-privileged-scc -z bai-prod-release-bai-psp-sa\noc adm policy add-scc-to-user ibm-anyuid-scc -z bai-prod-release-bai-psp-sa\n\noc adm policy add-scc-to-group ibm-anyuid-scc system:authenticated\noc adm policy add-scc-to-user ibm-privileged-scc system:authenticated\n```\n\n### Push the BAI images to the registry\n\nIf you are installing BAI on IBM Cloud managed OCP cluster:\n\n- Login to the Docker registry:\n```\ndocker login -u $(oc whoami) -p $(oc whoami -t) docker-registry.default.svc:5000\n```\n- Download the `loadimages.sh` script to your working directory:\n```\nwget https://raw.githubusercontent.com/icp4a/cert-kubernetes/19.0.1/scripts/loadimages.sh\nchmod +x loadimages.sh\n```\n- Load the images:\n```\n./loadimages.sh -p ICP4A19.0.1-bai.tgz -r docker-registry.default.svc:5000/baiproject\n```\n\nIf you are installing BAI on an on-prem OCP:\n\n- Find the url of the cluster registry:\n```\noc -n default get route\n# search for route to docker-registry\nsudo docker login -u $(oc whoami) -p $(oc whoami -t) <route_to_docker_registry>\n```\n\n- Download the `loadimages.sh` script to your working directory:\n```\nwget https://raw.githubusercontent.com/icp4a/cert-kubernetes/19.0.1/scripts/loadimages.sh\nchmod +x loadimages.sh\n```\n- Load the images:\n```\nsudo ./loadimages.sh -p ICP4A19.0.1-bai.tgz -r <route_to_docker_registry>/baiproject\n```\n\n### Set up the persistent volumes\n\nRun the following commands to create the required PV folders in NFS, where `/data/persistentvolumes/` is the mounted directory of your NFS server:\n\n```\nsudo mkdir -p /data/persistentvolumes/bai/ibm-bai-pv\nsudo chown 9999:9999 /data/persistentvolumes/bai/ibm-bai-pv\nsudo chmod 770 /data/persistentvolumes/bai/ibm-bai-pv\n\nmkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-0\nmkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-1\nmkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-2\nmkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-3\nsudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-0\nsudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-1\nsudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-2\nsudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-3\nsudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-0\nsudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-1\nsudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-2\nsudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-3\n\nmkdir /data/persistentvolumes/bai/es-snapshots-pv\nsudo chown 1000:1000 /data/persistentvolumes/bai/es-snapshots-pv\nsudo chmod 770 /data/persistentvolumes/bai/es-snapshots-pv\n```\n\nDownload the [`bai-pv.yaml`](/assets/automation/bai/bai-pv.yaml) PV configuration file to your working directory. Edit the file and replace the IP address of the NFS server by the name or the IP address of your server. Modify the path of the directory if necessary. \n\n```\t \noc apply -f bai-pv.yaml\n```\n\n### Deploy BAI\n\n- Download the [`values.yaml`](/assets/automation/bai/values.yaml) file to your working directory. Make sure that the Kafka `bootstrapServers` name corresponds to the name from the *Set-up the Kafka bootstrap server* section. \n\n- To update the operator configuration, copy this [`my_ibm_cp4a_cr_3.yaml`](/assets/automation/bai/my_ibm_cp4a_cr_3.yaml) template file in your working directory and update it as needed. You can highlight the BAI configuration sections that need your attention by doing a diff with the template file found in [`my_ibm_cp4a_cr_2.yaml`](/assets/automation/bas/my_ibm_cp4a_cr_2.yaml)\n\n- Apply the updated custom resource definition file:\n```\noc apply -f my_ibm_icp4a_cr_3.yaml\n```\n\nYou should see the following new pods deployed:\n```\nNAME                                              READY   STATUS      RESTARTS   AGE\ncp4a-prod-bai-admin-d877c8b66-8xzs5               1/1     Running     0          15h\ncp4a-prod-bai-bawadv-97flq                        0/1     Completed   0          15h\ncp4a-prod-bai-bpmn-bvrtx                          0/1     Completed   0          15h\ncp4a-prod-bai-content-nqgl2                       0/1     Completed   0          15h\ncp4a-prod-bai-flink-jobmanager-5844df9685-2dkz5   1/1     Running     0          15h\ncp4a-prod-bai-flink-taskmanager-0                 1/1     Running     3          15h\ncp4a-prod-bai-flink-taskmanager-1                 1/1     Running     3          15h\ncp4a-prod-bai-flink-taskmanager-2                 1/1     Running     3          15h\ncp4a-prod-bai-flink-taskmanager-3                 1/1     Running     0          15h\ncp4a-prod-bai-flink-taskmanager-4                 1/1     Running     3          15h\ncp4a-prod-bai-flink-zk-0                          1/1     Running     0          15h\ncp4a-prod-bai-icm-pc2vw                           0/1     Completed   0          15h\ncp4a-prod-bai-odm-94dch                           0/1     Completed   0          15h\ncp4a-prod-bai-setup-wphkf                         0/1     Completed   0          15h\ncp4a-prod-ibm-dba-ek-client-547dfdbd94-5jdxl      1/1     Running     0          15h\ncp4a-prod-ibm-dba-ek-data-0                       1/1     Running     0          15h\ncp4a-prod-ibm-dba-ek-kibana-7cb766fcd7-5bpl8      1/1     Running     0          15h\ncp4a-prod-ibm-dba-ek-master-0                     1/1     Running     0          15h\ncp4a-prod-ibm-dba-ek-security-config-5hj2p        0/1     Completed   0          15h\n```\n\n### Expose the Kibana service\n\nDownload [`route.yaml`](/assets/automation/bai/route.yaml) file to your working directory and run the following command:\n\n```\noc apply -f route.yaml\n```\n\n### Accessing the Kibana dashboards\n\nYou can find the URL of Kibana in the Application Console of your cluster, in the `Services` tab of your BAI project. \n\nThe URL should have the following form:\n`https://bai-prod-release-ibm-dba-ek-kibana-baiproject.<your-cluster-ID>.<cloud-zone>.containers.appdomain.cloud`.\n\nThe credential are `admin` for user name and `passw0rd` for password.\n\n## Uninstall\n\n### Uninstall Kafka:\n\n```\nhelm delete my-kafka  --purge\noc delete pvc datadir-my-kafka-0\noc delete pvc datadir-my-kafka-1\noc delete pvc datadir-my-kafka-2\n```\n\nIf you created your PVs manually, execute:\n```\noc delete -f kafka-pv.yaml\n```\n\n### Uninstall BAI\n```\nhelm delete bai-prod-release --purge\noc delete pvc data-bai-prod-release-ibm-dba-ek-data-0\noc delete pvc data-bai-prod-release-ibm-dba-ek-data-1\noc delete pvc data-bai-prod-release-ibm-dba-ek-data-2\noc delete pvc data-bai-prod-release-ibm-dba-ek-data-3\noc delete pvc data-bai-prod-release-ibm-dba-ek-master-0\noc delete -f bai-pv.yaml\noc delete -f route.yaml\n```\n\n## Other useful references to documentation\n\n- [BAI install on on Red Hat OpenShift on IBM Cloud](https://github.ibm.com/dba/cert-kubernetes/blob/19.0.1/BAI/platform/README_ROKS.md)\n","type":"Mdx","contentDigest":"03729093848ded77b411ca29e97e2657","counter":519,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Business Automation Insights","weight":700},"exports":{},"rawBody":"---\ntitle: Business Automation Insights\nweight: 700\n---\n\n### Log in to you OCP cluster\nSee the [Prerequisites](/content/automation/pre-requisites) chapter for details on logging in to your OCP cluster.\n\n## Install Kafka\n\n### Add the Helm repository\n- Execute:\n```\nhelm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator\n```\n\n### Create the Kafka project\n- Execute:\n```\noc new-project kafka\n```\n\n- Make sure you are working from the `kafka` project, then grant the tiller server `edit` access to current project:\n```\noc project kafka\noc adm policy add-role-to-user edit \"system:serviceaccount:tiller:tiller\"\n```\n\n- Add the Security Context Constraints (CSS) `anyuid` to the Service Account `default` of the project:\n```\noc adm policy add-scc-to-user anyuid -z default\n```\n### Create PVs if necessary\nIf your cluster does not support dynamic persistence volume, download [kafka-pv.yaml](/assets/automation/bai/kafka-pv.yaml) and execute:\n```\noc apply -f kafka-pv.yaml\n```\n\n### Install the Helm chart\n\nDownload the [kafka-values.yaml](/assets/automation/bai/kafka-values.yaml) file to your working directory and run the following command:\n```\nhelm install incubator/kafka --name my-kafka --namespace kafka -f kafka-values.yaml\n```\nThis will deploy [Apache Kafka](https://github.com/helm/charts/tree/master/incubator/kafka) and [Zookeeper](https://github.com/helm/charts/tree/master/incubator/zookeeper).\n\nKeep `my-kafka` as a release name because the rest of the installation assumes it. \n\n### Set-up the Kafka bootstrap server\n\nIf access is from inside the OpenShift cluster environment, then the bootstrap server is `my-kafka-headless.kafka.svc.cluster.local:9092`.\n\nIf access is from an external system, you need to perform the following steps:\n\n1 - Retrieve the OpenShift ingress address with the following command: \n![Kafka ingress](/assets/automation//images/rhos-kafka1.png)\n\n2 - Use this ingress address to set the bootstrap server to `{rhos-ingress-ip}:31090,{rhos-ingress-ip}:31091,{rhos-ingress-ip}:31092`.\n\n3 - Add the line `{rhos-ingress-ip} kafka.cluster.local` to the `/etc/hosts` file.\n\n\n## Install Business Automation Insights (BAI)\n\n### Download the BAI PPA \nDownload the following PPA from [IBM Passport Advantage](https://www.ibm.com/software/passportadvantage) to your working-directory:\n\n- *IBM Cloud Pak for Automation v19.0.1 - Business Operation Insights for Certified Kubernetes Multiplatform English (CC222EN)*\n\nThe downloaded archive should be `ICP4A19.0.1-bai.tgz`.\n\n### Create the BAI project\n- Create a new OpenShift project for BAI:\n```\noc new-project baiproject\n```\n- Make sure you are working from your newly created BAI project, then grant the tiller server `edit` access to current project:\n```\noc project baiproject\noc adm policy add-role-to-user edit \"system:serviceaccount:tiller:tiller\"\n```\n\n### Create a ServiceAccount and update the SCCs\n\nCheck if `ibm-anyuid-scc` and `ibm-privileged-scc` exist in your cluster:\n```\noc get scc ibm-anyuid-scc\noc get scc ibm-privileged-scc\n```\n\nIf they don't exist, download [ibm-anyuid-scc.yaml](/assets/automation/bai/ibm-anyuid-scc.yaml) and [ibm-privileged-scc.yaml](/assets/automation/bai/ibm-privileged-scc.yaml) files to your working directory and execute the following commands:\n\n```\noc apply -f ibm-anyuid-scc.yaml\noc apply -f ibm-privileged-scc.yaml\n\n```\n\nDownload [bai-psp.yaml](/assets/automation/bai/bai-psp.yaml) file to your working directory.\n```\noc apply -f bai-psp.yaml\n\noc adm policy add-scc-to-user ibm-privileged-scc -z bai-prod-release-bai-psp-sa\noc adm policy add-scc-to-user ibm-anyuid-scc -z bai-prod-release-bai-psp-sa\n\noc adm policy add-scc-to-group ibm-anyuid-scc system:authenticated\noc adm policy add-scc-to-user ibm-privileged-scc system:authenticated\n```\n\n### Push the BAI images to the registry\n\nIf you are installing BAI on IBM Cloud managed OCP cluster:\n\n- Login to the Docker registry:\n```\ndocker login -u $(oc whoami) -p $(oc whoami -t) docker-registry.default.svc:5000\n```\n- Download the `loadimages.sh` script to your working directory:\n```\nwget https://raw.githubusercontent.com/icp4a/cert-kubernetes/19.0.1/scripts/loadimages.sh\nchmod +x loadimages.sh\n```\n- Load the images:\n```\n./loadimages.sh -p ICP4A19.0.1-bai.tgz -r docker-registry.default.svc:5000/baiproject\n```\n\nIf you are installing BAI on an on-prem OCP:\n\n- Find the url of the cluster registry:\n```\noc -n default get route\n# search for route to docker-registry\nsudo docker login -u $(oc whoami) -p $(oc whoami -t) <route_to_docker_registry>\n```\n\n- Download the `loadimages.sh` script to your working directory:\n```\nwget https://raw.githubusercontent.com/icp4a/cert-kubernetes/19.0.1/scripts/loadimages.sh\nchmod +x loadimages.sh\n```\n- Load the images:\n```\nsudo ./loadimages.sh -p ICP4A19.0.1-bai.tgz -r <route_to_docker_registry>/baiproject\n```\n\n### Set up the persistent volumes\n\nRun the following commands to create the required PV folders in NFS, where `/data/persistentvolumes/` is the mounted directory of your NFS server:\n\n```\nsudo mkdir -p /data/persistentvolumes/bai/ibm-bai-pv\nsudo chown 9999:9999 /data/persistentvolumes/bai/ibm-bai-pv\nsudo chmod 770 /data/persistentvolumes/bai/ibm-bai-pv\n\nmkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-0\nmkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-1\nmkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-2\nmkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-3\nsudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-0\nsudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-1\nsudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-2\nsudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-3\nsudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-0\nsudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-1\nsudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-2\nsudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-3\n\nmkdir /data/persistentvolumes/bai/es-snapshots-pv\nsudo chown 1000:1000 /data/persistentvolumes/bai/es-snapshots-pv\nsudo chmod 770 /data/persistentvolumes/bai/es-snapshots-pv\n```\n\nDownload the [`bai-pv.yaml`](/assets/automation/bai/bai-pv.yaml) PV configuration file to your working directory. Edit the file and replace the IP address of the NFS server by the name or the IP address of your server. Modify the path of the directory if necessary. \n\n```\t \noc apply -f bai-pv.yaml\n```\n\n### Deploy BAI\n\n- Download the [`values.yaml`](/assets/automation/bai/values.yaml) file to your working directory. Make sure that the Kafka `bootstrapServers` name corresponds to the name from the *Set-up the Kafka bootstrap server* section. \n\n- To update the operator configuration, copy this [`my_ibm_cp4a_cr_3.yaml`](/assets/automation/bai/my_ibm_cp4a_cr_3.yaml) template file in your working directory and update it as needed. You can highlight the BAI configuration sections that need your attention by doing a diff with the template file found in [`my_ibm_cp4a_cr_2.yaml`](/assets/automation/bas/my_ibm_cp4a_cr_2.yaml)\n\n- Apply the updated custom resource definition file:\n```\noc apply -f my_ibm_icp4a_cr_3.yaml\n```\n\nYou should see the following new pods deployed:\n```\nNAME                                              READY   STATUS      RESTARTS   AGE\ncp4a-prod-bai-admin-d877c8b66-8xzs5               1/1     Running     0          15h\ncp4a-prod-bai-bawadv-97flq                        0/1     Completed   0          15h\ncp4a-prod-bai-bpmn-bvrtx                          0/1     Completed   0          15h\ncp4a-prod-bai-content-nqgl2                       0/1     Completed   0          15h\ncp4a-prod-bai-flink-jobmanager-5844df9685-2dkz5   1/1     Running     0          15h\ncp4a-prod-bai-flink-taskmanager-0                 1/1     Running     3          15h\ncp4a-prod-bai-flink-taskmanager-1                 1/1     Running     3          15h\ncp4a-prod-bai-flink-taskmanager-2                 1/1     Running     3          15h\ncp4a-prod-bai-flink-taskmanager-3                 1/1     Running     0          15h\ncp4a-prod-bai-flink-taskmanager-4                 1/1     Running     3          15h\ncp4a-prod-bai-flink-zk-0                          1/1     Running     0          15h\ncp4a-prod-bai-icm-pc2vw                           0/1     Completed   0          15h\ncp4a-prod-bai-odm-94dch                           0/1     Completed   0          15h\ncp4a-prod-bai-setup-wphkf                         0/1     Completed   0          15h\ncp4a-prod-ibm-dba-ek-client-547dfdbd94-5jdxl      1/1     Running     0          15h\ncp4a-prod-ibm-dba-ek-data-0                       1/1     Running     0          15h\ncp4a-prod-ibm-dba-ek-kibana-7cb766fcd7-5bpl8      1/1     Running     0          15h\ncp4a-prod-ibm-dba-ek-master-0                     1/1     Running     0          15h\ncp4a-prod-ibm-dba-ek-security-config-5hj2p        0/1     Completed   0          15h\n```\n\n### Expose the Kibana service\n\nDownload [`route.yaml`](/assets/automation/bai/route.yaml) file to your working directory and run the following command:\n\n```\noc apply -f route.yaml\n```\n\n### Accessing the Kibana dashboards\n\nYou can find the URL of Kibana in the Application Console of your cluster, in the `Services` tab of your BAI project. \n\nThe URL should have the following form:\n`https://bai-prod-release-ibm-dba-ek-kibana-baiproject.<your-cluster-ID>.<cloud-zone>.containers.appdomain.cloud`.\n\nThe credential are `admin` for user name and `passw0rd` for password.\n\n## Uninstall\n\n### Uninstall Kafka:\n\n```\nhelm delete my-kafka  --purge\noc delete pvc datadir-my-kafka-0\noc delete pvc datadir-my-kafka-1\noc delete pvc datadir-my-kafka-2\n```\n\nIf you created your PVs manually, execute:\n```\noc delete -f kafka-pv.yaml\n```\n\n### Uninstall BAI\n```\nhelm delete bai-prod-release --purge\noc delete pvc data-bai-prod-release-ibm-dba-ek-data-0\noc delete pvc data-bai-prod-release-ibm-dba-ek-data-1\noc delete pvc data-bai-prod-release-ibm-dba-ek-data-2\noc delete pvc data-bai-prod-release-ibm-dba-ek-data-3\noc delete pvc data-bai-prod-release-ibm-dba-ek-master-0\noc delete -f bai-pv.yaml\noc delete -f route.yaml\n```\n\n## Other useful references to documentation\n\n- [BAI install on on Red Hat OpenShift on IBM Cloud](https://github.ibm.com/dba/cert-kubernetes/blob/19.0.1/BAI/platform/README_ROKS.md)\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/automation/install-bai/index.mdx"}}}}