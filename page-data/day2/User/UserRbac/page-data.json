{"componentChunkName":"component---src-pages-day-2-user-user-rbac-index-mdx","path":"/day2/User/UserRbac/","result":{"pageContext":{"frontmatter":{"title":"User Management / Authentication / Authorization","description":"OCP Day2 User","keywords":"ocp, day2, user"},"relativePagePath":"/day2/User/UserRbac/index.mdx","titleType":"page","MdxNode":{"id":"01a10922-153a-5583-be92-92b6e9da787d","children":[],"parent":"2e3fd018-db40-5611-b2d1-194d630d6c29","internal":{"content":"---\ntitle: User Management / Authentication / Authorization\ndescription: OCP Day2 User\nkeywords: 'ocp, day2, user'\n---\n\n\n# Configure Groups and Cluster Role-Based Access Control\nWe will show you how to configure groups and cluster role-based access \ncontrol (RBAC) on your OpenShift Cluster.  We will cover the following items:\n- Create a cluster role binding to configure a group with direct \ncluster-admin access privileges\n- Configure a group with cluster-admin access through the **sudoer** cluster role\n- Restrict project self-provisioning to specific user groups\n- Configure project request messages   \n\nThe top two items tell you how to delegate the cluster administrative \nprivileges.\n\n## Create a cluster role binding to configure a group with direct cluster-admin access privileges\nYou assign the **cluster-admin** access to a group called **local-admin** which \nwe have already created in our environment. \nHere are steps to configure direct cluster-admin access.\n\n1. Check the list of groups and associated users in each group as follows:\n\n    ```\n    $ oc --user=admin get groups\n    NAME             USERS\n    local-admin      alice\n    ocp-platform     david, admin1, admin2, admin\n    ocp-production   karla, prod1, prod2, admin, redhat\n    ocp-users        andrew, marina, karla, david, portal1, portal2, payment1, payment2, prod1, prod2, platform1, platform2, admin1, admin2, admin\n    paymentapp       marina, payment1, payment2\n    portalapp        andrew, portal1, portal2\n    $ \n    ```\n\n    In our environment, there is a **local-admin** group with an **alice** user.  We \n    will use those in the following steps.\n\n    Note that we have disabled the **kubeadmin** in your environment as described \n    in the [VMware User](../User/UserVmware) page. Therefore, the steps in \n    this page must be \n    performed using the **system:admin** user account which means that we will \n    add the **--user=admin** option with the `oc` command if necessary.\n\n2. Create a cluster role binding to give **cluster-admin** rights to members \nof the **local-admin** group as follows:\n   ```\n   $ oc --user=admin adm policy add-cluster-role-to-group cluster-admin local-admin \n   clusterrole.rbac.authorization.k8s.io/cluster-admin added: \"local-admin\"\n   $ \n   ```\n\n3. To confirm that users who belong to the **local-admin** group has the \nadministrative access, you login as the **alice** user as follows:\n   ```\n   $ oc login -u alice -p 'p4ssw0rd'\n   Login successful.\n   \n   ... OUTPUT OMITTED ...\n   \n   ```\n\n4. Then, confirm full administrative access with any verb to any resource \ntype such as \"boo\" and \"bar\" as follows:\n   ```\n   $ oc auth can-i foo bar\n   Warning: the server doesn't have a resource type 'bar'\n   yes\n   $ \n   ```\n\n   You see **yes** as the command output which means that the user has the \nadministrative access. You can ignore the warning message above.\nThe **local-admin** group now has the administrative access and the users\nthat belong to that group can run the command which requires the administrative \naccess without specifying the **--user=admin** option.  \n\n## Configure a group with cluster-admin access through the sudoer cluster role\nWe will grant the **sudoer** rights to the group and show you how to use the \nadministrative access with the users in that group.  \n\n1. Check the list of groups and acciated users in each group as follows:\n    ```\n    $ oc --user=admin get groups\n    NAME             USERS\n    local-admin      alice\n    ocp-platform     david, admin1, admin2, admin\n    ocp-production   karla, prod1, prod2, admin, redhat\n    ocp-users        andrew, marina, karla, david, portal1, portal2, payment1, payment2, prod1, prod2, platform1, platform2, admin1, admin2, admin\n    paymentapp       marina, payment1, payment2\n    portalapp        andrew, portal1, portal2\n    $ \n    ```\n\n   In our environment, there is **ocp-platform** group with a **david** user.  We will use those in the following steps.\n\n2.\tCreate a cluster role binding to grant **sudoer** rights to members of \nthe **ocp-platform** group:\n   ```\n   $ oc adm policy add-cluster-role-to-group sudoer ocp-platform\n   clusterrole.rbac.authorization.k8s.io/sudoer added: \"ocp-platform\"\n   ```\n\n3.\tLog in as user **david** that belongs to the **ocp-platform** group to \nconfirm cluster administrative privileges.\n   ```\n   $ oc login -u david -p 'r3dh4t1!'\n   Login successful.\n   \n   You don't have any projects. You can try to create a new project, by running\n   \n   oc new-project <projectname>\n   ```\n\n4.\tTest direct cluster administrative access to confirm that it is not \navailable as follows:\n   ```\n   $ oc auth can-i foo bar\n   Warning: the server doesn't have a resource type 'bar'\n   no\n   ```  \n\n   You see **no** as the command output which means that the user does not \n   have the aminitrative access. You can ignore the warning message above.\n\n5.\tRepeat the same command with the **--as=system:admin** option using the \n**system:admin** account as follows:\n   ```\n   $ oc --as=system:admin auth can-i foo bar\n   Warning: the server doesn't have a resource type 'bar'\n   yes\n   ```\n\n   You see **yes** as the command output which means that the user has the \n   administrative access. You can ignore the warning message above.\n\n   As step 4 and 5 showed you, the **sudoer** rights can provide the \nadministrative access by specifying the **--as=system:admin** option.  \n\n   Now, you have delegated cluster administrative access to users such as \n   **alice** and **david** in our example. Therefore, it is no longer \n   appropriate to continue using TLS certificates to access **system:admin**.  \n\n\n### Remove Credentials for system:admin from Configuration File\nBefore we move to the next items which are \"Restrict project \nself-provisioning to specific user groups\" and \"Configure project request \nmessages\", we will show you how to remove the **cluster-admin** access from the \nuser to whom you previously delegated **cluster-admin** access.  In our example, \nthe user **alice** is the one.  \n\nWe remove the credentials for **system:admin** from the configuration file as \nfollows:  \n\n1.\tYou can get the API URL with oc command as follow.\n    ```\n    $oc whoami --show-server\n    ```\n\n2.\tRemove your kube configuration file.\n    ```\n    $ rm -f $HOME/.kube/config\n    ```\n\n3.\tLog in again as the **alice** user.  Answer **y** to the question \n\"Use insecure connections?\" which will reset your environment and \nauthenticate for the user **alice**.\n   ```\n   $ oc login -u alice -p 'p4ssw0rd' $API_URL\n   The server uses a certificate signed by an unknown authority.\n   You can bypass the certificate check, but any data you send to the server could be intercepted by others.\n   Use insecure connections? (y/n): y\n   \n   Login successful.\n   \n   ... output omitted ...\n   ```  \n\n## Restrict Access for Project Self-Provisioning\nWe will cover two items such as \"Restrict project self-provisioning to specific user groups\" and \"Configure project request messages\" in this section. We will demonstrate 3 topics as folow:\n- You remove the user’s default permission to create their own projects and allow only production administrators to create projects.\n- You set a message for users who attempt to create projects without appropriate permissions. \n- You allow users from the ocp-production group to create their own projects.  \n\n### Remove OAuth Authenticated Access to Role self-provisioner\n\n1.\tView the **self-provisioners** cluster role binding:\n\n    ```\n    $ oc get clusterrolebinding self-provisioners -o yaml\n    apiVersion: rbac.authorization.k8s.io/v1\n    kind: ClusterRoleBinding\n    metadata:\n      annotations:\n        rbac.authorization.kubernetes.io/autoupdate: \"true\"\n      creationTimestamp: \"2019-05-25T18:46:20Z\"\n      name: self-provisioners\n      resourceVersion: \"6265\"\n      selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/self-provisioners\n      uid: 6304fb71-7f1d-11e9-a345-0624d0163962\n    roleRef:\n      apiGroup: rbac.authorization.k8s.io\n      kind: ClusterRole\n      name: self-provisioner\n    subjects:\n    - apiGroup: rbac.authorization.k8s.io\n      kind: Group\n      name: system:authenticated:oauth\n    ```\n\n   This role binding has the **rbac.authorization.kubernetes.io/autoupdate: \"true\"** \n   annotation.\n\n   Typically, you remove a cluster role binding with \n   `oc adm policy remove-cluster-role-from-group` or \n   `oc adm policy remove-cluster-role-from-user`. \n   Because the autoupdate process restores the access, you cannot use either \n   approach with the **self-provisioners** role binding yet.\n\n2.\tSet the **rbac.authorization.kubernetes.io/autoupdate** annotation on the \n**self-provisioners** cluster role binding to **false**:\n   ```\n   $ oc annotate clusterrolebinding self-provisioners --overwrite \\\n       'rbac.authorization.kubernetes.io/autoupdate=false'\n   clusterrolebinding.rbac.authorization.k8s.io/self-provisioners annotated\n   ```\n\n3.\tRemove the **system:authenticated:oauth** group from the \n**self-provisioners** cluster role binding:\n   ```\n   $ oc adm policy remove-cluster-role-from-group \\\n       self-provisioner system:authenticated:oauth\n   clusterrole.rbac.authorization.k8s.io/self-provisioner removed: \"system:authenticated:oauth\"\n   ```\n\n4.\tConfirm that the **self-provisioners** cluster role binding still exists \nand has the **rbac.authorization.kubernetes.io/autoupdate: \"false\"** \nannotation, but no subjects:\n   ```\n   $ oc get clusterrolebinding self-provisioners -o yaml\n   apiVersion: rbac.authorization.k8s.io/v1\n   kind: ClusterRoleBinding\n   metadata:\n     annotations:\n       rbac.authorization.kubernetes.io/autoupdate: \"false\"\n     creationTimestamp: \"2019-05-25T18:46:20Z\"\n     name: self-provisioners\n     resourceVersion: \"1773477\"\n     selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/self-provisioners\n     uid: 6304fb71-7f1d-11e9-a345-0624d0163962\n   roleRef:\n     apiGroup: rbac.authorization.k8s.io\n     kind: ClusterRole\n     name: self-provisioner\n   ```\n\n### Configure Message with Project Request Instructions\n\nWith project self-provisioning disabled, it is helpful to provide users with \na message to inform them of the correct way to request a new project in \nOpenShift.\n\n1.\tCreate a file called **projects-config.patch.json** with the following \nJSON patch for the project request message:\n   ```\n   {\n     \"spec\": {\n       \"projectRequestMessage\": \"Please create projects using the portal http://portal.company.internal/provision or PaaS Support at paas-support@example.com\"\n     }\n   }\n   ```\n\n2.\tPatch the cluster resource of kind **projects.config.openshift.io** with \nthe patch file:\n   ```\n   $ oc patch projects.config.openshift.io cluster --type=merge \\\n       -p \"$(cat projects-config.patch.json)\"\n   project.config.openshift.io/cluster patched\n   ```\n\n3.\tLog in as the non-admin **andrew** user and attempt to create a project \nto verify that the project request message is active:\n   ```\n   $ oc login -u andrew -p 'r3dh4t1!'\n   Login successful.\n   \n   You don't have any projects. Contact your system administrator to request a project.\n   $ oc new-project test\n   Error from server (Forbidden): Please create project using the portal http://portal.company.internal/provision or PaaS Support at paas-support@example.com\n   ```\n\n   If you do not see the full error message, wait a minute or two and try \n   again. It takes a little while for the operator to update the configuration \n   after the patch is applied.\n\n\n\n### Allow Production Administrators to Create Projects\nYou configure the **ocp-production** group that the LDAP group sync created \nso that its members can create projects.\n\n1.\tLog in as the **alice** cluster-admin user:\n    ```\n    $ oc login -u alice -p 'p4ssw0rd'\n    ... output omitted ...\n    ```\n\n2.\tUse `oc adm policy` again, but this time add the cluster role of \n**self-provisioner** to the **ocp-production** group:\n   ```\n   $ oc adm policy add-cluster-role-to-group self-provisioner ocp-production\n   ```\n\n3.\tLog in to the system as the **karla** user, a member of the \n**ocp-production** group:\n   ```\n   $ oc login -u karla -p 'r3dh4t1!'\n   Login successful.\n   ```\n\n   You don't have any projects. You can try to create a new project, by \n   running:\n   ```\n   oc new-project <projectname>\n   ```\n\n4.\tAttempt to create a project as the **karla** user and verify that this is \nnow successful:\n   ```\n   $ oc new-project test\n   Now using project \"test\" on server \"https://api.cluster-7ae9.sandbox134.opentlc.com:6443\".\n   ```\n\n   You can add applications to this project with the `new-app` command. \n   For example, try:\n   ```\n   oc new-app django-psql-example\n   ```\n\n   to build a new example application in Python. Or use `kubectl` to deploy a \n   simple Kubernetes application:\n   ```\n   kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node\n   ```\n\n5.\tRemove the **test** project:\n    ```\n    $ oc delete project test\n    project.project.openshift.io \"test\" deleted\n    ```\n","type":"Mdx","contentDigest":"222de005e3975a5666bda16d315238e9","counter":536,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"User Management / Authentication / Authorization","description":"OCP Day2 User","keywords":"ocp, day2, user"},"exports":{},"rawBody":"---\ntitle: User Management / Authentication / Authorization\ndescription: OCP Day2 User\nkeywords: 'ocp, day2, user'\n---\n\n\n# Configure Groups and Cluster Role-Based Access Control\nWe will show you how to configure groups and cluster role-based access \ncontrol (RBAC) on your OpenShift Cluster.  We will cover the following items:\n- Create a cluster role binding to configure a group with direct \ncluster-admin access privileges\n- Configure a group with cluster-admin access through the **sudoer** cluster role\n- Restrict project self-provisioning to specific user groups\n- Configure project request messages   \n\nThe top two items tell you how to delegate the cluster administrative \nprivileges.\n\n## Create a cluster role binding to configure a group with direct cluster-admin access privileges\nYou assign the **cluster-admin** access to a group called **local-admin** which \nwe have already created in our environment. \nHere are steps to configure direct cluster-admin access.\n\n1. Check the list of groups and associated users in each group as follows:\n\n    ```\n    $ oc --user=admin get groups\n    NAME             USERS\n    local-admin      alice\n    ocp-platform     david, admin1, admin2, admin\n    ocp-production   karla, prod1, prod2, admin, redhat\n    ocp-users        andrew, marina, karla, david, portal1, portal2, payment1, payment2, prod1, prod2, platform1, platform2, admin1, admin2, admin\n    paymentapp       marina, payment1, payment2\n    portalapp        andrew, portal1, portal2\n    $ \n    ```\n\n    In our environment, there is a **local-admin** group with an **alice** user.  We \n    will use those in the following steps.\n\n    Note that we have disabled the **kubeadmin** in your environment as described \n    in the [VMware User](../User/UserVmware) page. Therefore, the steps in \n    this page must be \n    performed using the **system:admin** user account which means that we will \n    add the **--user=admin** option with the `oc` command if necessary.\n\n2. Create a cluster role binding to give **cluster-admin** rights to members \nof the **local-admin** group as follows:\n   ```\n   $ oc --user=admin adm policy add-cluster-role-to-group cluster-admin local-admin \n   clusterrole.rbac.authorization.k8s.io/cluster-admin added: \"local-admin\"\n   $ \n   ```\n\n3. To confirm that users who belong to the **local-admin** group has the \nadministrative access, you login as the **alice** user as follows:\n   ```\n   $ oc login -u alice -p 'p4ssw0rd'\n   Login successful.\n   \n   ... OUTPUT OMITTED ...\n   \n   ```\n\n4. Then, confirm full administrative access with any verb to any resource \ntype such as \"boo\" and \"bar\" as follows:\n   ```\n   $ oc auth can-i foo bar\n   Warning: the server doesn't have a resource type 'bar'\n   yes\n   $ \n   ```\n\n   You see **yes** as the command output which means that the user has the \nadministrative access. You can ignore the warning message above.\nThe **local-admin** group now has the administrative access and the users\nthat belong to that group can run the command which requires the administrative \naccess without specifying the **--user=admin** option.  \n\n## Configure a group with cluster-admin access through the sudoer cluster role\nWe will grant the **sudoer** rights to the group and show you how to use the \nadministrative access with the users in that group.  \n\n1. Check the list of groups and acciated users in each group as follows:\n    ```\n    $ oc --user=admin get groups\n    NAME             USERS\n    local-admin      alice\n    ocp-platform     david, admin1, admin2, admin\n    ocp-production   karla, prod1, prod2, admin, redhat\n    ocp-users        andrew, marina, karla, david, portal1, portal2, payment1, payment2, prod1, prod2, platform1, platform2, admin1, admin2, admin\n    paymentapp       marina, payment1, payment2\n    portalapp        andrew, portal1, portal2\n    $ \n    ```\n\n   In our environment, there is **ocp-platform** group with a **david** user.  We will use those in the following steps.\n\n2.\tCreate a cluster role binding to grant **sudoer** rights to members of \nthe **ocp-platform** group:\n   ```\n   $ oc adm policy add-cluster-role-to-group sudoer ocp-platform\n   clusterrole.rbac.authorization.k8s.io/sudoer added: \"ocp-platform\"\n   ```\n\n3.\tLog in as user **david** that belongs to the **ocp-platform** group to \nconfirm cluster administrative privileges.\n   ```\n   $ oc login -u david -p 'r3dh4t1!'\n   Login successful.\n   \n   You don't have any projects. You can try to create a new project, by running\n   \n   oc new-project <projectname>\n   ```\n\n4.\tTest direct cluster administrative access to confirm that it is not \navailable as follows:\n   ```\n   $ oc auth can-i foo bar\n   Warning: the server doesn't have a resource type 'bar'\n   no\n   ```  \n\n   You see **no** as the command output which means that the user does not \n   have the aminitrative access. You can ignore the warning message above.\n\n5.\tRepeat the same command with the **--as=system:admin** option using the \n**system:admin** account as follows:\n   ```\n   $ oc --as=system:admin auth can-i foo bar\n   Warning: the server doesn't have a resource type 'bar'\n   yes\n   ```\n\n   You see **yes** as the command output which means that the user has the \n   administrative access. You can ignore the warning message above.\n\n   As step 4 and 5 showed you, the **sudoer** rights can provide the \nadministrative access by specifying the **--as=system:admin** option.  \n\n   Now, you have delegated cluster administrative access to users such as \n   **alice** and **david** in our example. Therefore, it is no longer \n   appropriate to continue using TLS certificates to access **system:admin**.  \n\n\n### Remove Credentials for system:admin from Configuration File\nBefore we move to the next items which are \"Restrict project \nself-provisioning to specific user groups\" and \"Configure project request \nmessages\", we will show you how to remove the **cluster-admin** access from the \nuser to whom you previously delegated **cluster-admin** access.  In our example, \nthe user **alice** is the one.  \n\nWe remove the credentials for **system:admin** from the configuration file as \nfollows:  \n\n1.\tYou can get the API URL with oc command as follow.\n    ```\n    $oc whoami --show-server\n    ```\n\n2.\tRemove your kube configuration file.\n    ```\n    $ rm -f $HOME/.kube/config\n    ```\n\n3.\tLog in again as the **alice** user.  Answer **y** to the question \n\"Use insecure connections?\" which will reset your environment and \nauthenticate for the user **alice**.\n   ```\n   $ oc login -u alice -p 'p4ssw0rd' $API_URL\n   The server uses a certificate signed by an unknown authority.\n   You can bypass the certificate check, but any data you send to the server could be intercepted by others.\n   Use insecure connections? (y/n): y\n   \n   Login successful.\n   \n   ... output omitted ...\n   ```  \n\n## Restrict Access for Project Self-Provisioning\nWe will cover two items such as \"Restrict project self-provisioning to specific user groups\" and \"Configure project request messages\" in this section. We will demonstrate 3 topics as folow:\n- You remove the user’s default permission to create their own projects and allow only production administrators to create projects.\n- You set a message for users who attempt to create projects without appropriate permissions. \n- You allow users from the ocp-production group to create their own projects.  \n\n### Remove OAuth Authenticated Access to Role self-provisioner\n\n1.\tView the **self-provisioners** cluster role binding:\n\n    ```\n    $ oc get clusterrolebinding self-provisioners -o yaml\n    apiVersion: rbac.authorization.k8s.io/v1\n    kind: ClusterRoleBinding\n    metadata:\n      annotations:\n        rbac.authorization.kubernetes.io/autoupdate: \"true\"\n      creationTimestamp: \"2019-05-25T18:46:20Z\"\n      name: self-provisioners\n      resourceVersion: \"6265\"\n      selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/self-provisioners\n      uid: 6304fb71-7f1d-11e9-a345-0624d0163962\n    roleRef:\n      apiGroup: rbac.authorization.k8s.io\n      kind: ClusterRole\n      name: self-provisioner\n    subjects:\n    - apiGroup: rbac.authorization.k8s.io\n      kind: Group\n      name: system:authenticated:oauth\n    ```\n\n   This role binding has the **rbac.authorization.kubernetes.io/autoupdate: \"true\"** \n   annotation.\n\n   Typically, you remove a cluster role binding with \n   `oc adm policy remove-cluster-role-from-group` or \n   `oc adm policy remove-cluster-role-from-user`. \n   Because the autoupdate process restores the access, you cannot use either \n   approach with the **self-provisioners** role binding yet.\n\n2.\tSet the **rbac.authorization.kubernetes.io/autoupdate** annotation on the \n**self-provisioners** cluster role binding to **false**:\n   ```\n   $ oc annotate clusterrolebinding self-provisioners --overwrite \\\n       'rbac.authorization.kubernetes.io/autoupdate=false'\n   clusterrolebinding.rbac.authorization.k8s.io/self-provisioners annotated\n   ```\n\n3.\tRemove the **system:authenticated:oauth** group from the \n**self-provisioners** cluster role binding:\n   ```\n   $ oc adm policy remove-cluster-role-from-group \\\n       self-provisioner system:authenticated:oauth\n   clusterrole.rbac.authorization.k8s.io/self-provisioner removed: \"system:authenticated:oauth\"\n   ```\n\n4.\tConfirm that the **self-provisioners** cluster role binding still exists \nand has the **rbac.authorization.kubernetes.io/autoupdate: \"false\"** \nannotation, but no subjects:\n   ```\n   $ oc get clusterrolebinding self-provisioners -o yaml\n   apiVersion: rbac.authorization.k8s.io/v1\n   kind: ClusterRoleBinding\n   metadata:\n     annotations:\n       rbac.authorization.kubernetes.io/autoupdate: \"false\"\n     creationTimestamp: \"2019-05-25T18:46:20Z\"\n     name: self-provisioners\n     resourceVersion: \"1773477\"\n     selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/self-provisioners\n     uid: 6304fb71-7f1d-11e9-a345-0624d0163962\n   roleRef:\n     apiGroup: rbac.authorization.k8s.io\n     kind: ClusterRole\n     name: self-provisioner\n   ```\n\n### Configure Message with Project Request Instructions\n\nWith project self-provisioning disabled, it is helpful to provide users with \na message to inform them of the correct way to request a new project in \nOpenShift.\n\n1.\tCreate a file called **projects-config.patch.json** with the following \nJSON patch for the project request message:\n   ```\n   {\n     \"spec\": {\n       \"projectRequestMessage\": \"Please create projects using the portal http://portal.company.internal/provision or PaaS Support at paas-support@example.com\"\n     }\n   }\n   ```\n\n2.\tPatch the cluster resource of kind **projects.config.openshift.io** with \nthe patch file:\n   ```\n   $ oc patch projects.config.openshift.io cluster --type=merge \\\n       -p \"$(cat projects-config.patch.json)\"\n   project.config.openshift.io/cluster patched\n   ```\n\n3.\tLog in as the non-admin **andrew** user and attempt to create a project \nto verify that the project request message is active:\n   ```\n   $ oc login -u andrew -p 'r3dh4t1!'\n   Login successful.\n   \n   You don't have any projects. Contact your system administrator to request a project.\n   $ oc new-project test\n   Error from server (Forbidden): Please create project using the portal http://portal.company.internal/provision or PaaS Support at paas-support@example.com\n   ```\n\n   If you do not see the full error message, wait a minute or two and try \n   again. It takes a little while for the operator to update the configuration \n   after the patch is applied.\n\n\n\n### Allow Production Administrators to Create Projects\nYou configure the **ocp-production** group that the LDAP group sync created \nso that its members can create projects.\n\n1.\tLog in as the **alice** cluster-admin user:\n    ```\n    $ oc login -u alice -p 'p4ssw0rd'\n    ... output omitted ...\n    ```\n\n2.\tUse `oc adm policy` again, but this time add the cluster role of \n**self-provisioner** to the **ocp-production** group:\n   ```\n   $ oc adm policy add-cluster-role-to-group self-provisioner ocp-production\n   ```\n\n3.\tLog in to the system as the **karla** user, a member of the \n**ocp-production** group:\n   ```\n   $ oc login -u karla -p 'r3dh4t1!'\n   Login successful.\n   ```\n\n   You don't have any projects. You can try to create a new project, by \n   running:\n   ```\n   oc new-project <projectname>\n   ```\n\n4.\tAttempt to create a project as the **karla** user and verify that this is \nnow successful:\n   ```\n   $ oc new-project test\n   Now using project \"test\" on server \"https://api.cluster-7ae9.sandbox134.opentlc.com:6443\".\n   ```\n\n   You can add applications to this project with the `new-app` command. \n   For example, try:\n   ```\n   oc new-app django-psql-example\n   ```\n\n   to build a new example application in Python. Or use `kubectl` to deploy a \n   simple Kubernetes application:\n   ```\n   kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node\n   ```\n\n5.\tRemove the **test** project:\n    ```\n    $ oc delete project test\n    project.project.openshift.io \"test\" deleted\n    ```\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/day2/User/UserRbac/index.mdx"}}}}