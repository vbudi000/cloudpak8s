{"componentChunkName":"component---src-pages-day-2-user-index-mdx","path":"/day2/User/","result":{"pageContext":{"frontmatter":{"title":"User Management / Authentication / Authorization","description":"OpenShift Day2 User","keywords":"OpenShift, day2, user"},"relativePagePath":"/day2/User/index.mdx","titleType":"page","MdxNode":{"id":"9e074b6a-4f40-5733-a5d7-7b7346a7e969","children":[],"parent":"c50320f2-b174-5ce9-a4a9-dc16239b59d1","internal":{"content":"---\ntitle: User Management / Authentication / Authorization\ndescription: OpenShift Day2 User\nkeywords: 'OpenShift, day2, user'\n---\n\n\n## Authentication, Authorization & User Management\n\nOpenShift provides a rich set of Authentication and Authorization features \nfor securing access and Role Based Access Control (RBAC), to the platform and \napplication space (sometimes abbreviated as Security, Identity and Access \nManagement - SIAM). To ensure we clarify the difference, the following are \nthe key differences and industry norms for the two terms:\n- Authentication:  The ability for a user to identify themselves (via user/password, token or keys) as having an account to access the platform\n- Authorization:  The distinct roles and authorizations (RBAC), attached to this user that defines what actions they are authorized to perform on the platform (from anonymous, view only, project based to cluster admin, certain namespaces as some examples)\n\nFor a complete description on configuration and possible permutations, \nplease see the following links:\n\n[Understanding OpenShift Authentication](https://docs.openshift.com/container-platform/latest/authentication/understanding-authentication.html)\n\n[OpenShift Authentication](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.3/html/authentication/index)\n\n## Day 1 Platform\n\nDuring Day 0, it is normal to design and define which identity providers \n(i.e. Git, LDAP, Active Directory, Etc.), will be used along with their \nconfiguration during the installation (Day 1). While it is possible \n(and supported), to change identify provides post installation, changing \nidentity providers will require the use of User Mappings to map the current \nidentities to their new identity provider. For example, moving from Git where the \nidentity is a name to LDAP where the identity is an email address. For our \npurposes, these are the items that Day 2 should expect to be in-place and \ntested as part of Day 1:  \n\n- Identity provider configured and tested\n- TLS secure communication to the identify providers to ensure no \nMan-in-the-Middle attacks or password sniffing can occur\n- Documentation on how projects and namespaces will be managed\n- Test and verify that you can assign default platform roles\n- Test and verify you can create namespaces and projects and assign user \nroles to them\n  - e.g. Platform team creates projects and assigns admins to the project to \n  manage any additional user access or all user management will be provided \n  by the platform team.  Note: the latter (platform team manages all \n  identities), is not recommended and is an antipattern to the spirit of \n  OpenShift's self-service capability\n- LDAP/Active Directory Specific.  If LDAP or Active Directory are to be used, \ndefine processes on how (and when), group synchronization will be performed \nas the LDAP/AD may be managed by a different organization\n- Create a new Cluster Admin and remove the default (kubeadmn)\n\n\n## Day 2 Platform\n\nIn summary, there are a number of key functions that make up the Day 2 \noperations for Authentication and Authorization.  These key functions include:\n\n- [Configure Security Context Constraints for pod permissions](#SCCs)\n- [Establish Service Accounts](#Service-accounts)\n- [Test and verify Identity Provider](#Test-Verify-Identity-Provider)\n- [Test and verify RBAC](#Test-Verify-RBAC)\n\n\n## Day 1 Application\n\nWhile functionality like RBAC is part of OpenShift, such modern development \nplatforms still may, for example, require a carry over need to keep a \nDeveloper who implements a function from being able to place that function \ninto production. In such user management cases checks and balances can be \nachieved by policy gates to enforce constraints. These gates are typically \nautomated and implemented by a central DevOps team controlling the CI/CD \npipeline.  \n- Define Developer utilization policy for Application Development Environment\n- List of users and authorizations required for set of tools to be used\n\n\n## Day 2 Application\n\nFrom a Authentication and Authorization perspective, there is very little \n(if any), actions needed.  Once the RBAC is set for the DevOps Engineer \n(which includes the Security Context Constraints (SCC) the user is allowed to \napply to their application), the entire set of application specific \nAuthentication and Authorization issues move towards the configuration of the \nservice mesh, Ingress/Route and Secret access.  For that reason, the list \nbelow is minimal:  \n\n- [Document and implement Security Context Constraints Policy for pod related permissions for the application](#SCCs)\n\n\n## Mapping to Personas\n\nPersona | task\n--- | ---\nSRE, DevOpsEngineer| Security Context Constraint\nSRE | Service Accounts\nSRE | Test and verify Identity Provider\nSRE | Test and verify RBAC\n\n\n## Authentication:\n**Quote from Red Hat OPEN training: Red Hat OpenShift Container Platform 4 Configuration**\n\nRed Hat OpenShift Container Platform is typically deployed as a multi-tenant \nenvironment which includes DevOps Engineers, operations, security, and \nplatform administrators. Note:  \"Multi-tenant\" means that individual \nprojects and namespaces are completely separated via RBAC such that each \nproject/namespace appears to have its own platform. All of these users need \nto authenticate to the platform, whether to access the web console or the \nAPI—for example, with the command line interface. Authentication needs to be \nconfigured so that each user—no matter their role—authenticates with their \nown credentials so that all actions can be audited to determine who did what.  \n\nAuthentication with OAuth begins with either the web console or the API \nredirecting the user to the OAuth server component.  The OAuth server \nauthenticates the user in whatever method is configured with OAuth identity \nproviders.  \n\nThe identity provider authentication mechanisms supported by \nOpenShift 4.x are:  \n- HTPasswd\n- Keystone\n- LDAP\n- Basic authentication\n- Request header\n- GitHub or GitHub Enterprise\n- GitLab\n- Google\n- OpenID Connect  \n\nDepending on what your organization is using to manage authentication will \napply to how they are managed and their required access for tools. pods, etc.\n\nFor detailed information please see:\n[Identity Providers](https://docs.openshift.com/container-platform/4.3/authentication/understanding-identity-provider.html)\n[Configuring the OAuth Server](https://docs.openshift.com/container-platform/4.3/authentication/configuring-internal-oauth.html)\n\nOpenShift 4 clusters are initially configured with a **kubeadmin** user account. \nThe **kubeadmin** account is meant to be used only during the cluster’s \ninitial configuration and removed afterwards.  The password for **kubeadmin** \nis stored as a secret in the **kube-system** project namespace. \nAfter the **kubeadmin** secret is removed, it is no longer possible to \nauthenticate as this user, even if the secret is replaced. \nWe demonstrate how to disable the **kubeamin** account on the \n[VMware Users](UserVmware) page or the \n[Red Hat Documentation](https://docs.openshift.com/container-platform/4.3/authentication/remove-kubeadmin.html).\nMake sure you verify that the new cluser-wide admin is properly comfigured \nbefore removing **kubedmin**.\n\n\n### HTPasswd\nThe HTPasswd identity provides authentication to the cluster with passwords \nstored within the cluster. Passwords are managed as a secret in the \n**openshift-config** project namespace and stored as hashes in the **htpasswd** \nfile format. To create an **htpasswd** file suitable for the HTPasswd \nidentity provider, first create an empty **htpasswd** file, then add users \nwith hashed passwords, and finish by creating the **htpasswd secret** in the \n**openshift-config** namespace from the file.  \n\nAs an example, we will show you the steps to add user IDs with HTPasswd on \nOpenShift on VMware environment on the [VMware Users](UserVmware) page.  \n\n<InlineNotification>\nBest practices recommend that if an external Identify Provider (non HTPasswd) are utilized then at least one Cluster Manager role be created in HTPasswd (do not use the standard name kubeadm).  This cluster administrator account will allow you to login (using the local HTPasswd), and work on trouble-shooting issues with connectivity to your external Identify Provider.  In other words, this is a redundant Cluster Admin Authentication method to prevent the platform from not allowing any logins due to failures in external dependencies.\n</InlineNotification>\n\n\n### LDAP Authentication\nLDAP is a common component of Identity and Access Management infrastructure. \nAmong the information stored in LDAP may be password hashes used for \nauthentication.  The authentication steps are as follows:\n- First, query LDAP for the user identity by searching for the username \nprovided. If the search does not return exactly one entry, deny access.  \n- Next, attempt to authenticate using the found user identity and the \nuser-provided password. If authentication is successful, build an identity \nusing the configured attributes.\n\nPlease see [LDAP Identity Provider](https://docs.openshift.com/container-platform/4.3/authentication/identity_providers/configuring-ldap-identity-provider.html#configuring-ldap-identity-provider)\nfor more information.\n\nThe steps to add user IDs with LDAP on OpenShift are described on the \n[LDAP Users](UserLdap) page.  \n\n## Authorization:\n**Quote from Red Hat OPEN training: Red Hat OpenShift Container Platform 4 Configuration**\n\nRole-based access control (RBAC) objects determine whether a user may perform \na specific action within the platform or project. This lets platform \nadministrators use cluster roles and bindings to control who has various \naccess levels to the OpenShift platform itself and all projects. It also lets \nDevOps Engineers use local roles and bindings to control who has access to \ntheir projects. Authorization is a separate step from authentication, which \nis more about determining the identity of the person taking the action.  \n\nInteraction with OpenShift Container Platform is associated with a user. \nAmong the user types that can exist are regular users and system users. \n\n**Regular users** are how most interactive users are represented. In most cases, \nregular users are created automatically in OCP on first login, or you can \ncreate them via the API. Regular users are represented with the **User** object. \n\nMost **system users** are created automatically when the infrastructure is \ndefined, mainly for the purpose of enabling the infrastructure to interact \nwith the API securely. System users include a **cluster administrator**, who has \naccess to everything; a **per-node user**; **service accounts** for use by routers \nand registries; and various others. There is also an **anonymous system user** \nthat is used by default for unauthenticated requests.\n\n**Service Accounts** are special system users associated with projects; some are \ncreated automatically when the project is first created, while project \nadministrators can create more for the purpose of defining access to the \ncontents of each project. Service accounts are represented with the \n**ServiceAccount** object. \nExamples: **system:serviceaccount:default:deployer system:serviceaccount:foo:builder**  \n\nTwo levels of RBAC roles and bindings control authorization: Cluster RBAC and Local RBAC.\n- **Cluster RBAC** - Roles and bindings that are applicable across all \nprojects. Roles that exist cluster-wide are considered **cluster roles**. \n**Cluster role bindings** can only reference cluster roles.\n- **Local RBAC** - Roles and bindings that are scoped to a given project. \nRoles that exist only in a project are considered **local roles**. \n**Local role bindings** can reference both cluster and local roles.\nThis two-level hierarchy allows reusability over multiple projects through \nthe **cluster policy** while allowing customization inside individual projects \nthrough **local policies**. During evaluation, both the cluster bindings and the \nlocal bindings are used.\nFor example: First, the cluster-wide allow rules are checked. Then the \nlocally bound allow rules are checked. If no allow rules apply, authorization \nis denied by default.  \n\nYou manage authorization with rules, roles, and bindings.\n- **Rules** are sets of permitted verbs on a set of objects, for example, \nwhether something can create (which is the verb) pods (which are the objects).\n- **Roles** are collections of rules. Users and groups can be associated with, or \nbe bound to, multiple roles at the same time.\n- **Bindings** are associations between users and/or groups with a role.  \n\nRoles are collections of policy rules, which are sets of permitted verbs that \nyou can perform on a set of resources.  OpenShift includes a set of default \nroles that you can add to users and groups in either the cluster policy or \nthe local policy. You can use the CLI to visualize these roles, including a \nmatrix of the verbs and resources associated with each role, in the cluster \npolicy. The CLI output lists additional system roles as well. OpenShift uses \nthese roles for various system and component operations.  By default, in a \nlocal policy, only the binding for the admin role is immediately listed when \nyou use the CLI to view local bindings. However, if you add other default \nroles to users and groups within a local policy, these become listed in the \nCLI output as well.  \n\nThe admin user is a **project manager**. If used in a local binding, this user \nhas rights to view any resource in the project and modify any resource in the \nproject except for role creation and quota.  The **basic-user** can get basic \ninformation about projects the user participates in.  The **cluster-admin** is a \nsuperuser that can perform any action in any project. When granted to a user \nwithin a local policy, this user has full control over quota and roles and \nevery action on every resource in the project.  The **cluster-status** role lets \na user get basic cluster status information.  The **edit** role lets a user \nmodify most objects in a project, but does not give the user the power to \nview or modify roles or bindings. The **self-provisioner** is a user that can \ncreate their own projects. All users are granted this role by default.  The \n**view** role prohibits a user from making any modifications but lets the user \nsee most objects in a project. These users cannot view or modify roles or \nbindings.  \n\nYou can use the CLI to visualize rules, roles, and bindings. The example \nshows rule sets for the basic-user role, using the \n`oc describe clusterPolicy default` command.  Several factors are combined \nto make a decision when \nOpenShift evaluates an authorization request: the relationships between \ncluster roles, local roles, cluster role bindings, local role bindings, \nusers, groups, and service accounts.\n\nOpenShift evaluates authorization using the following steps:\n- OpenShift uses the identity and the project-scoped action to find all \nbindings that apply to the user or the user’s groups.\n- It then uses bindings to locate all the roles that apply.\n- Next it uses roles to find all the rules that apply.\n- Finally, it checks the action against each rule to find a match. If it does \nnot find a matching rule, the action is denied by default.  \n\n<a name=\"Service-accounts\"></a>\n\n## Service Accounts: [ SRE ]\n\n**Quote from Red Hat OPEN training: Red Hat OpenShift Container Platform 4 Configuration**\n\nWhen a person uses the command line or web console, that user’s API token \nauthenticates him or her to the OpenShift Container Platform API. However, \nwhen a regular user’s credentials are not available, it is common for \ncomponents to make API calls independently. For example:\n- Replication controllers can make API calls to create or delete pods.\n- Applications inside containers can make API calls for discovery purposes.\n- External applications can make API calls for monitoring or integration \npurposes.\n- Service accounts provide a flexible way to control API access without \nsharing a regular user’s credentials.\n- Service accounts are associated with a project.\n- When a pod requires access to make an API call to the OpenShift Container \nPlatform master, it uses the service ServiceAccount to represent the pod’s \ncredentials.\n- Some service accounts are created automatically when the project is first \ncreated. Users can create more service accounts to define access to the \nproject’s contents or to make API calls to the OpenShift Container Platform \nmaster.\n\nService accounts are represented with the **ServiceAccount** object. \nEvery service account has an associated username that can be granted roles, \njust like a regular user. The ServiceAccount username is derived from its \nproject and name. You can use the `oc sa` commands to manage the service \naccounts in your project.\n\nEvery service account is also a member of two groups:\n- **system:serviceaccounts** group, which includes all service accounts in \nthe system\n- **system:serviceaccounts:_project_ group**, which includes all service \naccounts in the specified project.\n\nFor more information please read the\n[Service Accounts](https://docs.openshift.com/container-platform/4.3/authentication/understanding-and-creating-service-accounts.html)\nsection of the OpenShift documentation.\n\n<a name=\"SCCs\"></a>\n\n## Security Context Constraints (SCCs): [ SRE, DevOps Engineer ]\n**Quote from Red Hat OPEN training: Red Hat OpenShift Container Platform 4 Configuration**  \n\nUnlike the authorization policies as detailed above, which control what a \nuser can do, **security context constraints**, or **SCCs**, control the actions that \na pod can perform and what it can access.  SCCs are objects that define a \nset of conditions that a pod must run with to be accepted into the system.  \n\nSCCs let an administrator control the following:\n- Capabilities a container can request to be added\n- The use of host directories as volumes\n- The SELinux context of the container,\n- The user ID\n- The use of host namespaces and networking\n- Allocating an FSGroup that owns the pod’s volumes\n- Configuring allowable supplemental groups\n- Requiring the use of a read-only root file system\n- Usage of volume types, and\n- Configuring allowable secure comping mode (seccomp) profiles .\n\nStandard SCCs are added to a cluster by default. They are viewable by cluster \nadministrators using the CLI.  \n\nTo find out which users can perform a particular action on a particular \nobject (aka resource) use the command `oc policy who-can`.\n\nFor Detailed information see: \n[Managing Security Context Constraints](https://docs.openshift.com/container-platform/4.3/authentication/managing-security-context-constraints.html).\n\n## LDAP and Active Directory Group Synchronization:\n\nLDAP and Active Directory (AD), are the most common Authentication and \nAuthorization solutions used with the OpenShift platform.  When LDAP/AD are \nutilized, it enables the ability to manage users, groups, memberships and \nroles in one place (i.e. LDAP), and synchronizes these groups with the \nOpenShift Platform so that the internal OpenShift Platform Records are kept \nin synch with the LDAP/AD source.\n\nFor a full description and the complete step-by-step instructions, please \nuse the following link for a fully detailed list of configuration and options \nthat can be utilized \n[OpenShift Group Synchronization](https://docs.openshift.com/container-platform/latest/authentication/ldap-syncing.html).\n\nHowever, from a Day 2 perspective, here are the highlights of the actions \nthat must be taken:\n\n- Configure the LDAP Client Configuration: Which should have been configured \nduring Day 1 and tested as part of Day 2\n- LDAP Query Definition:  This defines the search base, scope, rules and \nfilters to be used\n- Define the name mapping:  This mapping correlates between the platform \ngroup name and how it is represented in LDAP/AD\n- OpenShift provides a LDAP template mapping using RFC 2307 Schema which \nkickstarts the mapping process.  See **rfc2307_config.yaml**\n- Openshift provides an AD template mapping file for use with AD.  See \n**active_directory_config.yaml**\n- In addition to the AD template file, there is a more comprehensive \n(beyond just groups), called the **Augmented Active Directory** configuration \nfile.  See **augmented_active_directory_config.yaml**\n- Perform an initial synchronization.  You can synch from OpenShift to \nLDAP/AD or from LDAP/AD to OpenShift.  This initial synch (with the mapping \nfiles), ensures that the synch process is working and sets up the two \nsystems to exchange information and remain in synch from this point on.  \nNote: It is highly recommended that you have a backup of both systems \n(OpenShift Platform and the LDAP/AD) in case the mappings cause corruption \nwhich will require manual review and correction to fix.  A restoration is \nmuch easier\n- Regulary run pruning jobs to ensure that unused groups are removed. \nThis is not only good housekeeping, but can also remove a security exposure \nby someone adding themselves to an idle group that still has roles assigned \nto it.\n\n<a name=\"Namespaces-Projects\"></a>\n\n## Creating new Namespaces and Projects: [ DevOps Engineer ]\nNamespaces are managed via the **Namespace** option under the \n**Administration Menu** Option in the the Web Console. You can also \ncreate namespaces via the \n`kubectl create` command.\n\n<a name=\"Test-Verify-Identity-Provider\"></a>\n\n## Test and verify Identity Provider: [ SRE ]\nWhen installed, OCP provides the **kubeadmin** default user. To specify an \nidentity provider you first must create a custom resource which defines the \nidentity provider and assigns it to the cluster.  OCP provides nine identity \nproviders please read the \n[Red Hat Documentation](https://docs.openshift.com/container-platform/4.3/authentication/understanding-identity-provider.html#identity-provider-overview_understanding-identity-provider).\n\nOne of  most common providers used is \n[LDAP](https://docs.openshift.com/container-platform/4.3/authentication/identity_providers/configuring-ldap-identity-provider.html#configuring-ldap-identity-provider) \nand \"local\" authentication using \n[htpasswd](https://docs.openshift.com/container-platform/4.3/authentication/identity_providers/configuring-htpasswd-identity-provider.html), \nwhich is similar to /etc/passwd in Unix and Linux OSs. htpasswd is managed \nvia a command line utility.\n\nUse the `oc login` CLI to verify you can login once the Identity Provider is \nconfigured.\n\n<a name=\"Test-Verify-RBAC\"></a>\n\n## Test and verify RBAC: [ SRE ]\nWe demonstrate how to configure groups and cluster role-based access control \n(RBAC) on your OpenShift cluster [here](UserRbac).  \n\n## Implementing Authentication, Authorization & User Management\n\n## Kubernetes\n\nWe will explain the feature for this topic which is provided by Kubernetes.  \nSince there are so many product/solutions for Kubernetes which are provided \nby 3rd party vendors or communities, I wouldn't put a long list of those \nsolutions in here. Because, that's not really our focus.  \nHowever, in case OpenShift solution and banira Kubernetes solution are \ndifference, it would be good to mention.  Therefore, we would put such \ninformation here.  \n\n## IAM settings for OpenShift on IBM Cloud (Managed Service)\nRed Hat OpenShift on IBM Cloud (a.k.a. ROKS) is configured to use IBM Cloud \nIdentity and Access Management (IAM) roles. IBM Cloud IAM platform roles \ndetermine the actions that users can perform on IBM Cloud resources such as \nclusters, worker nodes, and Ingress application load balancers (ALBs). \nIBM Cloud IAM platform roles also automatically set basic infrastructure \npermissions for users.  \n\nYou can find detailed information of IBM Cloud IAM platform roles for \nOpenShift Container Platform \nin the IBM Cloud official documentation at\n[IBM Cloud IAM platform roles](https://cloud.ibm.com/docs/openshift?topic=openshift-access_reference#iam_platform).  \n\nSince IBM Cloud IAM is completely integrated and there is not need to perform specific operation with OpenShift user management. IBM Cloud IAM will propagate all necessary configurations.  For example, if you delete an account from IBM Cloud, that user in OpenShift will be removed.  If you change the permissions with IBM Cloud IAM, it will reflect the permission in your OpenShift cluster.\nIn other words, instead of maintaining users on OpenShift, you will maintain users in IBM Cloud IAM.  \n\n## With IBM Cloudpak for MCM\n\nIBM Cloud Pak for MCM includes Identity and Access Management which integrates \nwith OCP. Please see \n[IAM with OpenShift](https://www.ibm.com/support/knowledgecenter/SSFC4F_1.2.0/iam/3.4.0/iam_openshift.html). \nWhen IAM is implemented one manages the LDAP connections users and roles via \nIAM. Note: Command line authentication can not use IAM, use of the \nCopy Login for command line is still necessary.\n\n## IAM settings for OpenShift on VMware\nYou are required two separate Identity and Access Management when you work \nwith Red Hat OpenShift on VMware.   \n\nIt is required to access VMware environment if you have a system administrator \nrole for the OpenShift cluster. For example, if you are the person to build the \nOpenShift Cluster on VMware, then you definitely need to have an access with \nappropriate permissions. You can find detailed information for required \npermissions for VMware environment in the product documentation.  \n\nYou also need to understand the Identity management on Red Hat OpenShift. \nYou probably need to create a few user IDs on your OCP depends on \nrequirements. As an example, we will show you the steps to add user IDs \non OpenShift on VMware environment in the [VMware Users](UserVmware) section.  \n\nAdditionally, if there is a system as a jump server to manage the \nOpenShift cluster, you also need to maintain the IDs on that server in case \nyou are the system administrator of it. Imagine that you would use the \njump server when you build your OpenShift cluster to download the \ninstallation programs, for example.  \n\n## Other considerations\n\nn/a\n\n\n","type":"Mdx","contentDigest":"59dee90972a03101a4b3335fcc07294f","counter":564,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"User Management / Authentication / Authorization","description":"OpenShift Day2 User","keywords":"OpenShift, day2, user"},"exports":{},"rawBody":"---\ntitle: User Management / Authentication / Authorization\ndescription: OpenShift Day2 User\nkeywords: 'OpenShift, day2, user'\n---\n\n\n## Authentication, Authorization & User Management\n\nOpenShift provides a rich set of Authentication and Authorization features \nfor securing access and Role Based Access Control (RBAC), to the platform and \napplication space (sometimes abbreviated as Security, Identity and Access \nManagement - SIAM). To ensure we clarify the difference, the following are \nthe key differences and industry norms for the two terms:\n- Authentication:  The ability for a user to identify themselves (via user/password, token or keys) as having an account to access the platform\n- Authorization:  The distinct roles and authorizations (RBAC), attached to this user that defines what actions they are authorized to perform on the platform (from anonymous, view only, project based to cluster admin, certain namespaces as some examples)\n\nFor a complete description on configuration and possible permutations, \nplease see the following links:\n\n[Understanding OpenShift Authentication](https://docs.openshift.com/container-platform/latest/authentication/understanding-authentication.html)\n\n[OpenShift Authentication](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.3/html/authentication/index)\n\n## Day 1 Platform\n\nDuring Day 0, it is normal to design and define which identity providers \n(i.e. Git, LDAP, Active Directory, Etc.), will be used along with their \nconfiguration during the installation (Day 1). While it is possible \n(and supported), to change identify provides post installation, changing \nidentity providers will require the use of User Mappings to map the current \nidentities to their new identity provider. For example, moving from Git where the \nidentity is a name to LDAP where the identity is an email address. For our \npurposes, these are the items that Day 2 should expect to be in-place and \ntested as part of Day 1:  \n\n- Identity provider configured and tested\n- TLS secure communication to the identify providers to ensure no \nMan-in-the-Middle attacks or password sniffing can occur\n- Documentation on how projects and namespaces will be managed\n- Test and verify that you can assign default platform roles\n- Test and verify you can create namespaces and projects and assign user \nroles to them\n  - e.g. Platform team creates projects and assigns admins to the project to \n  manage any additional user access or all user management will be provided \n  by the platform team.  Note: the latter (platform team manages all \n  identities), is not recommended and is an antipattern to the spirit of \n  OpenShift's self-service capability\n- LDAP/Active Directory Specific.  If LDAP or Active Directory are to be used, \ndefine processes on how (and when), group synchronization will be performed \nas the LDAP/AD may be managed by a different organization\n- Create a new Cluster Admin and remove the default (kubeadmn)\n\n\n## Day 2 Platform\n\nIn summary, there are a number of key functions that make up the Day 2 \noperations for Authentication and Authorization.  These key functions include:\n\n- [Configure Security Context Constraints for pod permissions](#SCCs)\n- [Establish Service Accounts](#Service-accounts)\n- [Test and verify Identity Provider](#Test-Verify-Identity-Provider)\n- [Test and verify RBAC](#Test-Verify-RBAC)\n\n\n## Day 1 Application\n\nWhile functionality like RBAC is part of OpenShift, such modern development \nplatforms still may, for example, require a carry over need to keep a \nDeveloper who implements a function from being able to place that function \ninto production. In such user management cases checks and balances can be \nachieved by policy gates to enforce constraints. These gates are typically \nautomated and implemented by a central DevOps team controlling the CI/CD \npipeline.  \n- Define Developer utilization policy for Application Development Environment\n- List of users and authorizations required for set of tools to be used\n\n\n## Day 2 Application\n\nFrom a Authentication and Authorization perspective, there is very little \n(if any), actions needed.  Once the RBAC is set for the DevOps Engineer \n(which includes the Security Context Constraints (SCC) the user is allowed to \napply to their application), the entire set of application specific \nAuthentication and Authorization issues move towards the configuration of the \nservice mesh, Ingress/Route and Secret access.  For that reason, the list \nbelow is minimal:  \n\n- [Document and implement Security Context Constraints Policy for pod related permissions for the application](#SCCs)\n\n\n## Mapping to Personas\n\nPersona | task\n--- | ---\nSRE, DevOpsEngineer| Security Context Constraint\nSRE | Service Accounts\nSRE | Test and verify Identity Provider\nSRE | Test and verify RBAC\n\n\n## Authentication:\n**Quote from Red Hat OPEN training: Red Hat OpenShift Container Platform 4 Configuration**\n\nRed Hat OpenShift Container Platform is typically deployed as a multi-tenant \nenvironment which includes DevOps Engineers, operations, security, and \nplatform administrators. Note:  \"Multi-tenant\" means that individual \nprojects and namespaces are completely separated via RBAC such that each \nproject/namespace appears to have its own platform. All of these users need \nto authenticate to the platform, whether to access the web console or the \nAPI—for example, with the command line interface. Authentication needs to be \nconfigured so that each user—no matter their role—authenticates with their \nown credentials so that all actions can be audited to determine who did what.  \n\nAuthentication with OAuth begins with either the web console or the API \nredirecting the user to the OAuth server component.  The OAuth server \nauthenticates the user in whatever method is configured with OAuth identity \nproviders.  \n\nThe identity provider authentication mechanisms supported by \nOpenShift 4.x are:  \n- HTPasswd\n- Keystone\n- LDAP\n- Basic authentication\n- Request header\n- GitHub or GitHub Enterprise\n- GitLab\n- Google\n- OpenID Connect  \n\nDepending on what your organization is using to manage authentication will \napply to how they are managed and their required access for tools. pods, etc.\n\nFor detailed information please see:\n[Identity Providers](https://docs.openshift.com/container-platform/4.3/authentication/understanding-identity-provider.html)\n[Configuring the OAuth Server](https://docs.openshift.com/container-platform/4.3/authentication/configuring-internal-oauth.html)\n\nOpenShift 4 clusters are initially configured with a **kubeadmin** user account. \nThe **kubeadmin** account is meant to be used only during the cluster’s \ninitial configuration and removed afterwards.  The password for **kubeadmin** \nis stored as a secret in the **kube-system** project namespace. \nAfter the **kubeadmin** secret is removed, it is no longer possible to \nauthenticate as this user, even if the secret is replaced. \nWe demonstrate how to disable the **kubeamin** account on the \n[VMware Users](UserVmware) page or the \n[Red Hat Documentation](https://docs.openshift.com/container-platform/4.3/authentication/remove-kubeadmin.html).\nMake sure you verify that the new cluser-wide admin is properly comfigured \nbefore removing **kubedmin**.\n\n\n### HTPasswd\nThe HTPasswd identity provides authentication to the cluster with passwords \nstored within the cluster. Passwords are managed as a secret in the \n**openshift-config** project namespace and stored as hashes in the **htpasswd** \nfile format. To create an **htpasswd** file suitable for the HTPasswd \nidentity provider, first create an empty **htpasswd** file, then add users \nwith hashed passwords, and finish by creating the **htpasswd secret** in the \n**openshift-config** namespace from the file.  \n\nAs an example, we will show you the steps to add user IDs with HTPasswd on \nOpenShift on VMware environment on the [VMware Users](UserVmware) page.  \n\n<InlineNotification>\nBest practices recommend that if an external Identify Provider (non HTPasswd) are utilized then at least one Cluster Manager role be created in HTPasswd (do not use the standard name kubeadm).  This cluster administrator account will allow you to login (using the local HTPasswd), and work on trouble-shooting issues with connectivity to your external Identify Provider.  In other words, this is a redundant Cluster Admin Authentication method to prevent the platform from not allowing any logins due to failures in external dependencies.\n</InlineNotification>\n\n\n### LDAP Authentication\nLDAP is a common component of Identity and Access Management infrastructure. \nAmong the information stored in LDAP may be password hashes used for \nauthentication.  The authentication steps are as follows:\n- First, query LDAP for the user identity by searching for the username \nprovided. If the search does not return exactly one entry, deny access.  \n- Next, attempt to authenticate using the found user identity and the \nuser-provided password. If authentication is successful, build an identity \nusing the configured attributes.\n\nPlease see [LDAP Identity Provider](https://docs.openshift.com/container-platform/4.3/authentication/identity_providers/configuring-ldap-identity-provider.html#configuring-ldap-identity-provider)\nfor more information.\n\nThe steps to add user IDs with LDAP on OpenShift are described on the \n[LDAP Users](UserLdap) page.  \n\n## Authorization:\n**Quote from Red Hat OPEN training: Red Hat OpenShift Container Platform 4 Configuration**\n\nRole-based access control (RBAC) objects determine whether a user may perform \na specific action within the platform or project. This lets platform \nadministrators use cluster roles and bindings to control who has various \naccess levels to the OpenShift platform itself and all projects. It also lets \nDevOps Engineers use local roles and bindings to control who has access to \ntheir projects. Authorization is a separate step from authentication, which \nis more about determining the identity of the person taking the action.  \n\nInteraction with OpenShift Container Platform is associated with a user. \nAmong the user types that can exist are regular users and system users. \n\n**Regular users** are how most interactive users are represented. In most cases, \nregular users are created automatically in OCP on first login, or you can \ncreate them via the API. Regular users are represented with the **User** object. \n\nMost **system users** are created automatically when the infrastructure is \ndefined, mainly for the purpose of enabling the infrastructure to interact \nwith the API securely. System users include a **cluster administrator**, who has \naccess to everything; a **per-node user**; **service accounts** for use by routers \nand registries; and various others. There is also an **anonymous system user** \nthat is used by default for unauthenticated requests.\n\n**Service Accounts** are special system users associated with projects; some are \ncreated automatically when the project is first created, while project \nadministrators can create more for the purpose of defining access to the \ncontents of each project. Service accounts are represented with the \n**ServiceAccount** object. \nExamples: **system:serviceaccount:default:deployer system:serviceaccount:foo:builder**  \n\nTwo levels of RBAC roles and bindings control authorization: Cluster RBAC and Local RBAC.\n- **Cluster RBAC** - Roles and bindings that are applicable across all \nprojects. Roles that exist cluster-wide are considered **cluster roles**. \n**Cluster role bindings** can only reference cluster roles.\n- **Local RBAC** - Roles and bindings that are scoped to a given project. \nRoles that exist only in a project are considered **local roles**. \n**Local role bindings** can reference both cluster and local roles.\nThis two-level hierarchy allows reusability over multiple projects through \nthe **cluster policy** while allowing customization inside individual projects \nthrough **local policies**. During evaluation, both the cluster bindings and the \nlocal bindings are used.\nFor example: First, the cluster-wide allow rules are checked. Then the \nlocally bound allow rules are checked. If no allow rules apply, authorization \nis denied by default.  \n\nYou manage authorization with rules, roles, and bindings.\n- **Rules** are sets of permitted verbs on a set of objects, for example, \nwhether something can create (which is the verb) pods (which are the objects).\n- **Roles** are collections of rules. Users and groups can be associated with, or \nbe bound to, multiple roles at the same time.\n- **Bindings** are associations between users and/or groups with a role.  \n\nRoles are collections of policy rules, which are sets of permitted verbs that \nyou can perform on a set of resources.  OpenShift includes a set of default \nroles that you can add to users and groups in either the cluster policy or \nthe local policy. You can use the CLI to visualize these roles, including a \nmatrix of the verbs and resources associated with each role, in the cluster \npolicy. The CLI output lists additional system roles as well. OpenShift uses \nthese roles for various system and component operations.  By default, in a \nlocal policy, only the binding for the admin role is immediately listed when \nyou use the CLI to view local bindings. However, if you add other default \nroles to users and groups within a local policy, these become listed in the \nCLI output as well.  \n\nThe admin user is a **project manager**. If used in a local binding, this user \nhas rights to view any resource in the project and modify any resource in the \nproject except for role creation and quota.  The **basic-user** can get basic \ninformation about projects the user participates in.  The **cluster-admin** is a \nsuperuser that can perform any action in any project. When granted to a user \nwithin a local policy, this user has full control over quota and roles and \nevery action on every resource in the project.  The **cluster-status** role lets \na user get basic cluster status information.  The **edit** role lets a user \nmodify most objects in a project, but does not give the user the power to \nview or modify roles or bindings. The **self-provisioner** is a user that can \ncreate their own projects. All users are granted this role by default.  The \n**view** role prohibits a user from making any modifications but lets the user \nsee most objects in a project. These users cannot view or modify roles or \nbindings.  \n\nYou can use the CLI to visualize rules, roles, and bindings. The example \nshows rule sets for the basic-user role, using the \n`oc describe clusterPolicy default` command.  Several factors are combined \nto make a decision when \nOpenShift evaluates an authorization request: the relationships between \ncluster roles, local roles, cluster role bindings, local role bindings, \nusers, groups, and service accounts.\n\nOpenShift evaluates authorization using the following steps:\n- OpenShift uses the identity and the project-scoped action to find all \nbindings that apply to the user or the user’s groups.\n- It then uses bindings to locate all the roles that apply.\n- Next it uses roles to find all the rules that apply.\n- Finally, it checks the action against each rule to find a match. If it does \nnot find a matching rule, the action is denied by default.  \n\n<a name=\"Service-accounts\"></a>\n\n## Service Accounts: [ SRE ]\n\n**Quote from Red Hat OPEN training: Red Hat OpenShift Container Platform 4 Configuration**\n\nWhen a person uses the command line or web console, that user’s API token \nauthenticates him or her to the OpenShift Container Platform API. However, \nwhen a regular user’s credentials are not available, it is common for \ncomponents to make API calls independently. For example:\n- Replication controllers can make API calls to create or delete pods.\n- Applications inside containers can make API calls for discovery purposes.\n- External applications can make API calls for monitoring or integration \npurposes.\n- Service accounts provide a flexible way to control API access without \nsharing a regular user’s credentials.\n- Service accounts are associated with a project.\n- When a pod requires access to make an API call to the OpenShift Container \nPlatform master, it uses the service ServiceAccount to represent the pod’s \ncredentials.\n- Some service accounts are created automatically when the project is first \ncreated. Users can create more service accounts to define access to the \nproject’s contents or to make API calls to the OpenShift Container Platform \nmaster.\n\nService accounts are represented with the **ServiceAccount** object. \nEvery service account has an associated username that can be granted roles, \njust like a regular user. The ServiceAccount username is derived from its \nproject and name. You can use the `oc sa` commands to manage the service \naccounts in your project.\n\nEvery service account is also a member of two groups:\n- **system:serviceaccounts** group, which includes all service accounts in \nthe system\n- **system:serviceaccounts:_project_ group**, which includes all service \naccounts in the specified project.\n\nFor more information please read the\n[Service Accounts](https://docs.openshift.com/container-platform/4.3/authentication/understanding-and-creating-service-accounts.html)\nsection of the OpenShift documentation.\n\n<a name=\"SCCs\"></a>\n\n## Security Context Constraints (SCCs): [ SRE, DevOps Engineer ]\n**Quote from Red Hat OPEN training: Red Hat OpenShift Container Platform 4 Configuration**  \n\nUnlike the authorization policies as detailed above, which control what a \nuser can do, **security context constraints**, or **SCCs**, control the actions that \na pod can perform and what it can access.  SCCs are objects that define a \nset of conditions that a pod must run with to be accepted into the system.  \n\nSCCs let an administrator control the following:\n- Capabilities a container can request to be added\n- The use of host directories as volumes\n- The SELinux context of the container,\n- The user ID\n- The use of host namespaces and networking\n- Allocating an FSGroup that owns the pod’s volumes\n- Configuring allowable supplemental groups\n- Requiring the use of a read-only root file system\n- Usage of volume types, and\n- Configuring allowable secure comping mode (seccomp) profiles .\n\nStandard SCCs are added to a cluster by default. They are viewable by cluster \nadministrators using the CLI.  \n\nTo find out which users can perform a particular action on a particular \nobject (aka resource) use the command `oc policy who-can`.\n\nFor Detailed information see: \n[Managing Security Context Constraints](https://docs.openshift.com/container-platform/4.3/authentication/managing-security-context-constraints.html).\n\n## LDAP and Active Directory Group Synchronization:\n\nLDAP and Active Directory (AD), are the most common Authentication and \nAuthorization solutions used with the OpenShift platform.  When LDAP/AD are \nutilized, it enables the ability to manage users, groups, memberships and \nroles in one place (i.e. LDAP), and synchronizes these groups with the \nOpenShift Platform so that the internal OpenShift Platform Records are kept \nin synch with the LDAP/AD source.\n\nFor a full description and the complete step-by-step instructions, please \nuse the following link for a fully detailed list of configuration and options \nthat can be utilized \n[OpenShift Group Synchronization](https://docs.openshift.com/container-platform/latest/authentication/ldap-syncing.html).\n\nHowever, from a Day 2 perspective, here are the highlights of the actions \nthat must be taken:\n\n- Configure the LDAP Client Configuration: Which should have been configured \nduring Day 1 and tested as part of Day 2\n- LDAP Query Definition:  This defines the search base, scope, rules and \nfilters to be used\n- Define the name mapping:  This mapping correlates between the platform \ngroup name and how it is represented in LDAP/AD\n- OpenShift provides a LDAP template mapping using RFC 2307 Schema which \nkickstarts the mapping process.  See **rfc2307_config.yaml**\n- Openshift provides an AD template mapping file for use with AD.  See \n**active_directory_config.yaml**\n- In addition to the AD template file, there is a more comprehensive \n(beyond just groups), called the **Augmented Active Directory** configuration \nfile.  See **augmented_active_directory_config.yaml**\n- Perform an initial synchronization.  You can synch from OpenShift to \nLDAP/AD or from LDAP/AD to OpenShift.  This initial synch (with the mapping \nfiles), ensures that the synch process is working and sets up the two \nsystems to exchange information and remain in synch from this point on.  \nNote: It is highly recommended that you have a backup of both systems \n(OpenShift Platform and the LDAP/AD) in case the mappings cause corruption \nwhich will require manual review and correction to fix.  A restoration is \nmuch easier\n- Regulary run pruning jobs to ensure that unused groups are removed. \nThis is not only good housekeeping, but can also remove a security exposure \nby someone adding themselves to an idle group that still has roles assigned \nto it.\n\n<a name=\"Namespaces-Projects\"></a>\n\n## Creating new Namespaces and Projects: [ DevOps Engineer ]\nNamespaces are managed via the **Namespace** option under the \n**Administration Menu** Option in the the Web Console. You can also \ncreate namespaces via the \n`kubectl create` command.\n\n<a name=\"Test-Verify-Identity-Provider\"></a>\n\n## Test and verify Identity Provider: [ SRE ]\nWhen installed, OCP provides the **kubeadmin** default user. To specify an \nidentity provider you first must create a custom resource which defines the \nidentity provider and assigns it to the cluster.  OCP provides nine identity \nproviders please read the \n[Red Hat Documentation](https://docs.openshift.com/container-platform/4.3/authentication/understanding-identity-provider.html#identity-provider-overview_understanding-identity-provider).\n\nOne of  most common providers used is \n[LDAP](https://docs.openshift.com/container-platform/4.3/authentication/identity_providers/configuring-ldap-identity-provider.html#configuring-ldap-identity-provider) \nand \"local\" authentication using \n[htpasswd](https://docs.openshift.com/container-platform/4.3/authentication/identity_providers/configuring-htpasswd-identity-provider.html), \nwhich is similar to /etc/passwd in Unix and Linux OSs. htpasswd is managed \nvia a command line utility.\n\nUse the `oc login` CLI to verify you can login once the Identity Provider is \nconfigured.\n\n<a name=\"Test-Verify-RBAC\"></a>\n\n## Test and verify RBAC: [ SRE ]\nWe demonstrate how to configure groups and cluster role-based access control \n(RBAC) on your OpenShift cluster [here](UserRbac).  \n\n## Implementing Authentication, Authorization & User Management\n\n## Kubernetes\n\nWe will explain the feature for this topic which is provided by Kubernetes.  \nSince there are so many product/solutions for Kubernetes which are provided \nby 3rd party vendors or communities, I wouldn't put a long list of those \nsolutions in here. Because, that's not really our focus.  \nHowever, in case OpenShift solution and banira Kubernetes solution are \ndifference, it would be good to mention.  Therefore, we would put such \ninformation here.  \n\n## IAM settings for OpenShift on IBM Cloud (Managed Service)\nRed Hat OpenShift on IBM Cloud (a.k.a. ROKS) is configured to use IBM Cloud \nIdentity and Access Management (IAM) roles. IBM Cloud IAM platform roles \ndetermine the actions that users can perform on IBM Cloud resources such as \nclusters, worker nodes, and Ingress application load balancers (ALBs). \nIBM Cloud IAM platform roles also automatically set basic infrastructure \npermissions for users.  \n\nYou can find detailed information of IBM Cloud IAM platform roles for \nOpenShift Container Platform \nin the IBM Cloud official documentation at\n[IBM Cloud IAM platform roles](https://cloud.ibm.com/docs/openshift?topic=openshift-access_reference#iam_platform).  \n\nSince IBM Cloud IAM is completely integrated and there is not need to perform specific operation with OpenShift user management. IBM Cloud IAM will propagate all necessary configurations.  For example, if you delete an account from IBM Cloud, that user in OpenShift will be removed.  If you change the permissions with IBM Cloud IAM, it will reflect the permission in your OpenShift cluster.\nIn other words, instead of maintaining users on OpenShift, you will maintain users in IBM Cloud IAM.  \n\n## With IBM Cloudpak for MCM\n\nIBM Cloud Pak for MCM includes Identity and Access Management which integrates \nwith OCP. Please see \n[IAM with OpenShift](https://www.ibm.com/support/knowledgecenter/SSFC4F_1.2.0/iam/3.4.0/iam_openshift.html). \nWhen IAM is implemented one manages the LDAP connections users and roles via \nIAM. Note: Command line authentication can not use IAM, use of the \nCopy Login for command line is still necessary.\n\n## IAM settings for OpenShift on VMware\nYou are required two separate Identity and Access Management when you work \nwith Red Hat OpenShift on VMware.   \n\nIt is required to access VMware environment if you have a system administrator \nrole for the OpenShift cluster. For example, if you are the person to build the \nOpenShift Cluster on VMware, then you definitely need to have an access with \nappropriate permissions. You can find detailed information for required \npermissions for VMware environment in the product documentation.  \n\nYou also need to understand the Identity management on Red Hat OpenShift. \nYou probably need to create a few user IDs on your OCP depends on \nrequirements. As an example, we will show you the steps to add user IDs \non OpenShift on VMware environment in the [VMware Users](UserVmware) section.  \n\nAdditionally, if there is a system as a jump server to manage the \nOpenShift cluster, you also need to maintain the IDs on that server in case \nyou are the system administrator of it. Imagine that you would use the \njump server when you build your OpenShift cluster to download the \ninstallation programs, for example.  \n\n## Other considerations\n\nn/a\n\n\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/day2/User/index.mdx"}}}}