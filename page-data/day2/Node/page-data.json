{"componentChunkName":"component---src-pages-day-2-node-index-mdx","path":"/day2/Node/","result":{"pageContext":{"frontmatter":{"title":"OpenShift Platform Day2 - Node","description":"OpenShift Day2 Node","keywords":"OpenShift, day2, node"},"relativePagePath":"/day2/Node/index.mdx","titleType":"page","MdxNode":{"id":"3ce7e566-456d-55e4-bfa7-f341f88bf860","children":[],"parent":"ed750fc9-a720-5841-b268-451527e96b9b","internal":{"content":"---\ntitle: OpenShift Platform Day2 - Node\ndescription: OpenShift Day2 Node\nkeywords: 'OpenShift, day2, node'\n---\n\n## Node Management Overview\nAll pods in Kubernetes or OpenShift Container Platform run in the nodes.  Because of this, ensuring the health of your nodes is a critical Day 2 activity.  You should be able to replace a node without affecting the service.  As part of Day 2 activities, you need to be able to ensure this.\n\nWhen you look at node management, it is also important to refer back to the design of the cluster. Design and sizing your node is a Day 0 activity. By referring to this information, you will know the design principle and the design decision that was made for the cluster.\n\nThis chapter will focus on steps that you can take to ensure the health of your nodes using the standard facility that come out of the box with OpenShift.  The steps presented here can be evolved to make use of other practices presented in the other chapters of this repository, such as performing [logging](../Logging), [monitoring](../Monitoring), and [event management](../EventManagement).\n\n## Day 1 Platform\nWhen you consider Day 2 node management, you need to have access to the Day 0 design principle and/or design decision.  These are examples of useful information to collect:\n- __Sizing Considerations__. How many pods can my cluster handle?  Are there any assumptions made on the application potential loading?  What is the design decision on the number of master and worker nodes?  Is there any scale-out consideration made?\n- __Environment Scenarios__. What is the design for the number of environment types (Dev, Test, Prod) for the clusters?  Is the cluster running as a private cloud, or hosted on a cloud provider, or a hybrid?\n- __Installation Method__.  Terraform, UPI, IPI?\n- __Feature__.  What features am I going to offer my tenants?\n- __CI/CD__. Do I have a strategy for CI/CD?\n- __Security__.  What is supported and not supported by the security department?  Has the cluster design been approved by the security department?\n- __Management__.  What is the design of the management functionality of the cluster?\n\n## Day 2 Platform\n\nThe following shows Day 2 activities on managing the node:\n- [Manage Master node health](#master-node)\n  - [Do you increase the number of master nodes?](#mnode-number)\n  - [Master nodes platform requirement](#mnode-requirement)\n  - [Resiliency of master node](#mnode-resiliency)\n  - [Make sure only the intended pods is running in the master nodes](#mnode-intend)\n  - [Troubleshooting etcd](#etcd-troubleshoot)\n- [Maintain worker node health](#worker-node)\n  - [How to stop and start a worker node manually](#stop-start)\n- [Ensure node resiliency](#node-up)\n- [Assure cluster operator health](#co-health)\n  - [Machine Config Operator](#mco)\n\n\n## Day 1 Application\n\nFor Node Management, you need to ensure that the applications that are already installed on Day 1 are running in the nodes as designed.   You need to __verify that the application has been deployed in the correct node__.  For example, you need to define the initial pod placement policy for the applications identified in Day 1. This activity will be extended in Day 2 for applications that will be introduced as part of the steady-state operation.\n\n## Day 2 Application\n\nDuring Day 2 operations, new applications will be deployed to the OpenShift cluster. You need to ensure that your worker nodes will have enough capacity to support these applications.  One way is to define the project template includes the required Range Limit resources and Quota resources. _The Project template_ is covered in [Builds and Deploy](../BuildDeploy) chapter, Range Limit, and Quota are discussed in the [Capacity](../Capacity) chapter.   \n\nAnother important aspect of Day 2 Node Management on Application is to satisfy the applications' workload requirement for performance, security, and manageability through these activities:\n- [Configure node for a specialized function](#mgmt-node)\n  - [Preventing security breach by using Node Restriction](#mgmt-security)\n  - [Node placement and DaemonSets](#daemonsets)\n\n\n## Mapping to Personas\n\nPersona | task\n--- | ---\nSRE | Manage master node health\nSRE | Maintain worker node health\nSRE | Ensure node resiliency\nSRE | Assure cluster operator health\nSRE, DevOps Engineer | Configure node for a specialized function\n\n\n\n## Platform tasks\n\n<a name=\"master-node\"></a>\n\n## Manage master node health: [ SRE ]\n\nAs part of the Day 2 operation, you need to ensure the health of your master node.  In particular, there are three pods that you need to ensure to be in good running operation: __the etcd service provider__, __the API Server__, and __the Controller and Scheduler__.  \n\nAmongst these three types of pods, etcd requires the persistent volume (storage) as it can generate substantial network traffic, so one way to ensure the master node health is to ensure the health of the etcd services.  The Operators manage the API server, the controller, and the scheduler.  \n\n<a name=\"mnode-number\"></a>\n\n### Do you increase the number of master nodes?\nThe etcd's main function is to store Kubernetesâ€™ configuration information; hence its performance is crucial to the efficient performance of your cluster.\n\netcd is a distributed key-value store using [the RAFT consensus algorithm](https://raft.github.io). Because of the consensus algorithm, etcd must be deployed in odd numbers of pods to maintain quorum.  This normally translates to three nodes in a Production environment.\n\nFrom the operational aspects of etcd, the etcd service is considered an active-active cluster. Meaning, an etcd client can write to any of the etcd nodes and the cluster will replicate the data, and maintain consistency of the data across the instances.  When one etcd pod in one master node performs a write, it needs to synchronize the content with other etcd pods.\n\nYour Day 0 design document may already specify three etcd pods.  As your cluster size grow, you might think that you need to increase your etcd nodes (Master nodes) beyond 3. However, as general guidance:\n\n> For performance reason, you should not have more than 3 etcd nodes (master nodes).\n\nThe main reason is that as etcd implements a consensus algorithm, each node needs to synchronize with other nodes.  When you have three nodes, each node needs to communicate with two other nodes.  When you have five nodes, then each node needs to talk to four other nodes, causing exponential growth in network traffic that might affect the network capacity of your cluster.  \n\nWhen your cluster grows, if you need to scale etcd, which is unlikely, then rather than adding more etcd pods, it might be better to increase the capacity of each of the etcd pods.  You can also increase the compute (CPU/memory) power of the master node where the etcd runs.\n\n<a name=\"mnode-requirement\"></a>\n\n### Master nodes platform requirement\nThe design of the master nodes platform mostly had been done in Day 0, but the design decision might not have been spelled out in detail in the design document.\n\nAs part of Day 2 activities, you might need to replace a bad master node, or upgrade the CPU/memory capacity of the master node.  The following is a general recommendation on the selection of the master node to host the etcd pod:\n\n> - It needs storage with fast access disk.\n> - It needs a low latency in communicating with other etcd pods, this means fast networking.\n> - The etcd store should not be located on the same disk as a disk-intensive service such as the database to store logging and metric information.\n> - It should not be spread across data centers or, in the case of public clouds, availability zones\n\n<a name=\"mnode-resiliency\"></a>\n\n### Resiliency of master node\nWe have recommended three master nodes as a good number for production.  For etcd, the quorum for three master nodes is two nodes.  Let us consider the failure scenario:\n\nWhen one master node (i.e., etcd pod) is lost, The quorum is still ok.  So the operation can still recover by replacing the lost master node.\n\nWhen two master nodes are lost, etcd does not have the quorum to write to disk.  In other words, you can not make changes, no new project or resource, no scaling up or down.  However, the currently running pods will not be terminated.\n\nWhen three (all) master nodes are lost, OpenShift stops functioning, and you need to recover the etcd data from backup.\n\n> To maintain high availability, restore a lost master node as soon as possible.\n\n<a name=\"mnode-intend\"></a>\n\n### Make sure only the intended pods is running in the master nodes\nThis might sound obvious; however, we have found out customers running the monitoring server in the master node.  Even though OpenShift has put some taint on the master node to discourage this, it still happened.\n\nHere is an example of how to quickly check them.  The first command list all the nodes in the cluster. The second command list all the pods that are running in one of the master nodes.\n\n```\n$ oc get nodes\nNAME                                         STATUS   ROLES    AGE   VERSION\nip-10-0-139-124.us-west-2.compute.internal   Ready    master   13d   v1.16.2\nip-10-0-141-46.us-west-2.compute.internal    Ready    worker   13d   v1.16.2\nip-10-0-147-123.us-west-2.compute.internal   Ready    worker   13d   v1.16.2\nip-10-0-147-175.us-west-2.compute.internal   Ready    master   13d   v1.16.2\nip-10-0-161-65.us-west-2.compute.internal    Ready    worker   13d   v1.16.2\nip-10-0-171-121.us-west-2.compute.internal   Ready    master   13d   v1.16.2\n\n$ oc get pods --all-namespaces --field-selector status.phase=Running,spec.nodeName=ip-10-0-139-124.us-west-2.compute.internal\nNAMESPACE                                      NAME                                                                  READY   STATUS    RESTARTS   AGE\nopenshift-apiserver                            apiserver-g886t                                                       1/1     Running   0          13d\nopenshift-controller-manager                   controller-manager-xk4kw                                              1/1     Running   0          12d\nopenshift-etcd                                 etcd-member-ip-10-0-139-124.us-west-2.compute.internal                2/2     Running   0          13d\nopenshift-kube-apiserver                       kube-apiserver-ip-10-0-139-124.us-west-2.compute.internal             3/3     Running   0          12d\nopenshift-kube-controller-manager              kube-controller-manager-ip-10-0-139-124.us-west-2.compute.internal    3/3     Running   0          12d\nopenshift-kube-scheduler                       openshift-kube-scheduler-ip-10-0-139-124.us-west-2.compute.internal   1/1     Running   0          13d\nopenshift-machine-api                          machine-api-controllers-5d4cbc74bc-6gvtt                              4/4     Running   0          13d\nopenshift-monitoring                           node-exporter-m2pth                                                   2/2     Running   0          13d\nopenshift-monitoring                           thanos-querier-5db59f6b48-hzqh6                                       4/4     Running   0          13d\nopenshift-sdn                                  ovs-v9fjb                                                             1/1     Running   0          13d\nopenshift-sdn                                  sdn-controller-pslzf                                                  1/1     Running   0          13d\nopenshift-sdn                                  sdn-q5ddt                                                             1/1     Running   1          13d\n```\n\nFrom here, you can see the API-server, the controller-manager, the scheduler pods.  These are what we expect to run in the master node.  You can also see the networking component OVS and SDN.  There are also two monitoring pods, the Prometheus node-exporter and the Thanos querier running in the Openshift-monitoring namespace.  These are ok as they are the metric collector, not the actual monitoring server itself.  \n\n> If you start seeing the pods running in your _user-created project namespace_, then you know something is not correct.\n\n<a name=\"etcd-troubleshoot\"></a>\n\n### Troubleshooting etcd\nThe following list some of the error messages on etcd that you might be able to see in the log, with some suggestions on what could be causing it.\n\n```\nConnection error: desc = \"transport: Error while dialing dial tcp 0.0.0.0:2379: i/o timeout\"; Reconnecting to {0.0.0.0:2379 0 <nil>}\n```\n\n  - Most likely this is caused by a misconfigured networking, such as the Master Node firewall or misconfigured SDN.\n\n```\ndatabase space exceeded or applying raft message exceeded backend quota\n```\n\n  - The Kubernetes system might have a space quota for etcd, and one or more members of the etcd is encountering this quota.  This error normally will put the cluster into maintenance mode, and etcd only accepts key reads and deletes.\n\n```\ndial tcp <ip>:2379: getsockopt: connection refused or dial tcp <ip>:2380: getsockopt: connection refused\n```\n\n  - A connection to the etcd endpoint could not be established. Ensure that the etcd container is running on the host with the address shown.\n\n```\napply entries took too long\n```\n\n  - If the average write duration exceeds 100 milliseconds, etcd will warn that entries are taking too long to write. This issue can have a few causes: Slow disk, CPU starvation, Slow network.  We have described a platform environment suitable to run etcd.  Prevention is preferred to correcting the error.  \n\n```\nsnapshotting is taking more than <num> seconds to finish\n```\n\n  - Sending a snapshot took too long and exceeded the expected transfer time.  Most likely due to slow or congested network.\n\n<a name=\"worker-node\"></a>\n\n## Maintain worker node health: [ SRE ]\n\nUser application and platform management function should be run on the Worker node. A worker node is also called a _Compute_ node. Our recommendation is to run the management function on a special worker node.  This will be covered in the [Node for a specialized function](#mgmt-node) section of this chapter.\n\n> A node is not created by Kubernetes.  Depending on how OpenShift is provisioned, a node is created externally by the platform provisioning, such as cloud providers or through virtual machines.\n\nSo when Kubernetes creates a node, what it really creates is an object that represents the node. Kubernetes then validates the node.  If there are no pressure conditions (see example below), then the node is eligible to run pods.  \n\nNode status and other details about a node can be displayed using the following command:\n\n```\noc describe node <node-name>\n```\n\nThe following shows a reduced output of the command; some lines have been removed for brevity.\n\n```\nJuliusMBP:tmp jwahidin$ oc describe node ip-10-0-161-65.us-west-2.compute.internal\nName:               ip-10-0-161-65.us-west-2.compute.internal\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.8xlarge\nCreationTimestamp:  Wed, 05 Feb 2020 18:36:27 +1100\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 19 Feb 2020 21:42:41 +1100   Wed, 05 Feb 2020 18:36:27 +1100   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 19 Feb 2020 21:42:41 +1100   Wed, 05 Feb 2020 18:36:27 +1100   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 19 Feb 2020 21:42:41 +1100   Wed, 05 Feb 2020 18:36:27 +1100   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 19 Feb 2020 21:42:41 +1100   Wed, 05 Feb 2020 18:37:07 +1100   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.161.65\n  Hostname:     ip-10-0-161-65.us-west-2.compute.internal\n  InternalDNS:  ip-10-0-161-65.us-west-2.compute.internal\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         32\n  ephemeral-storage:           125277164Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      130514256Ki\n  pods:                        250\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         31500m\n  ephemeral-storage:           115455434152\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      129899856Ki\n  pods:                        250\nProviderID:                               aws:///us-west-2c/i-07e6720b3a098deeb\nNon-terminated Pods:                      (20 in total)\n  Namespace                               Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                                               ------------  ----------  ---------------  -------------  ---\n  openshift-cluster-node-tuning-operator  tuned-rjj8m                                                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         14d\n  openshift-dns                           dns-default-22wc4                                                  110m (0%)     0 (0%)      70Mi (0%)        512Mi (0%)     14d\n  openshift-image-registry                node-ca-ld8nq                                                      10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         14d\n\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests       Limits\n  --------                    --------       ------\n  cpu                         5520m (17%)    6 (19%)\n  memory                      15556Mi (12%)  18944Mi (14%)\n  ephemeral-storage           0 (0%)         0 (0%)\n  attachable-volumes-aws-ebs  0              0\nEvents:                       <none>\n\n```\n\nThere is some information worth mentioning from the Node Management point of view.\n- __Capacity__.  As you can see, there are three types of capacity-related listed: _Capacity_, _Allocatable_, and _Allocated resources_.  This will be important to see the capacity of your node/cluster.  As you can see, the capacity of the node is part of the node object.  Capacity definition includes: _cpu_, _memory_, _ephemeral-storage_, and _attachable-volumes_.\n- __Conditions__.  You can see that the Nodes currently is not under _Memory_, _PID_, and _Disk_ pressure.  So the Status is _Ready_. Note PID represents the number of processes.\n\nHeartbeats, sent by Kubernetes nodes to the node controller running in the master node, help determine the availability of a node.\n\nThe Kubernetes scheduler ensures that there are enough resources for all the pods on a node. It checks that the sum of the requests of containers on the node is no greater than the node capacity.\n\nWhen you want to see the capacity of all your nodes in a cluster then you can run `oc adm top node`.\n\n```\n$ oc adm top node\nNAME                                         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nip-10-0-139-124.us-west-2.compute.internal   506m         14%    2643Mi          17%       \nip-10-0-141-46.us-west-2.compute.internal    408m         1%     5636Mi          4%        \nip-10-0-147-123.us-west-2.compute.internal   374m         1%     6204Mi          4%        \nip-10-0-147-175.us-west-2.compute.internal   559m         15%    3270Mi          21%       \nip-10-0-161-65.us-west-2.compute.internal    183m         0%     4731Mi          3%        \nip-10-0-171-121.us-west-2.compute.internal   623m         17%    3189Mi          21%       \n```\n\n<a name=\"stop-start\"></a>\n\n### How to stop and start a worker node manually\nAs part of Day 2 activities, you might want to do maintenance on a worker node.  The following describes the process of restarting a worker node:\n\n- Disable Scheduling on the node using `oc adm cordon <node-name>`.\n\n```\n$ oc get node ip-10-0-147-123.us-west-2.compute.internal\nNAME                                         STATUS   ROLES    AGE   VERSION\nip-10-0-147-123.us-west-2.compute.internal   Ready    worker   14d   v1.16.2\n\n$ oc adm cordon ip-10-0-147-123.us-west-2.compute.internal\nnode/ip-10-0-147-123.us-west-2.compute.internal cordoned\n\n$ oc get node ip-10-0-147-123.us-west-2.compute.internal\nNAME                                         STATUS                     ROLES    AGE   VERSION\nip-10-0-147-123.us-west-2.compute.internal   Ready,SchedulingDisabled   worker   14d   v1.16.2\n```\n\n- Perform a dry-run on evacuating/draining the pod from the node.\n\n```\n$ oc adm drain ip-10-0-147-123.us-west-2.compute.internal --dry-run=true  \nnode/ip-10-0-147-123.us-west-2.compute.internal already cordoned (dry run)\nnode/ip-10-0-147-123.us-west-2.compute.internal drained (dry run)\n```\n\n- Evacuate the current running pod from the node.  If your pod is managed by a _DaemonSet_, or if your pod is using a local storage such as _EmptyDir_ Storage, the drain will display some message, you then need to specify additional command-line flags to the drain operation.\n  - If you are sure that you can discard the local data or evacuate the daemon set managed pod, then use the `--ignore-daemonsets=true` and `--delete-local-data=true` to override the conditions.\n  - You can also use the `--force=true`.\n  - If your node is providing services to the cluster, such as router or master node, there are additional steps that you need to take. These steps is mentioned in the OpenShift documentation on [Nodes](https://docs.openshift.com/container-platform/4.3/nodes/nodes/nodes-nodes-viewing.html)\n  - You might need to run the drain operation a few times until there is no error message.\n  - Because of this possible iteration, OpenShift 4.3 introduces the [Machine Config Operator](#mco). Other tools to evacuate pods from nodes are available, and an example is [Draino](https://github.com/planetlabs/draino).\n\n```\n    $ oc adm drain ip-10-0-147-123.us-west-2.compute.internal\n    node/ip-10-0-147-123.us-west-2.compute.internal already cordoned\n    error: unable to drain node \"ip-10-0-147-123.us-west-2.compute.internal\", aborting command...\n\n    There are pending nodes to be drained:\n     ip-10-0-147-123.us-west-2.compute.internal\n    cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): openshift-cluster-node-tuning-operator/tuned-67c6c, openshift-dns/dns-default-xltzj, openshift-image-registry/node-ca-lw72z, openshift-machine-config-operator/machine-config-daemon-fkskx, openshift-monitoring/node-exporter-j6pz8, openshift-multus/multus-ln5zt, openshift-sdn/ovs-mqclb, openshift-sdn/sdn-qz497, openshift-storage/csi-cephfsplugin-dzzhw, openshift-storage/csi-rbdplugin-c8q62\n    cannot delete Pods with local storage (use --delete-local-data to override): openshift-monitoring/alertmanager-main-1, openshift-monitoring/prometheus-k8s-0, openshift-storage/csi-cephfsplugin-provisioner-5cdcfcc86b-x6bnb, openshift-storage/csi-rbdplugin-provisioner-8fdc8f955-pxmw5, openshift-storage/noobaa-core-0, openshift-storage/rook-ceph-mgr-a-78c4576db8-cjxxh, openshift-storage/rook-ceph-osd-1-bb9cdcfd6-nrdwc\n```\n\n- Once the drain is completed, you can shutdown the kubelet and the docker(or containerd) before you perform the maintenance action required.  Depending on the infrastructure running your node, the kubelet and the docker shutdown may be different.  For RHEL you can use the following command.\n  - To shutdown the kubelet.\n\n```\n    sudo systemctl stop kubelet\n```\n\n- To shutdown docker\n\n```\n    sudo systemctl stop docker\n```\n\n- To bring up docker\n\n```\n    sudo systemctl start kubelet\n```  \n\n- To bring up the kubelet\n\n```\n    sudo systemctl start kubelet\n    sudo systemctl status kubelet\n```\n\n- To view the log of the kubelet on the operating system\n\n```\n    sudo journalctl -e -u kubelet\n```\n\n- Enable Scheduling on the node using `oc adm uncordon <node-name>`.\n\n```\n$ oc adm uncordon ip-10-0-147-123.us-west-2.compute.internal\nnode/ip-10-0-147-123.us-west-2.compute.internal uncordoned\n$ oc get node ip-10-0-147-123.us-west-2.compute.internal\nNAME                                         STATUS   ROLES    AGE   VERSION\nip-10-0-147-123.us-west-2.compute.internal   Ready    worker   14m   v1.16.2\n```\n\n#### Use of cordon and uncordon during debugging\nEarlier we have seen how do you use the cordon and uncordon as part of the restart of a Node.  We can use the cordon and uncordon as a generic approach to isolate a node during troubleshooting.\n\n> During troubleshooting operation on a Node, you can use the cordon to stop the node from receiving any more pods\n\n<a name=\"node-up\"></a>\n\n## Ensure node resiliency: [ SRE ]\n\nYou have your OpenShift cluster running. Your management stack is in place. You have done the backup and prove that the backup is good by doing a restore. What is next?  How do you determine that your cluster is resilient as an OpenShift cluster should be?  Well, like backup you test it by doing a restore, with node resiliency you check it by bringing down the node down.  This action is in line with the foundation of Chaos Engineering.  \n\nIn [the Principle of Chaos](http://principlesofchaos.org/?lang=ENcontent), it defines Chaos Engineering as follow:\n> Chaos Engineering is the discipline of experimenting on a system in order to build confidence in the systemâ€™s capability to withstand turbulent conditions in production.\n\nWe recommend the following approach:\n- Become familiar with the cluster and the steps required to simulate the test.  To do this, you might start with the Development or the Test Environment.\n- Both the Master and Worker nodes need to be tested; at the beginning, they should be tested separately.\n- Once you are familiar with the process on the Test environment, then you can do it in the pre-production or even the production environment.  Make sure you plan it first.  \n- As part of the planning, you can plan to start the production test during the cluster least busy hour; when you bring down nodes, the loads need to be transferred to other operating nodes.  Based on your capacity information, can your cluster still handle the production loads without the nodes that you are bringing down?  If not, then you have a gap; refer to [the Capacity](../Capacity) chapter of this repository for more information on OpenShift capacity.\n- Another aspect of the plan is during the initial test run on production, and you might want to take the _error budget_ into consideration, i.e., do it when you still have some error budget.\n- Your goal is to be able to confidently bring down any node unplanned.  \n- Once you achieve this goal, set as a standard process to apply the Kubernetes principle of __Disposable Infrastructure__; that is, all your nodes should age no more than a set period (for example, seven days).\n\n<a name=\"co-health\"></a>\n\n## Assure cluster operator health: [ SRE ]\n\nIn OpenShift 4.x, the master and worker nodes, are managed by operators, amongst them are `kube-apiserver`, `kube-controller-manager` and `kube-scheduler` operators.  To ensure the health of the nodes then you have to ensure that you monitor these operators.  OpenShift 4.x has the Cluster Operator, `co`, CRD resources.\n\nTo quickly check the status of your Cluster Operator Status, you can run the following command.\n\n```\n$ oc get co\nNAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE\nauthentication                             4.3.0     True        False         False      17d\ncloud-credential                           4.3.0     True        False         False      17d\ncluster-autoscaler                         4.3.0     True        False         False      17d\nconsole                                    4.3.0     True        False         False      17d\ndns                                        4.3.0     True        False         False      17d\nimage-registry                             4.3.0     True        False         False      16d\ningress                                    4.3.0     True        False         False      16d\ninsights                                   4.3.0     True        False         False      17d\nkube-apiserver                             4.3.0     True        False         False      17d\nkube-controller-manager                    4.3.0     True        False         False      17d\nkube-scheduler                             4.3.0     True        False         False      17d\nmachine-api                                4.3.0     True        False         False      17d\nmachine-config                             4.3.0     True        False         False      17d\nmarketplace                                4.3.0     True        False         False      17d\nmonitoring                                 4.3.0     True        False         False      2d1h\nnetwork                                    4.3.0     True        False         False      17d\nnode-tuning                                4.3.0     True        False         False      17d\nopenshift-apiserver                        4.3.0     True        False         False      17d\nopenshift-controller-manager               4.3.0     True        False         False      17d\nopenshift-samples                          4.3.0     True        False         False      17d\noperator-lifecycle-manager                 4.3.0     True        False         False      17d\noperator-lifecycle-manager-catalog         4.3.0     True        False         False      17d\noperator-lifecycle-manager-packageserver   4.3.0     True        False         False      16d\nservice-ca                                 4.3.0     True        False         False      17d\nservice-catalog-apiserver                  4.3.0     True        False         False      17d\nservice-catalog-controller-manager         4.3.0     True        False         False      17d\nstorage                                    4.3.0     True        False         False      17d\n\n```\n\nIf you want to learn more about the specifics of the operator, then you can run the `oc describe co/<operator-name>`.\nFor example, the following command describes the _openshift-apiserver_ operator, some of the output had been cut for brevity, but notice the `Status` section, and it listed `Conditions` as well as `Related Objects.`  From this Related Objects, you can learn about all the Related Objects to the apiserver.\n\n```\n$ oc describe co/openshift-apiserver\nName:         openshift-apiserver\nNamespace:    \nLabels:       <none>\nAnnotations:  <none>\nAPI Version:  config.openshift.io/v1\nKind:         ClusterOperator\nMetadata:\n  Creation Timestamp:  2020-02-04T20:12:14Z\n  Generation:          1\n  Resource Version:    11975\n  Self Link:           /apis/config.openshift.io/v1/clusteroperators/openshift-apiserver\n  UID:                 7774b841-583b-4b82-9e88-6edd44dca3d2\nSpec:\nStatus:\n  Conditions:\n    Last Transition Time:  2020-02-04T20:16:34Z\n    Reason:                AsExpected\n    Status:                False\n    Type:                  Degraded\n    Last Transition Time:  2020-02-04T20:18:42Z\n    Reason:                AsExpected\n    Status:                False\n    Type:                  Progressing\n    Last Transition Time:  2020-02-04T20:18:29Z\n    Reason:                AsExpected\n    Status:                True\n    Type:                  Available\n    Last Transition Time:  2020-02-04T20:12:15Z\n    Reason:                AsExpected\n    Status:                True\n    Type:                  Upgradeable\n  Extension:               <nil>\n  Related Objects:\n    Group:     operator.openshift.io\n    Name:      cluster\n    Resource:  openshiftapiservers\n    Group:     \n    Name:      openshift-config\n    Resource:  namespaces\n    Group:     \n    Name:      openshift-config-managed\n    Resource:  namespaces\n    Group:     \n    Name:      openshift-apiserver-operator\n    Resource:  namespaces\n    Group:     \n    Name:      openshift-apiserver\n    Resource:  namespaces\n    Group:     apiregistration.k8s.io\n    Name:      v1.apps.openshift.io\n    Resource:  apiservices\n    . . .\n    Group:     apiregistration.k8s.io\n    Name:      v1.user.openshift.io\n    Resource:  apiservices\n  Versions:\n    Name:     operator\n    Version:  4.3.0\n    Name:     openshift-apiserver\n    Version:  \nEvents:       <none>\n```\n\n<a name=\"mco\"></a>\n\n### Machine Config Operator\nOpenShift 4.x introduces lots of Operator.  One of them is the Machine Config Operator or MCO for short.  One of the components of the Machine Config Operator is the Machine Config Daemon.\n\nThe Machine Config Daemon runs on every node to manage the configuration changes and updates on the nodes. Machine Config Daemon also provides a set of metrics. Some of the metrics are:\n- *ssh_accessed*. Shows the number of successful SSH authentications into the node.\n- *drain_time*. Shows how much time the drain took.\n\nThese metrics can be viewed through the Prometheus Cluster Monitoring stack. The chapter on [Monitoring](../Monitoring) will describe the Prometheus Monitoring in more detail.\n\nMore information on MCO can be found in the [Machine Config Operator](https://github.com/openshift/machine-config-operator) git pages.\n\n## Application tasks\n\n<a name=\"mgmt-node\"></a>\n\n## Configure node for a specialized function: [ SRE, DevOps Engineer ]\nKubernetes employs a master/worker node architecture. Kubernetes master servers maintain the information about the server cluster, and worker nodes run the actual application workloads.  In practice, you might want to introduce a specialized function node.  \n\nA good example of nodes with specialized function is a set of nodes to run the cluster management application. The main reason is to separate the application load and management load.  \n\n> If the nodes are not separated, then there is a potential disruption on the application because of the load from the management pods.  You do not want to have the application pods being restricted from scaling up because all the resources are taken by somebody running an analysis on the logs collected for the past three months, for example.\n\nThere are several methods to ensure the placement of the management function into a specific node.  \n- using [labels](https://docs.openshift.com/container-platform/4.3/nodes/pods/nodes-pods-node-selectors.html)\n- using [node affinity](https://docs.openshift.com/container-platform/4.3/nodes/scheduling/nodes-scheduler-node-affinity.html)\n- using [node taint](https://docs.openshift.com/container-platform/4.3/nodes/scheduling/nodes-scheduler-taints-tolerations.html)\n\nThese pod placement methods are covered in more detail in the [OCP day-2 Operations: on node selection](https://apps.na.collabserv.com/blogs/7be81ac0-37f8-466b-a858-2525e0b798cf/entry/OCP_day_2_operations_on_Node_Selection?lang=en_us) blog.\n\n<a name=\"mgmt-security\"></a>\n\n### Preventing security breach by using Node Restriction\nUsing a _nodeSelector_ is useful as it allows you to direct the pod to only run on a selected node.  For security reasons, it is recommended to use a label that can not be modified by the kubelet process that runs on the node.\n\nKubelet is an application that runs on the host operating system that provides the node functionality, so Kubelet is not running in a pod.  As a kubelet is an application running on the operating system, it is possible that it can be infected.  Theoretically, it is possible for an infected kubelet to change the label of the node that it is running.  By changing the node label, then it is possible that the kubelet will direct the scheduler to schedule workloads on the infected node.  \n\nTo address this, Kubernetes introduced the concept of Node restriction through the `NodeRestriction admission plugin`.  The plugin prevents kubelets from setting or modifying labels with a node-restriction `kubernetes.io/` prefix.  \n\n> Ensure to enable the NodeRestriction admission plugin\n\nEven though you are not using a NodeSelector, you should consider enabling the NodeRestriction admission plugin to ensure your node security.\n\nMore information on Node restriction can be found in the Kubernetes site on [Using Admission Controllers](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers).\n\n<a name=\"daemonsets\"></a>\n\n### Node placement and DaemonSets\nYou use a DaemonSet to ensure that all (or some) Nodes run the pod mentioned in the DaemonSet.  This is useful for implementing common functionality across the whole cluster.  Examples include Cluster storage, such as _ceph_, Log collection, such as _fluentd_, and Monitoring collectors, such as the _Prometheus Node Exporter_.  Using DaemonSet helps you ensuring that monitoring, backup, logging are done across all your nodes.\n\n[The OpenShift Documentation on DaemonSets](https://docs.openshift.com/container-platform/4.3/nodes/jobs/nodes-pods-daemonsets.html) warns on the potential conflict between daemon sets and the default Project Node Selector.  The effect will be in frequent pod recreates that put a heavy load on the cluster.\n\nIn the case of the default Project Node Selector, you can disable them by doing the following.\n\n```\n# to change the default project setting\noc patch namespace myproject -p '{\"metadata\": {\"annotations\": {\"openshift.io/node-selector\": \"\"}}}'\n\n# if you do not change the default project setting. Use the --node-selector as you create a new project.\noc adm new-project --node-selector=\"\".\n```\n\nDaemonsets should ignore the node-placement methods mentioned in this chapter.   \n\n> However, as part of your Day 2 activities, should you observe some irregularity in frequent pod recreates as you configure your pod placement policy, then you may want to check the daemon sets.  There is a possibility of configuration conflict between DaemonSets and the Node Placement policy.\n\n\n## Implementing Node Management\n\n## Kubernetes\n\nKubernetes site provides more information on [Managing Computer Resources on Containers](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/)\n\n## OpenShift\n\n- OpenShift has sections on its documentation discussing [Nodes](https://docs.openshift.com/container-platform/4.3/nodes/nodes/nodes-nodes-viewing.html) and  [Controlling pod placement using the scheduler](https://docs.openshift.com/container-platform/4.3/nodes/scheduling/nodes-scheduler-about.html)\n- [The Machine Config Operator](https://github.com/openshift/machine-config-operator)\n\n## On IBM Cloud\nTBD\n\n## With IBM Cloud Pak for MCM\n\nMCM extends the discussion presented here for multi-cluster environments, as well as creating and enforcing place rule as well as Governance rule.  More information can be found on [the IBM Cloudpak for MCM documentation](https://www.ibm.com/support/knowledgecenter/SSFC4F_1.2.0/kc_welcome_cloud_pak.html).\n\n\n## Others\n\n- [Draino](https://github.com/planetlabs/draino) Automatically cordon and drain Kubernetes nodes based on node conditions.\n\n## Other consideration\n\nn/a","type":"Mdx","contentDigest":"cc38e84f1d52a5463144e120e1fe1061","counter":569,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"OpenShift Platform Day2 - Node","description":"OpenShift Day2 Node","keywords":"OpenShift, day2, node"},"exports":{},"rawBody":"---\ntitle: OpenShift Platform Day2 - Node\ndescription: OpenShift Day2 Node\nkeywords: 'OpenShift, day2, node'\n---\n\n## Node Management Overview\nAll pods in Kubernetes or OpenShift Container Platform run in the nodes.  Because of this, ensuring the health of your nodes is a critical Day 2 activity.  You should be able to replace a node without affecting the service.  As part of Day 2 activities, you need to be able to ensure this.\n\nWhen you look at node management, it is also important to refer back to the design of the cluster. Design and sizing your node is a Day 0 activity. By referring to this information, you will know the design principle and the design decision that was made for the cluster.\n\nThis chapter will focus on steps that you can take to ensure the health of your nodes using the standard facility that come out of the box with OpenShift.  The steps presented here can be evolved to make use of other practices presented in the other chapters of this repository, such as performing [logging](../Logging), [monitoring](../Monitoring), and [event management](../EventManagement).\n\n## Day 1 Platform\nWhen you consider Day 2 node management, you need to have access to the Day 0 design principle and/or design decision.  These are examples of useful information to collect:\n- __Sizing Considerations__. How many pods can my cluster handle?  Are there any assumptions made on the application potential loading?  What is the design decision on the number of master and worker nodes?  Is there any scale-out consideration made?\n- __Environment Scenarios__. What is the design for the number of environment types (Dev, Test, Prod) for the clusters?  Is the cluster running as a private cloud, or hosted on a cloud provider, or a hybrid?\n- __Installation Method__.  Terraform, UPI, IPI?\n- __Feature__.  What features am I going to offer my tenants?\n- __CI/CD__. Do I have a strategy for CI/CD?\n- __Security__.  What is supported and not supported by the security department?  Has the cluster design been approved by the security department?\n- __Management__.  What is the design of the management functionality of the cluster?\n\n## Day 2 Platform\n\nThe following shows Day 2 activities on managing the node:\n- [Manage Master node health](#master-node)\n  - [Do you increase the number of master nodes?](#mnode-number)\n  - [Master nodes platform requirement](#mnode-requirement)\n  - [Resiliency of master node](#mnode-resiliency)\n  - [Make sure only the intended pods is running in the master nodes](#mnode-intend)\n  - [Troubleshooting etcd](#etcd-troubleshoot)\n- [Maintain worker node health](#worker-node)\n  - [How to stop and start a worker node manually](#stop-start)\n- [Ensure node resiliency](#node-up)\n- [Assure cluster operator health](#co-health)\n  - [Machine Config Operator](#mco)\n\n\n## Day 1 Application\n\nFor Node Management, you need to ensure that the applications that are already installed on Day 1 are running in the nodes as designed.   You need to __verify that the application has been deployed in the correct node__.  For example, you need to define the initial pod placement policy for the applications identified in Day 1. This activity will be extended in Day 2 for applications that will be introduced as part of the steady-state operation.\n\n## Day 2 Application\n\nDuring Day 2 operations, new applications will be deployed to the OpenShift cluster. You need to ensure that your worker nodes will have enough capacity to support these applications.  One way is to define the project template includes the required Range Limit resources and Quota resources. _The Project template_ is covered in [Builds and Deploy](../BuildDeploy) chapter, Range Limit, and Quota are discussed in the [Capacity](../Capacity) chapter.   \n\nAnother important aspect of Day 2 Node Management on Application is to satisfy the applications' workload requirement for performance, security, and manageability through these activities:\n- [Configure node for a specialized function](#mgmt-node)\n  - [Preventing security breach by using Node Restriction](#mgmt-security)\n  - [Node placement and DaemonSets](#daemonsets)\n\n\n## Mapping to Personas\n\nPersona | task\n--- | ---\nSRE | Manage master node health\nSRE | Maintain worker node health\nSRE | Ensure node resiliency\nSRE | Assure cluster operator health\nSRE, DevOps Engineer | Configure node for a specialized function\n\n\n\n## Platform tasks\n\n<a name=\"master-node\"></a>\n\n## Manage master node health: [ SRE ]\n\nAs part of the Day 2 operation, you need to ensure the health of your master node.  In particular, there are three pods that you need to ensure to be in good running operation: __the etcd service provider__, __the API Server__, and __the Controller and Scheduler__.  \n\nAmongst these three types of pods, etcd requires the persistent volume (storage) as it can generate substantial network traffic, so one way to ensure the master node health is to ensure the health of the etcd services.  The Operators manage the API server, the controller, and the scheduler.  \n\n<a name=\"mnode-number\"></a>\n\n### Do you increase the number of master nodes?\nThe etcd's main function is to store Kubernetesâ€™ configuration information; hence its performance is crucial to the efficient performance of your cluster.\n\netcd is a distributed key-value store using [the RAFT consensus algorithm](https://raft.github.io). Because of the consensus algorithm, etcd must be deployed in odd numbers of pods to maintain quorum.  This normally translates to three nodes in a Production environment.\n\nFrom the operational aspects of etcd, the etcd service is considered an active-active cluster. Meaning, an etcd client can write to any of the etcd nodes and the cluster will replicate the data, and maintain consistency of the data across the instances.  When one etcd pod in one master node performs a write, it needs to synchronize the content with other etcd pods.\n\nYour Day 0 design document may already specify three etcd pods.  As your cluster size grow, you might think that you need to increase your etcd nodes (Master nodes) beyond 3. However, as general guidance:\n\n> For performance reason, you should not have more than 3 etcd nodes (master nodes).\n\nThe main reason is that as etcd implements a consensus algorithm, each node needs to synchronize with other nodes.  When you have three nodes, each node needs to communicate with two other nodes.  When you have five nodes, then each node needs to talk to four other nodes, causing exponential growth in network traffic that might affect the network capacity of your cluster.  \n\nWhen your cluster grows, if you need to scale etcd, which is unlikely, then rather than adding more etcd pods, it might be better to increase the capacity of each of the etcd pods.  You can also increase the compute (CPU/memory) power of the master node where the etcd runs.\n\n<a name=\"mnode-requirement\"></a>\n\n### Master nodes platform requirement\nThe design of the master nodes platform mostly had been done in Day 0, but the design decision might not have been spelled out in detail in the design document.\n\nAs part of Day 2 activities, you might need to replace a bad master node, or upgrade the CPU/memory capacity of the master node.  The following is a general recommendation on the selection of the master node to host the etcd pod:\n\n> - It needs storage with fast access disk.\n> - It needs a low latency in communicating with other etcd pods, this means fast networking.\n> - The etcd store should not be located on the same disk as a disk-intensive service such as the database to store logging and metric information.\n> - It should not be spread across data centers or, in the case of public clouds, availability zones\n\n<a name=\"mnode-resiliency\"></a>\n\n### Resiliency of master node\nWe have recommended three master nodes as a good number for production.  For etcd, the quorum for three master nodes is two nodes.  Let us consider the failure scenario:\n\nWhen one master node (i.e., etcd pod) is lost, The quorum is still ok.  So the operation can still recover by replacing the lost master node.\n\nWhen two master nodes are lost, etcd does not have the quorum to write to disk.  In other words, you can not make changes, no new project or resource, no scaling up or down.  However, the currently running pods will not be terminated.\n\nWhen three (all) master nodes are lost, OpenShift stops functioning, and you need to recover the etcd data from backup.\n\n> To maintain high availability, restore a lost master node as soon as possible.\n\n<a name=\"mnode-intend\"></a>\n\n### Make sure only the intended pods is running in the master nodes\nThis might sound obvious; however, we have found out customers running the monitoring server in the master node.  Even though OpenShift has put some taint on the master node to discourage this, it still happened.\n\nHere is an example of how to quickly check them.  The first command list all the nodes in the cluster. The second command list all the pods that are running in one of the master nodes.\n\n```\n$ oc get nodes\nNAME                                         STATUS   ROLES    AGE   VERSION\nip-10-0-139-124.us-west-2.compute.internal   Ready    master   13d   v1.16.2\nip-10-0-141-46.us-west-2.compute.internal    Ready    worker   13d   v1.16.2\nip-10-0-147-123.us-west-2.compute.internal   Ready    worker   13d   v1.16.2\nip-10-0-147-175.us-west-2.compute.internal   Ready    master   13d   v1.16.2\nip-10-0-161-65.us-west-2.compute.internal    Ready    worker   13d   v1.16.2\nip-10-0-171-121.us-west-2.compute.internal   Ready    master   13d   v1.16.2\n\n$ oc get pods --all-namespaces --field-selector status.phase=Running,spec.nodeName=ip-10-0-139-124.us-west-2.compute.internal\nNAMESPACE                                      NAME                                                                  READY   STATUS    RESTARTS   AGE\nopenshift-apiserver                            apiserver-g886t                                                       1/1     Running   0          13d\nopenshift-controller-manager                   controller-manager-xk4kw                                              1/1     Running   0          12d\nopenshift-etcd                                 etcd-member-ip-10-0-139-124.us-west-2.compute.internal                2/2     Running   0          13d\nopenshift-kube-apiserver                       kube-apiserver-ip-10-0-139-124.us-west-2.compute.internal             3/3     Running   0          12d\nopenshift-kube-controller-manager              kube-controller-manager-ip-10-0-139-124.us-west-2.compute.internal    3/3     Running   0          12d\nopenshift-kube-scheduler                       openshift-kube-scheduler-ip-10-0-139-124.us-west-2.compute.internal   1/1     Running   0          13d\nopenshift-machine-api                          machine-api-controllers-5d4cbc74bc-6gvtt                              4/4     Running   0          13d\nopenshift-monitoring                           node-exporter-m2pth                                                   2/2     Running   0          13d\nopenshift-monitoring                           thanos-querier-5db59f6b48-hzqh6                                       4/4     Running   0          13d\nopenshift-sdn                                  ovs-v9fjb                                                             1/1     Running   0          13d\nopenshift-sdn                                  sdn-controller-pslzf                                                  1/1     Running   0          13d\nopenshift-sdn                                  sdn-q5ddt                                                             1/1     Running   1          13d\n```\n\nFrom here, you can see the API-server, the controller-manager, the scheduler pods.  These are what we expect to run in the master node.  You can also see the networking component OVS and SDN.  There are also two monitoring pods, the Prometheus node-exporter and the Thanos querier running in the Openshift-monitoring namespace.  These are ok as they are the metric collector, not the actual monitoring server itself.  \n\n> If you start seeing the pods running in your _user-created project namespace_, then you know something is not correct.\n\n<a name=\"etcd-troubleshoot\"></a>\n\n### Troubleshooting etcd\nThe following list some of the error messages on etcd that you might be able to see in the log, with some suggestions on what could be causing it.\n\n```\nConnection error: desc = \"transport: Error while dialing dial tcp 0.0.0.0:2379: i/o timeout\"; Reconnecting to {0.0.0.0:2379 0 <nil>}\n```\n\n  - Most likely this is caused by a misconfigured networking, such as the Master Node firewall or misconfigured SDN.\n\n```\ndatabase space exceeded or applying raft message exceeded backend quota\n```\n\n  - The Kubernetes system might have a space quota for etcd, and one or more members of the etcd is encountering this quota.  This error normally will put the cluster into maintenance mode, and etcd only accepts key reads and deletes.\n\n```\ndial tcp <ip>:2379: getsockopt: connection refused or dial tcp <ip>:2380: getsockopt: connection refused\n```\n\n  - A connection to the etcd endpoint could not be established. Ensure that the etcd container is running on the host with the address shown.\n\n```\napply entries took too long\n```\n\n  - If the average write duration exceeds 100 milliseconds, etcd will warn that entries are taking too long to write. This issue can have a few causes: Slow disk, CPU starvation, Slow network.  We have described a platform environment suitable to run etcd.  Prevention is preferred to correcting the error.  \n\n```\nsnapshotting is taking more than <num> seconds to finish\n```\n\n  - Sending a snapshot took too long and exceeded the expected transfer time.  Most likely due to slow or congested network.\n\n<a name=\"worker-node\"></a>\n\n## Maintain worker node health: [ SRE ]\n\nUser application and platform management function should be run on the Worker node. A worker node is also called a _Compute_ node. Our recommendation is to run the management function on a special worker node.  This will be covered in the [Node for a specialized function](#mgmt-node) section of this chapter.\n\n> A node is not created by Kubernetes.  Depending on how OpenShift is provisioned, a node is created externally by the platform provisioning, such as cloud providers or through virtual machines.\n\nSo when Kubernetes creates a node, what it really creates is an object that represents the node. Kubernetes then validates the node.  If there are no pressure conditions (see example below), then the node is eligible to run pods.  \n\nNode status and other details about a node can be displayed using the following command:\n\n```\noc describe node <node-name>\n```\n\nThe following shows a reduced output of the command; some lines have been removed for brevity.\n\n```\nJuliusMBP:tmp jwahidin$ oc describe node ip-10-0-161-65.us-west-2.compute.internal\nName:               ip-10-0-161-65.us-west-2.compute.internal\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.8xlarge\nCreationTimestamp:  Wed, 05 Feb 2020 18:36:27 +1100\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 19 Feb 2020 21:42:41 +1100   Wed, 05 Feb 2020 18:36:27 +1100   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 19 Feb 2020 21:42:41 +1100   Wed, 05 Feb 2020 18:36:27 +1100   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 19 Feb 2020 21:42:41 +1100   Wed, 05 Feb 2020 18:36:27 +1100   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 19 Feb 2020 21:42:41 +1100   Wed, 05 Feb 2020 18:37:07 +1100   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.161.65\n  Hostname:     ip-10-0-161-65.us-west-2.compute.internal\n  InternalDNS:  ip-10-0-161-65.us-west-2.compute.internal\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         32\n  ephemeral-storage:           125277164Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      130514256Ki\n  pods:                        250\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         31500m\n  ephemeral-storage:           115455434152\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      129899856Ki\n  pods:                        250\nProviderID:                               aws:///us-west-2c/i-07e6720b3a098deeb\nNon-terminated Pods:                      (20 in total)\n  Namespace                               Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                                               ------------  ----------  ---------------  -------------  ---\n  openshift-cluster-node-tuning-operator  tuned-rjj8m                                                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         14d\n  openshift-dns                           dns-default-22wc4                                                  110m (0%)     0 (0%)      70Mi (0%)        512Mi (0%)     14d\n  openshift-image-registry                node-ca-ld8nq                                                      10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         14d\n\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests       Limits\n  --------                    --------       ------\n  cpu                         5520m (17%)    6 (19%)\n  memory                      15556Mi (12%)  18944Mi (14%)\n  ephemeral-storage           0 (0%)         0 (0%)\n  attachable-volumes-aws-ebs  0              0\nEvents:                       <none>\n\n```\n\nThere is some information worth mentioning from the Node Management point of view.\n- __Capacity__.  As you can see, there are three types of capacity-related listed: _Capacity_, _Allocatable_, and _Allocated resources_.  This will be important to see the capacity of your node/cluster.  As you can see, the capacity of the node is part of the node object.  Capacity definition includes: _cpu_, _memory_, _ephemeral-storage_, and _attachable-volumes_.\n- __Conditions__.  You can see that the Nodes currently is not under _Memory_, _PID_, and _Disk_ pressure.  So the Status is _Ready_. Note PID represents the number of processes.\n\nHeartbeats, sent by Kubernetes nodes to the node controller running in the master node, help determine the availability of a node.\n\nThe Kubernetes scheduler ensures that there are enough resources for all the pods on a node. It checks that the sum of the requests of containers on the node is no greater than the node capacity.\n\nWhen you want to see the capacity of all your nodes in a cluster then you can run `oc adm top node`.\n\n```\n$ oc adm top node\nNAME                                         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nip-10-0-139-124.us-west-2.compute.internal   506m         14%    2643Mi          17%       \nip-10-0-141-46.us-west-2.compute.internal    408m         1%     5636Mi          4%        \nip-10-0-147-123.us-west-2.compute.internal   374m         1%     6204Mi          4%        \nip-10-0-147-175.us-west-2.compute.internal   559m         15%    3270Mi          21%       \nip-10-0-161-65.us-west-2.compute.internal    183m         0%     4731Mi          3%        \nip-10-0-171-121.us-west-2.compute.internal   623m         17%    3189Mi          21%       \n```\n\n<a name=\"stop-start\"></a>\n\n### How to stop and start a worker node manually\nAs part of Day 2 activities, you might want to do maintenance on a worker node.  The following describes the process of restarting a worker node:\n\n- Disable Scheduling on the node using `oc adm cordon <node-name>`.\n\n```\n$ oc get node ip-10-0-147-123.us-west-2.compute.internal\nNAME                                         STATUS   ROLES    AGE   VERSION\nip-10-0-147-123.us-west-2.compute.internal   Ready    worker   14d   v1.16.2\n\n$ oc adm cordon ip-10-0-147-123.us-west-2.compute.internal\nnode/ip-10-0-147-123.us-west-2.compute.internal cordoned\n\n$ oc get node ip-10-0-147-123.us-west-2.compute.internal\nNAME                                         STATUS                     ROLES    AGE   VERSION\nip-10-0-147-123.us-west-2.compute.internal   Ready,SchedulingDisabled   worker   14d   v1.16.2\n```\n\n- Perform a dry-run on evacuating/draining the pod from the node.\n\n```\n$ oc adm drain ip-10-0-147-123.us-west-2.compute.internal --dry-run=true  \nnode/ip-10-0-147-123.us-west-2.compute.internal already cordoned (dry run)\nnode/ip-10-0-147-123.us-west-2.compute.internal drained (dry run)\n```\n\n- Evacuate the current running pod from the node.  If your pod is managed by a _DaemonSet_, or if your pod is using a local storage such as _EmptyDir_ Storage, the drain will display some message, you then need to specify additional command-line flags to the drain operation.\n  - If you are sure that you can discard the local data or evacuate the daemon set managed pod, then use the `--ignore-daemonsets=true` and `--delete-local-data=true` to override the conditions.\n  - You can also use the `--force=true`.\n  - If your node is providing services to the cluster, such as router or master node, there are additional steps that you need to take. These steps is mentioned in the OpenShift documentation on [Nodes](https://docs.openshift.com/container-platform/4.3/nodes/nodes/nodes-nodes-viewing.html)\n  - You might need to run the drain operation a few times until there is no error message.\n  - Because of this possible iteration, OpenShift 4.3 introduces the [Machine Config Operator](#mco). Other tools to evacuate pods from nodes are available, and an example is [Draino](https://github.com/planetlabs/draino).\n\n```\n    $ oc adm drain ip-10-0-147-123.us-west-2.compute.internal\n    node/ip-10-0-147-123.us-west-2.compute.internal already cordoned\n    error: unable to drain node \"ip-10-0-147-123.us-west-2.compute.internal\", aborting command...\n\n    There are pending nodes to be drained:\n     ip-10-0-147-123.us-west-2.compute.internal\n    cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): openshift-cluster-node-tuning-operator/tuned-67c6c, openshift-dns/dns-default-xltzj, openshift-image-registry/node-ca-lw72z, openshift-machine-config-operator/machine-config-daemon-fkskx, openshift-monitoring/node-exporter-j6pz8, openshift-multus/multus-ln5zt, openshift-sdn/ovs-mqclb, openshift-sdn/sdn-qz497, openshift-storage/csi-cephfsplugin-dzzhw, openshift-storage/csi-rbdplugin-c8q62\n    cannot delete Pods with local storage (use --delete-local-data to override): openshift-monitoring/alertmanager-main-1, openshift-monitoring/prometheus-k8s-0, openshift-storage/csi-cephfsplugin-provisioner-5cdcfcc86b-x6bnb, openshift-storage/csi-rbdplugin-provisioner-8fdc8f955-pxmw5, openshift-storage/noobaa-core-0, openshift-storage/rook-ceph-mgr-a-78c4576db8-cjxxh, openshift-storage/rook-ceph-osd-1-bb9cdcfd6-nrdwc\n```\n\n- Once the drain is completed, you can shutdown the kubelet and the docker(or containerd) before you perform the maintenance action required.  Depending on the infrastructure running your node, the kubelet and the docker shutdown may be different.  For RHEL you can use the following command.\n  - To shutdown the kubelet.\n\n```\n    sudo systemctl stop kubelet\n```\n\n- To shutdown docker\n\n```\n    sudo systemctl stop docker\n```\n\n- To bring up docker\n\n```\n    sudo systemctl start kubelet\n```  \n\n- To bring up the kubelet\n\n```\n    sudo systemctl start kubelet\n    sudo systemctl status kubelet\n```\n\n- To view the log of the kubelet on the operating system\n\n```\n    sudo journalctl -e -u kubelet\n```\n\n- Enable Scheduling on the node using `oc adm uncordon <node-name>`.\n\n```\n$ oc adm uncordon ip-10-0-147-123.us-west-2.compute.internal\nnode/ip-10-0-147-123.us-west-2.compute.internal uncordoned\n$ oc get node ip-10-0-147-123.us-west-2.compute.internal\nNAME                                         STATUS   ROLES    AGE   VERSION\nip-10-0-147-123.us-west-2.compute.internal   Ready    worker   14m   v1.16.2\n```\n\n#### Use of cordon and uncordon during debugging\nEarlier we have seen how do you use the cordon and uncordon as part of the restart of a Node.  We can use the cordon and uncordon as a generic approach to isolate a node during troubleshooting.\n\n> During troubleshooting operation on a Node, you can use the cordon to stop the node from receiving any more pods\n\n<a name=\"node-up\"></a>\n\n## Ensure node resiliency: [ SRE ]\n\nYou have your OpenShift cluster running. Your management stack is in place. You have done the backup and prove that the backup is good by doing a restore. What is next?  How do you determine that your cluster is resilient as an OpenShift cluster should be?  Well, like backup you test it by doing a restore, with node resiliency you check it by bringing down the node down.  This action is in line with the foundation of Chaos Engineering.  \n\nIn [the Principle of Chaos](http://principlesofchaos.org/?lang=ENcontent), it defines Chaos Engineering as follow:\n> Chaos Engineering is the discipline of experimenting on a system in order to build confidence in the systemâ€™s capability to withstand turbulent conditions in production.\n\nWe recommend the following approach:\n- Become familiar with the cluster and the steps required to simulate the test.  To do this, you might start with the Development or the Test Environment.\n- Both the Master and Worker nodes need to be tested; at the beginning, they should be tested separately.\n- Once you are familiar with the process on the Test environment, then you can do it in the pre-production or even the production environment.  Make sure you plan it first.  \n- As part of the planning, you can plan to start the production test during the cluster least busy hour; when you bring down nodes, the loads need to be transferred to other operating nodes.  Based on your capacity information, can your cluster still handle the production loads without the nodes that you are bringing down?  If not, then you have a gap; refer to [the Capacity](../Capacity) chapter of this repository for more information on OpenShift capacity.\n- Another aspect of the plan is during the initial test run on production, and you might want to take the _error budget_ into consideration, i.e., do it when you still have some error budget.\n- Your goal is to be able to confidently bring down any node unplanned.  \n- Once you achieve this goal, set as a standard process to apply the Kubernetes principle of __Disposable Infrastructure__; that is, all your nodes should age no more than a set period (for example, seven days).\n\n<a name=\"co-health\"></a>\n\n## Assure cluster operator health: [ SRE ]\n\nIn OpenShift 4.x, the master and worker nodes, are managed by operators, amongst them are `kube-apiserver`, `kube-controller-manager` and `kube-scheduler` operators.  To ensure the health of the nodes then you have to ensure that you monitor these operators.  OpenShift 4.x has the Cluster Operator, `co`, CRD resources.\n\nTo quickly check the status of your Cluster Operator Status, you can run the following command.\n\n```\n$ oc get co\nNAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE\nauthentication                             4.3.0     True        False         False      17d\ncloud-credential                           4.3.0     True        False         False      17d\ncluster-autoscaler                         4.3.0     True        False         False      17d\nconsole                                    4.3.0     True        False         False      17d\ndns                                        4.3.0     True        False         False      17d\nimage-registry                             4.3.0     True        False         False      16d\ningress                                    4.3.0     True        False         False      16d\ninsights                                   4.3.0     True        False         False      17d\nkube-apiserver                             4.3.0     True        False         False      17d\nkube-controller-manager                    4.3.0     True        False         False      17d\nkube-scheduler                             4.3.0     True        False         False      17d\nmachine-api                                4.3.0     True        False         False      17d\nmachine-config                             4.3.0     True        False         False      17d\nmarketplace                                4.3.0     True        False         False      17d\nmonitoring                                 4.3.0     True        False         False      2d1h\nnetwork                                    4.3.0     True        False         False      17d\nnode-tuning                                4.3.0     True        False         False      17d\nopenshift-apiserver                        4.3.0     True        False         False      17d\nopenshift-controller-manager               4.3.0     True        False         False      17d\nopenshift-samples                          4.3.0     True        False         False      17d\noperator-lifecycle-manager                 4.3.0     True        False         False      17d\noperator-lifecycle-manager-catalog         4.3.0     True        False         False      17d\noperator-lifecycle-manager-packageserver   4.3.0     True        False         False      16d\nservice-ca                                 4.3.0     True        False         False      17d\nservice-catalog-apiserver                  4.3.0     True        False         False      17d\nservice-catalog-controller-manager         4.3.0     True        False         False      17d\nstorage                                    4.3.0     True        False         False      17d\n\n```\n\nIf you want to learn more about the specifics of the operator, then you can run the `oc describe co/<operator-name>`.\nFor example, the following command describes the _openshift-apiserver_ operator, some of the output had been cut for brevity, but notice the `Status` section, and it listed `Conditions` as well as `Related Objects.`  From this Related Objects, you can learn about all the Related Objects to the apiserver.\n\n```\n$ oc describe co/openshift-apiserver\nName:         openshift-apiserver\nNamespace:    \nLabels:       <none>\nAnnotations:  <none>\nAPI Version:  config.openshift.io/v1\nKind:         ClusterOperator\nMetadata:\n  Creation Timestamp:  2020-02-04T20:12:14Z\n  Generation:          1\n  Resource Version:    11975\n  Self Link:           /apis/config.openshift.io/v1/clusteroperators/openshift-apiserver\n  UID:                 7774b841-583b-4b82-9e88-6edd44dca3d2\nSpec:\nStatus:\n  Conditions:\n    Last Transition Time:  2020-02-04T20:16:34Z\n    Reason:                AsExpected\n    Status:                False\n    Type:                  Degraded\n    Last Transition Time:  2020-02-04T20:18:42Z\n    Reason:                AsExpected\n    Status:                False\n    Type:                  Progressing\n    Last Transition Time:  2020-02-04T20:18:29Z\n    Reason:                AsExpected\n    Status:                True\n    Type:                  Available\n    Last Transition Time:  2020-02-04T20:12:15Z\n    Reason:                AsExpected\n    Status:                True\n    Type:                  Upgradeable\n  Extension:               <nil>\n  Related Objects:\n    Group:     operator.openshift.io\n    Name:      cluster\n    Resource:  openshiftapiservers\n    Group:     \n    Name:      openshift-config\n    Resource:  namespaces\n    Group:     \n    Name:      openshift-config-managed\n    Resource:  namespaces\n    Group:     \n    Name:      openshift-apiserver-operator\n    Resource:  namespaces\n    Group:     \n    Name:      openshift-apiserver\n    Resource:  namespaces\n    Group:     apiregistration.k8s.io\n    Name:      v1.apps.openshift.io\n    Resource:  apiservices\n    . . .\n    Group:     apiregistration.k8s.io\n    Name:      v1.user.openshift.io\n    Resource:  apiservices\n  Versions:\n    Name:     operator\n    Version:  4.3.0\n    Name:     openshift-apiserver\n    Version:  \nEvents:       <none>\n```\n\n<a name=\"mco\"></a>\n\n### Machine Config Operator\nOpenShift 4.x introduces lots of Operator.  One of them is the Machine Config Operator or MCO for short.  One of the components of the Machine Config Operator is the Machine Config Daemon.\n\nThe Machine Config Daemon runs on every node to manage the configuration changes and updates on the nodes. Machine Config Daemon also provides a set of metrics. Some of the metrics are:\n- *ssh_accessed*. Shows the number of successful SSH authentications into the node.\n- *drain_time*. Shows how much time the drain took.\n\nThese metrics can be viewed through the Prometheus Cluster Monitoring stack. The chapter on [Monitoring](../Monitoring) will describe the Prometheus Monitoring in more detail.\n\nMore information on MCO can be found in the [Machine Config Operator](https://github.com/openshift/machine-config-operator) git pages.\n\n## Application tasks\n\n<a name=\"mgmt-node\"></a>\n\n## Configure node for a specialized function: [ SRE, DevOps Engineer ]\nKubernetes employs a master/worker node architecture. Kubernetes master servers maintain the information about the server cluster, and worker nodes run the actual application workloads.  In practice, you might want to introduce a specialized function node.  \n\nA good example of nodes with specialized function is a set of nodes to run the cluster management application. The main reason is to separate the application load and management load.  \n\n> If the nodes are not separated, then there is a potential disruption on the application because of the load from the management pods.  You do not want to have the application pods being restricted from scaling up because all the resources are taken by somebody running an analysis on the logs collected for the past three months, for example.\n\nThere are several methods to ensure the placement of the management function into a specific node.  \n- using [labels](https://docs.openshift.com/container-platform/4.3/nodes/pods/nodes-pods-node-selectors.html)\n- using [node affinity](https://docs.openshift.com/container-platform/4.3/nodes/scheduling/nodes-scheduler-node-affinity.html)\n- using [node taint](https://docs.openshift.com/container-platform/4.3/nodes/scheduling/nodes-scheduler-taints-tolerations.html)\n\nThese pod placement methods are covered in more detail in the [OCP day-2 Operations: on node selection](https://apps.na.collabserv.com/blogs/7be81ac0-37f8-466b-a858-2525e0b798cf/entry/OCP_day_2_operations_on_Node_Selection?lang=en_us) blog.\n\n<a name=\"mgmt-security\"></a>\n\n### Preventing security breach by using Node Restriction\nUsing a _nodeSelector_ is useful as it allows you to direct the pod to only run on a selected node.  For security reasons, it is recommended to use a label that can not be modified by the kubelet process that runs on the node.\n\nKubelet is an application that runs on the host operating system that provides the node functionality, so Kubelet is not running in a pod.  As a kubelet is an application running on the operating system, it is possible that it can be infected.  Theoretically, it is possible for an infected kubelet to change the label of the node that it is running.  By changing the node label, then it is possible that the kubelet will direct the scheduler to schedule workloads on the infected node.  \n\nTo address this, Kubernetes introduced the concept of Node restriction through the `NodeRestriction admission plugin`.  The plugin prevents kubelets from setting or modifying labels with a node-restriction `kubernetes.io/` prefix.  \n\n> Ensure to enable the NodeRestriction admission plugin\n\nEven though you are not using a NodeSelector, you should consider enabling the NodeRestriction admission plugin to ensure your node security.\n\nMore information on Node restriction can be found in the Kubernetes site on [Using Admission Controllers](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers).\n\n<a name=\"daemonsets\"></a>\n\n### Node placement and DaemonSets\nYou use a DaemonSet to ensure that all (or some) Nodes run the pod mentioned in the DaemonSet.  This is useful for implementing common functionality across the whole cluster.  Examples include Cluster storage, such as _ceph_, Log collection, such as _fluentd_, and Monitoring collectors, such as the _Prometheus Node Exporter_.  Using DaemonSet helps you ensuring that monitoring, backup, logging are done across all your nodes.\n\n[The OpenShift Documentation on DaemonSets](https://docs.openshift.com/container-platform/4.3/nodes/jobs/nodes-pods-daemonsets.html) warns on the potential conflict between daemon sets and the default Project Node Selector.  The effect will be in frequent pod recreates that put a heavy load on the cluster.\n\nIn the case of the default Project Node Selector, you can disable them by doing the following.\n\n```\n# to change the default project setting\noc patch namespace myproject -p '{\"metadata\": {\"annotations\": {\"openshift.io/node-selector\": \"\"}}}'\n\n# if you do not change the default project setting. Use the --node-selector as you create a new project.\noc adm new-project --node-selector=\"\".\n```\n\nDaemonsets should ignore the node-placement methods mentioned in this chapter.   \n\n> However, as part of your Day 2 activities, should you observe some irregularity in frequent pod recreates as you configure your pod placement policy, then you may want to check the daemon sets.  There is a possibility of configuration conflict between DaemonSets and the Node Placement policy.\n\n\n## Implementing Node Management\n\n## Kubernetes\n\nKubernetes site provides more information on [Managing Computer Resources on Containers](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/)\n\n## OpenShift\n\n- OpenShift has sections on its documentation discussing [Nodes](https://docs.openshift.com/container-platform/4.3/nodes/nodes/nodes-nodes-viewing.html) and  [Controlling pod placement using the scheduler](https://docs.openshift.com/container-platform/4.3/nodes/scheduling/nodes-scheduler-about.html)\n- [The Machine Config Operator](https://github.com/openshift/machine-config-operator)\n\n## On IBM Cloud\nTBD\n\n## With IBM Cloud Pak for MCM\n\nMCM extends the discussion presented here for multi-cluster environments, as well as creating and enforcing place rule as well as Governance rule.  More information can be found on [the IBM Cloudpak for MCM documentation](https://www.ibm.com/support/knowledgecenter/SSFC4F_1.2.0/kc_welcome_cloud_pak.html).\n\n\n## Others\n\n- [Draino](https://github.com/planetlabs/draino) Automatically cordon and drain Kubernetes nodes based on node conditions.\n\n## Other consideration\n\nn/a","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/day2/Node/index.mdx"}}}}