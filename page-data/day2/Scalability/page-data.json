{"componentChunkName":"component---src-pages-day-2-scalability-index-mdx","path":"/day2/Scalability/","result":{"pageContext":{"frontmatter":{"title":"OpenShift Platform Day2 - Scalability","description":"OpenShift Day2 Scalability","keywords":"OpenShift, day2, scalability"},"relativePagePath":"/day2/Scalability/index.mdx","titleType":"page","MdxNode":{"id":"97435d7c-6337-5f48-bad7-a2efe92625ff","children":[],"parent":"d6dcb810-8c65-5407-881f-5933e09651f5","internal":{"content":"---\ntitle: OpenShift Platform Day2 - Scalability\ndescription: OpenShift Day2 Scalability\nkeywords: 'OpenShift, day2, scalability'\n---\n\n\n## Scalability Overview\n\nOne of the major advantages of a cloud platform such as Open Shift Container Platform is the capability to scale up resources to deal with additional load and to scale down to conserve resources when there is less load.\nIn this document we will detail some of the tasks relevant to scalability in OpenShift.\n\nIn practice, make sure to test the scalability capabilities of your infrastructure and ensure that your procedures are suitable. For example, attempt to roll out additional nodes gradually and don't burst out with hundreds of new nodes simultaneously. This will needlessly strain the provisioning capabilties of your infrastructure and you may hit rate-limits set by your cloud provider.\n\nAdditionally, take into account the actual availability zones your clusters require - don't perform all your scaling in one physical location.\n\nIn this document, we will focus on the Master Nodes scalability and Worker Nodes scalability.  \n\nNote that there are two basic patterns to deploy OpenShift infrastructure:  \n- User Provisioned Infrastructure (UPI) is when the resources are provisioned externally and OpenShift uses them.\n- Installer Provisioned Infrastructure (IPI) is when the OpenShift installer programmatically creates the resources. \nThe choice between UPI and IPI is done as part of the overall OpenShift architecture during Day 0 planning.\n\n\n\n## Day 1 Platform\n\nCare must be taken during the initial design and deployment of the OpenShift cluster to allow the cluster to expand. For example, set the cluster classless inter-domain routing (CIDR) to be large enough to accept the number of nodes you expect to grow into.\n\n**Day 1 Operations tasks for Scalability:**\n  \n  - [Set the cluster network CIDR](#set-the-cluster-network-cidr)\n  - [Set the pod capacity of Nodes](#set-the-pod-capacity-of-nodes)\n\n\n\n## Day 2 Platform\n\nDuring regular Day 2 operations the cluster may scale up, based on [Capacity](../Capacity) requirements.\n\n- [Add Worker Nodes](#add-worker-nodes)\n- [Add Worker Nodes Manually](#add-worker-nodes-manually)\n- [Add Master Nodes](#add-master-nodes)\n\n\n\n## Day 1 Application\nFor the purposes of this section, scaling the application is considered to be part of application management and deployment and is not covered here, but in [Build & Deploy](../BuildDeploy).\n\n\n\n## Day 2 Application\nFor the purposes of this section, scaling the application is considered to be part of application management and deployment and is not covered here, but in [Build & Deploy](../BuildDeploy).\n\n\n\n## Mapping to Personas\n\nPersona | Task\n--- | ---\nSRE | Set the cluster network CIDR\nSRE | Set the pod capacity of Nodes\nSRE | Add Worker Nodes \nSRE | Add Worker Nodes Manually\nSRE | Add Master Nodes\n\n\n\n<a name=\"set-the-cluster-network-cidr\"></a>\n\n## Set the cluster network CIDR\nThe [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) setting defines the maximum size of the overall OpenShift network.\n\n<InlineNotification>\nThe default clusterNetwork cidr 10.128.0.0/14 cannot be used if the cluster size is more than 500 nodes. It must be set to 10.128.0.0/12 or 10.128.0.0/10 to get to larger node counts beyond 500 nodes.\n</InlineNotification>\n\nMore information can be found in the [OpenShift 4.3 documentation](https://docs.openshift.com/container-platform/4.3/scalability_and_performance/recommended-install-practices.html).\n\n\n<a name=\"set-the-pod-capacity-of-nodes\"></a>\n\n## Set the pod capacity of Nodes\n\n<InlineNotification>\nThe OpenShift Container Platform node configuration file contains important options. For example, two parameters control the maximum number of pods that can be scheduled to a node: podsPerCore and maxPods.\n</InlineNotification>\n\nIn Kubernetes, a pod that is holding a single container actually uses two containers. The second container is used to set up networking prior to the actual container starting. Therefore, a system running 10 pods will actually have 20 containers running.\n\n`maxPods` sets the number of pods the node can run to a fixed value, regardless of the properties of the node.\n\n`podsPerCore` sets the number of pods the node can run based on the number of processor cores on the node.\n\nSetting podsPerCore to 0 disables this limit. The default is 0. podsPerCore cannot exceed maxPods.\n\nWhen both options are in use, the lower of the two values limits the number of pods on a node. \n\nMore information about configuring these settings can be found in the [OpenShift 4.3 documentation](https://docs.openshift.com/container-platform/4.3/scalability_and_performance/recommended-host-practices.html#recommended-node-host-practices_).\n\n\n\n<a name=\"add-worker-nodes\"></a>\n\n## Add Worker Nodes\nTo manage high volume workload with your applications, the OpenShift can be scaled by adding more worker nodes manually or by the AutoScaler.  There are two ways to add worker nodes; 1). adding worker nodes manually or 2). adding worker node by AutoScaler. We will talk about those ways. You also would like to understand that there will be different ways to add worker node manually. It depends on how you create you cluster environment. We will discuss several cases such as a). Cluster in IPI (AWS, Azure, GCP), b). Cluster in UPI (VMware and Bere Metal), and c). Cluster in IBM Cloud.  \n\n\n\n<a name=\"add-worker-nodes-manually\"></a>\n\n### Add Worker Nodes manually  \nWe will start discussing how to add worker nodes manually.  As mentioned, the steps will be different depends on the cluster environment.  We will cover 3 scenarios such as AWS for IPI, VMware for UPI, and IBM Cloud.  \n\n\n\n### Add Worker Nodes in IPI (AWS, Azure)\nTo add worker nodes in IPI, you would need to know about the resources **Machine** and **MachineSet**. It is based on the Kubernets Cluster API.  You can find the information about the Cluster API in the following URL.\n\nKubernetes Cluster API:  \nhttps://github.com/kubernetes-sigs/cluster-api   \n\n\n\n### The Machine resource\nThe Machine is the resource which describes the status of nodes.\nWhen you deploy your cluster in IPI, the Installer automatically creates the Machine resource for the Master and Worker nodes.\n\nRun the `oc get machines` command to obtain the Machine resource information. You need to be a user which has cluster admin permission.\n \n```\n$ oc get machines -n openshift-machine-api\nNAME                                     STATE     TYPE        REGION      ZONE         AGE\ncsmo1aws-fsdjv-master-0                  running   m4.xlarge   us-west-1   us-west-1b   43h\ncsmo1aws-fsdjv-master-1                  running   m4.xlarge   us-west-1   us-west-1c   43h\ncsmo1aws-fsdjv-master-2                  running   m4.xlarge   us-west-1   us-west-1b   43h\ncsmo1aws-fsdjv-worker-us-west-1b-mkdqf   running   m4.large    us-west-1   us-west-1b   43h\ncsmo1aws-fsdjv-worker-us-west-1b-p24nk   running   m4.large    us-west-1   us-west-1b   43h\ncsmo1aws-fsdjv-worker-us-west-1c-pm2bj   running   m4.large    us-west-1   us-west-1c   43h\n$ \n```\n\nThe Machine is defined as the resource in the Machine API project called **openshift-machine-api**.\nAs you see in the command output above, the Machines are created per nodes (both Master and Worker).\n\nThere are a several options to display the output of **oc get machines** command. \n\n```\n$ oc get machines -n openshift-machine-api -o jsonpath='{range .items[*]}{\"\\n\"}{.metadata.name}{\"\\t\"}{.spec.providerSpec.value.instanceType}{end}{\"\\n\"}'\n\ncsmo1aws-fsdjv-master-0\tm4.xlarge\ncsmo1aws-fsdjv-master-1\tm4.xlarge\ncsmo1aws-fsdjv-master-2\tm4.xlarge\ncsmo1aws-fsdjv-worker-us-west-1b-mkdqf\tm4.large\ncsmo1aws-fsdjv-worker-us-west-1b-p24nk\tm4.large\ncsmo1aws-fsdjv-worker-us-west-1c-pm2bj\tm4.large\n$ \n```\nFor your reference, further examples are available [here](./scalability_command_examples.md#oc-get-machines).\n\nYou can specify **-o yaml** option with the oc get machine command, you can find the meta date such as Machine name and Label as well as the spec information.\n\n```\n$ oc get machine/<machine_name> -n openshift-machine-api -o yaml\n```\n\n\n\n### The Machine Set resource\nThe MachineSet is the resource which makes a group of the Machine resource in the **openshift-machine-api** project. The MachineSet is also created by the Installer automatically.\nThe cluster administrator can add or remove the Machine by increasing or decresing the number of replicas of the MachineSet.\nYou can find out the Machine Set information by issuing the oc get machineset command as follow.\n\n```\n$ oc get machinesets -n openshift-machine-api\nNAME                               DESIRED   CURRENT   READY   AVAILABLE   AGE\ncsmo1aws-fsdjv-worker-us-west-1b   2         2         2       2           2d12h\ncsmo1aws-fsdjv-worker-us-west-1c   1         1         1       1           2d12h\n$\n```\n\nYou can also get the MachineSet information as the YAML format with the **-o yaml** option for the oc get machineset command.\n```\n$ oc get machineset/<machineset_name> -n openshift-machine-api -o yaml\n```\n\nYou can get detailed information of the machinesets by executing **oc describe machinesets** command. You can find some examples of the use of [oc commands](./scalability_command_examples.md#oc-describe-machine).\n\n\nThe MachineSet for the Machine of Worker nodes is created automatically in the **Availability Zone** which you specify during the cluster installation.\n\nYou can find out on which Availability Zone is used if you see the **providerSpec**.  You may need to add the new MachineSet if a new Region or new Availability Zone is added in your Cloud provider.  \n\n\n\n\n### Adding / Removing Machine\n\nTo add the Machine to a specific Availability Zone, you can run the **oc scale** command.  It will increase the number of replica of MachineSet. The following is an example to add two worker nodes.\n\n```\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1c --replicas=2 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1c scaled\n$\n```\n\nBy the command above, the number of replica of the MachineSet will become 2.  The worker nodes and Machine in the availability zone in the MachineSet will be 2 as well.\n\nIf you removed applications/services or decrease workload for some reason, you may want to reduce number of worker nodes.  If that's the case, then you can run oc scale command and specify appropriate number of replicas as follow. \n\n```\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1b --replicas=1 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1b scaled\n$\n```\n\nIn this case, the number of replica will be 1.\n\nAfter you execute the command, you can verify the result with the commands such as **oc get machines** and/or **oc get nodes**.  \n\nIn our example above, the number of workers on us-west-1b zone is 1 and the number of workers on us-west-1c zone is 2.\n\n```\n$ oc get machines -n openshift-machine-api\nNAME                                     STATE     TYPE        REGION      ZONE         AGE\ncsmo1aws-fsdjv-master-0                  running   m4.xlarge   us-west-1   us-west-1b   2d13h\ncsmo1aws-fsdjv-master-1                  running   m4.xlarge   us-west-1   us-west-1c   2d13h\ncsmo1aws-fsdjv-master-2                  running   m4.xlarge   us-west-1   us-west-1b   2d13h\ncsmo1aws-fsdjv-worker-us-west-1b-gx8d2   running   m4.xlarge   us-west-1   us-west-1b   4m33s\ncsmo1aws-fsdjv-worker-us-west-1c-5sr2w   running   m4.xlarge   us-west-1   us-west-1c   17m\ncsmo1aws-fsdjv-worker-us-west-1c-x8gxr   running   m4.xlarge   us-west-1   us-west-1c   6m38s\n$\n```\n\n```\n$ oc get nodes\nNAME                                         STATUS   ROLES    AGE     VERSION\nip-10-0-129-227.us-west-1.compute.internal   Ready    master   2d13h   v1.14.6+cebabbf4a\nip-10-0-136-11.us-west-1.compute.internal    Ready    worker   57s     v1.14.6+cebabbf4a\nip-10-0-139-19.us-west-1.compute.internal    Ready    master   2d13h   v1.14.6+cebabbf4a\nip-10-0-146-131.us-west-1.compute.internal   Ready    worker   3m8s    v1.14.6+cebabbf4a\nip-10-0-146-194.us-west-1.compute.internal   Ready    worker   13m     v1.14.6+cebabbf4a\nip-10-0-151-108.us-west-1.compute.internal   Ready    master   2d13h   v1.14.6+cebabbf4a\n$ \n```  \n\n\n\n\n### Adding / Removing worker nodes via Web Console\nYou can also add/remove worker nodes via Web Console.\n1. Login with the cluster admin permission\n2. Go to Compute --> Machine Sets\n![scalability_aws_1](./images/scalability_aws_1.png)\n\n3. Select MachineSet on the list and click it\n4. Click on X machine under the **Desired Count** on the **Overview** tab in the **Machine Set Details** page\n![scalability_aws_2](./images/scalability_aws_2.png)\n\n5. On the Edit Count window, type the replica number which you want to have and click on Save button\n![scalability_aws_3](./images/scalability_aws_3.png)\nIn our example, we updated the replica number from 1 to 2 for the csmo1aws-fsdjv-worker-us-west-1c Machine Set.\n\n6. Once it's done, then the worker nodes will be added/removed.  \n   ![scalability_aws_5](./images/scalability_aws_5.png)\n   As you see above, the Desired Count becomes 2 in our case.  \n   You can also find out that there is a new node created in the us-west-1c zone.\n   ![scalability_aws_6](./images/scalability_aws_6.png)   \n\n\n\n\n\n## Add Worker Nodes in UPI (VMware, Bare Metal)\nThe worker nodes can be added using the same steps as the control plane nodes. \n\nCreate a new VM with the RHCOS installer as normal, using the **worker.ign** file generated during the original install. During the machine's start up, it will sync with the existing OpenShift cluster's **Machine Config Operator** to be admitted into the cluster.\n\nNote: it may be required to manually approve the node's CSR (Certificate Signing Request) as described in the installation documentation of vSphere or Baremetal.   \n\n\n\n\n\n## Add Worker Nodes in IBM Cloud (a.k.a. ROKS)\nWith IBM Cloud, adding worker nodes in the OpenShift cluster will be done by IBM Cloud Console or by using the IBM Cloud CLI with commands such as \n  `ibmcloud ks worker-pool resize --cluster <cluster_name_or_ID> --worker-pool <pool_name>  --size-per-zone <number_of_workers_per_zone>`\nMore information can be found in the [ROKS documentation](https://cloud.ibm.com/docs/containers?topic=containers-add_workers).\n\n\n\n\n## Add Worker Nodes by AutoScaler  \nIn the previous section, we discussed how to add worker nodes manually.  In this section, we will show you how to add worker nodes by the AutoScaler. With the Auto Scaling capability in OpenShift, the worker nodes will be added / removed automatically based on the application deployment status. You need to understand two more resources such as **ClusterAutoscaler** and **MachineAutoscaler** in addition to Machine and MachineSet resources. \n\n\n\n\n### Create ClusterAutoscaler\nThe ClusterAutoscaler is a resource for automatically adjusting the size of OpenShift cluster. In the ClusterAutoscaler resource, it describes the maximum number of nodes, the possibility of scaling down, and the minimum and maximum values of CPU, memory, and GPU that can be used by the cluster. When it does the auto scaling (scale up), the upper limit defined by the ClusterAutoscaler will not be exceeded. Note that the ClusterAutoscaler is set for the OpenShift cluster wide, so that only one ClusterAutoscaler can be created per cluster. It is not tied to a specific project. The ClusterAutoscaler is managed by the Operator.  \n\nThe following example creates a YAML file that describes a resource named \"ca-sample\" and creates a ClusterAutoscaler resource with the oc create command. In this sample resource, the upper limit of the total number of nodes in the cluster is 10 and auto scale down is enabled.\n\n```\n$ cat << EOF  > cluster-autoscaler-sample.yaml\napiVersion: \"autoscaling.openshift.io/v1\"\nkind: \"ClusterAutoscaler\"\nmetadata:\n  name: \"ca-sample\"          ## Name of the ClusterAutoscaler resource\nspec:  \n  resourceLimits:    \n    maxNodesTotal: 10        ## Max number of nodes in the cluser  \n  scaleDown:    \n    enabled: true            ## Enable Scale Down\nEOF\n\n$ oc create -f cluster-autoscaler-sample.yaml\n```   \n\n\n\n### Create MachineAutoscaler\nThe MachineAutoscaler is a resource for automatically adjusting the number of Machines in the MachineSet. The number of Machines is adjusted so as not to exceed the upper limit defined by the ClusterAutoscaler. The MachineAutoscaler is required to determine which MachineSet the ClusterAutoscaler will adjust the number of replicas for.  \n\nThe following example creates a MachineAutoscaler resource named ma-sample01. The number of replicas is automatically adjusted within the range of 1 to 5 for the specified MachineSet. The MachineAutoscaler is created in the openshift-machine-api project as well as the Machine and the MachineSet. You can check the created the ClusterAutoscaler and the MachineAutoscaler information with the oc get command.\n\n```\n$ cat << EOF  > machine-autoscaler-sample01.yaml\napiVersion: \"autoscaling.openshift.io/v1beta1\"\nkind: \"MachineAutoscaler\"\nmetadata:\n  name: \"ma-sample01\"\n  namespace: \"openshift-machine-api\"\nspec:\n  minReplicas: 1                       ## Min replica number of MachineSet\n  maxReplicas: 5                       ## Max replica number of MachineSet\n  scaleTargetRef:    \n    apiVersion: machine.openshift.io/v1beta1\n    kind: MachineSet\n    name: <clusterID>-ap-northeast-1a   ## Specify an existing MachineSet name\nEOF\n$ oc create -f machine-autoscaler-sample01.yaml\n```  \n\nNote that the AutoScale function by ClusterAutoscaler will not work unless the following two conditions are met.\n- MachineAutoscaler is set for all MachineSets\n- The number of replicas of all MachineSets is set to 1 or more and one or more Machines are operating.  \n\n\nCheck to see if it works. Create a job like the following, and start a large number of containers at once that only execute the sleep command.\n\n```\n$ cat << EOF  > job-work-queue-sample.yaml\napiVersion: batch/v1\nkind: Jobmetadata:\n  generateName: work-queue-  \n  namespace: autoscale-demo01\nspec:  \n  template:    \n    spec:      \n      containers:      \n      - name: work        \n        image: busybox        \n        command: [\"sleep\",  \"300\"]        \n        resources:          \n          requests:            \n            memory: 500Mi            \n            cpu: 500m        \n        restartPolicy: Never  \n  completions: 20  \n  parallelism: 20\nEOF\n\n$ oc new-project autoscale-demo01; oc project autoscale-demo01\n$ oc create -f job-work-queue-sample.yaml\n```\n\nAssuming that your OpenShift cluster has the default configuration. With the above example, the resource is not enough to deploy pods at once.  Therefore, after a while, the worker node will be added automatically.  The pod status will be changed from **Pending** to **Running**.  Then the pod which was on hold will be deployed.\nYou can run **oc get nodes** command and/or oc get pods command to see those behavior.  You can also verify that the worker node will be delete automatically if you delete a newly created pod by issuing **oc delete project** which will also delete the project.\n\nNote that there are a few cases which the ClusterAutoscaler doesn't remove worker nodes.  For example, if the pod is using the local storage on the worker node, the worker node won't be removed by the ClusterAutoscaler.  Another example is that the pod which won't be move to other worker node due to the cluster resource shortage, then the worker will not be removed by the ClusterAutoscaler.  \n\n\n\n<a name=\"add-master-nodes\"></a>\n\n## Add Master Nodes\nThere should be at least 3 Master nodes deployed with OpenShift 4.x.  If you wish to add new masters due to load (on etcd, for example) then the procedure for adding a new master is the same as adding a regular worker node, except that the node must have the label `machineconfiguration.openshift.io/role: infra` instead of `worker`.\n\n\n\n\n","type":"Mdx","contentDigest":"082c7401e9b02489d02be96b8d6b4da3","counter":547,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"OpenShift Platform Day2 - Scalability","description":"OpenShift Day2 Scalability","keywords":"OpenShift, day2, scalability"},"exports":{},"rawBody":"---\ntitle: OpenShift Platform Day2 - Scalability\ndescription: OpenShift Day2 Scalability\nkeywords: 'OpenShift, day2, scalability'\n---\n\n\n## Scalability Overview\n\nOne of the major advantages of a cloud platform such as Open Shift Container Platform is the capability to scale up resources to deal with additional load and to scale down to conserve resources when there is less load.\nIn this document we will detail some of the tasks relevant to scalability in OpenShift.\n\nIn practice, make sure to test the scalability capabilities of your infrastructure and ensure that your procedures are suitable. For example, attempt to roll out additional nodes gradually and don't burst out with hundreds of new nodes simultaneously. This will needlessly strain the provisioning capabilties of your infrastructure and you may hit rate-limits set by your cloud provider.\n\nAdditionally, take into account the actual availability zones your clusters require - don't perform all your scaling in one physical location.\n\nIn this document, we will focus on the Master Nodes scalability and Worker Nodes scalability.  \n\nNote that there are two basic patterns to deploy OpenShift infrastructure:  \n- User Provisioned Infrastructure (UPI) is when the resources are provisioned externally and OpenShift uses them.\n- Installer Provisioned Infrastructure (IPI) is when the OpenShift installer programmatically creates the resources. \nThe choice between UPI and IPI is done as part of the overall OpenShift architecture during Day 0 planning.\n\n\n\n## Day 1 Platform\n\nCare must be taken during the initial design and deployment of the OpenShift cluster to allow the cluster to expand. For example, set the cluster classless inter-domain routing (CIDR) to be large enough to accept the number of nodes you expect to grow into.\n\n**Day 1 Operations tasks for Scalability:**\n  \n  - [Set the cluster network CIDR](#set-the-cluster-network-cidr)\n  - [Set the pod capacity of Nodes](#set-the-pod-capacity-of-nodes)\n\n\n\n## Day 2 Platform\n\nDuring regular Day 2 operations the cluster may scale up, based on [Capacity](../Capacity) requirements.\n\n- [Add Worker Nodes](#add-worker-nodes)\n- [Add Worker Nodes Manually](#add-worker-nodes-manually)\n- [Add Master Nodes](#add-master-nodes)\n\n\n\n## Day 1 Application\nFor the purposes of this section, scaling the application is considered to be part of application management and deployment and is not covered here, but in [Build & Deploy](../BuildDeploy).\n\n\n\n## Day 2 Application\nFor the purposes of this section, scaling the application is considered to be part of application management and deployment and is not covered here, but in [Build & Deploy](../BuildDeploy).\n\n\n\n## Mapping to Personas\n\nPersona | Task\n--- | ---\nSRE | Set the cluster network CIDR\nSRE | Set the pod capacity of Nodes\nSRE | Add Worker Nodes \nSRE | Add Worker Nodes Manually\nSRE | Add Master Nodes\n\n\n\n<a name=\"set-the-cluster-network-cidr\"></a>\n\n## Set the cluster network CIDR\nThe [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) setting defines the maximum size of the overall OpenShift network.\n\n<InlineNotification>\nThe default clusterNetwork cidr 10.128.0.0/14 cannot be used if the cluster size is more than 500 nodes. It must be set to 10.128.0.0/12 or 10.128.0.0/10 to get to larger node counts beyond 500 nodes.\n</InlineNotification>\n\nMore information can be found in the [OpenShift 4.3 documentation](https://docs.openshift.com/container-platform/4.3/scalability_and_performance/recommended-install-practices.html).\n\n\n<a name=\"set-the-pod-capacity-of-nodes\"></a>\n\n## Set the pod capacity of Nodes\n\n<InlineNotification>\nThe OpenShift Container Platform node configuration file contains important options. For example, two parameters control the maximum number of pods that can be scheduled to a node: podsPerCore and maxPods.\n</InlineNotification>\n\nIn Kubernetes, a pod that is holding a single container actually uses two containers. The second container is used to set up networking prior to the actual container starting. Therefore, a system running 10 pods will actually have 20 containers running.\n\n`maxPods` sets the number of pods the node can run to a fixed value, regardless of the properties of the node.\n\n`podsPerCore` sets the number of pods the node can run based on the number of processor cores on the node.\n\nSetting podsPerCore to 0 disables this limit. The default is 0. podsPerCore cannot exceed maxPods.\n\nWhen both options are in use, the lower of the two values limits the number of pods on a node. \n\nMore information about configuring these settings can be found in the [OpenShift 4.3 documentation](https://docs.openshift.com/container-platform/4.3/scalability_and_performance/recommended-host-practices.html#recommended-node-host-practices_).\n\n\n\n<a name=\"add-worker-nodes\"></a>\n\n## Add Worker Nodes\nTo manage high volume workload with your applications, the OpenShift can be scaled by adding more worker nodes manually or by the AutoScaler.  There are two ways to add worker nodes; 1). adding worker nodes manually or 2). adding worker node by AutoScaler. We will talk about those ways. You also would like to understand that there will be different ways to add worker node manually. It depends on how you create you cluster environment. We will discuss several cases such as a). Cluster in IPI (AWS, Azure, GCP), b). Cluster in UPI (VMware and Bere Metal), and c). Cluster in IBM Cloud.  \n\n\n\n<a name=\"add-worker-nodes-manually\"></a>\n\n### Add Worker Nodes manually  \nWe will start discussing how to add worker nodes manually.  As mentioned, the steps will be different depends on the cluster environment.  We will cover 3 scenarios such as AWS for IPI, VMware for UPI, and IBM Cloud.  \n\n\n\n### Add Worker Nodes in IPI (AWS, Azure)\nTo add worker nodes in IPI, you would need to know about the resources **Machine** and **MachineSet**. It is based on the Kubernets Cluster API.  You can find the information about the Cluster API in the following URL.\n\nKubernetes Cluster API:  \nhttps://github.com/kubernetes-sigs/cluster-api   \n\n\n\n### The Machine resource\nThe Machine is the resource which describes the status of nodes.\nWhen you deploy your cluster in IPI, the Installer automatically creates the Machine resource for the Master and Worker nodes.\n\nRun the `oc get machines` command to obtain the Machine resource information. You need to be a user which has cluster admin permission.\n \n```\n$ oc get machines -n openshift-machine-api\nNAME                                     STATE     TYPE        REGION      ZONE         AGE\ncsmo1aws-fsdjv-master-0                  running   m4.xlarge   us-west-1   us-west-1b   43h\ncsmo1aws-fsdjv-master-1                  running   m4.xlarge   us-west-1   us-west-1c   43h\ncsmo1aws-fsdjv-master-2                  running   m4.xlarge   us-west-1   us-west-1b   43h\ncsmo1aws-fsdjv-worker-us-west-1b-mkdqf   running   m4.large    us-west-1   us-west-1b   43h\ncsmo1aws-fsdjv-worker-us-west-1b-p24nk   running   m4.large    us-west-1   us-west-1b   43h\ncsmo1aws-fsdjv-worker-us-west-1c-pm2bj   running   m4.large    us-west-1   us-west-1c   43h\n$ \n```\n\nThe Machine is defined as the resource in the Machine API project called **openshift-machine-api**.\nAs you see in the command output above, the Machines are created per nodes (both Master and Worker).\n\nThere are a several options to display the output of **oc get machines** command. \n\n```\n$ oc get machines -n openshift-machine-api -o jsonpath='{range .items[*]}{\"\\n\"}{.metadata.name}{\"\\t\"}{.spec.providerSpec.value.instanceType}{end}{\"\\n\"}'\n\ncsmo1aws-fsdjv-master-0\tm4.xlarge\ncsmo1aws-fsdjv-master-1\tm4.xlarge\ncsmo1aws-fsdjv-master-2\tm4.xlarge\ncsmo1aws-fsdjv-worker-us-west-1b-mkdqf\tm4.large\ncsmo1aws-fsdjv-worker-us-west-1b-p24nk\tm4.large\ncsmo1aws-fsdjv-worker-us-west-1c-pm2bj\tm4.large\n$ \n```\nFor your reference, further examples are available [here](./scalability_command_examples.md#oc-get-machines).\n\nYou can specify **-o yaml** option with the oc get machine command, you can find the meta date such as Machine name and Label as well as the spec information.\n\n```\n$ oc get machine/<machine_name> -n openshift-machine-api -o yaml\n```\n\n\n\n### The Machine Set resource\nThe MachineSet is the resource which makes a group of the Machine resource in the **openshift-machine-api** project. The MachineSet is also created by the Installer automatically.\nThe cluster administrator can add or remove the Machine by increasing or decresing the number of replicas of the MachineSet.\nYou can find out the Machine Set information by issuing the oc get machineset command as follow.\n\n```\n$ oc get machinesets -n openshift-machine-api\nNAME                               DESIRED   CURRENT   READY   AVAILABLE   AGE\ncsmo1aws-fsdjv-worker-us-west-1b   2         2         2       2           2d12h\ncsmo1aws-fsdjv-worker-us-west-1c   1         1         1       1           2d12h\n$\n```\n\nYou can also get the MachineSet information as the YAML format with the **-o yaml** option for the oc get machineset command.\n```\n$ oc get machineset/<machineset_name> -n openshift-machine-api -o yaml\n```\n\nYou can get detailed information of the machinesets by executing **oc describe machinesets** command. You can find some examples of the use of [oc commands](./scalability_command_examples.md#oc-describe-machine).\n\n\nThe MachineSet for the Machine of Worker nodes is created automatically in the **Availability Zone** which you specify during the cluster installation.\n\nYou can find out on which Availability Zone is used if you see the **providerSpec**.  You may need to add the new MachineSet if a new Region or new Availability Zone is added in your Cloud provider.  \n\n\n\n\n### Adding / Removing Machine\n\nTo add the Machine to a specific Availability Zone, you can run the **oc scale** command.  It will increase the number of replica of MachineSet. The following is an example to add two worker nodes.\n\n```\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1c --replicas=2 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1c scaled\n$\n```\n\nBy the command above, the number of replica of the MachineSet will become 2.  The worker nodes and Machine in the availability zone in the MachineSet will be 2 as well.\n\nIf you removed applications/services or decrease workload for some reason, you may want to reduce number of worker nodes.  If that's the case, then you can run oc scale command and specify appropriate number of replicas as follow. \n\n```\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1b --replicas=1 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1b scaled\n$\n```\n\nIn this case, the number of replica will be 1.\n\nAfter you execute the command, you can verify the result with the commands such as **oc get machines** and/or **oc get nodes**.  \n\nIn our example above, the number of workers on us-west-1b zone is 1 and the number of workers on us-west-1c zone is 2.\n\n```\n$ oc get machines -n openshift-machine-api\nNAME                                     STATE     TYPE        REGION      ZONE         AGE\ncsmo1aws-fsdjv-master-0                  running   m4.xlarge   us-west-1   us-west-1b   2d13h\ncsmo1aws-fsdjv-master-1                  running   m4.xlarge   us-west-1   us-west-1c   2d13h\ncsmo1aws-fsdjv-master-2                  running   m4.xlarge   us-west-1   us-west-1b   2d13h\ncsmo1aws-fsdjv-worker-us-west-1b-gx8d2   running   m4.xlarge   us-west-1   us-west-1b   4m33s\ncsmo1aws-fsdjv-worker-us-west-1c-5sr2w   running   m4.xlarge   us-west-1   us-west-1c   17m\ncsmo1aws-fsdjv-worker-us-west-1c-x8gxr   running   m4.xlarge   us-west-1   us-west-1c   6m38s\n$\n```\n\n```\n$ oc get nodes\nNAME                                         STATUS   ROLES    AGE     VERSION\nip-10-0-129-227.us-west-1.compute.internal   Ready    master   2d13h   v1.14.6+cebabbf4a\nip-10-0-136-11.us-west-1.compute.internal    Ready    worker   57s     v1.14.6+cebabbf4a\nip-10-0-139-19.us-west-1.compute.internal    Ready    master   2d13h   v1.14.6+cebabbf4a\nip-10-0-146-131.us-west-1.compute.internal   Ready    worker   3m8s    v1.14.6+cebabbf4a\nip-10-0-146-194.us-west-1.compute.internal   Ready    worker   13m     v1.14.6+cebabbf4a\nip-10-0-151-108.us-west-1.compute.internal   Ready    master   2d13h   v1.14.6+cebabbf4a\n$ \n```  \n\n\n\n\n### Adding / Removing worker nodes via Web Console\nYou can also add/remove worker nodes via Web Console.\n1. Login with the cluster admin permission\n2. Go to Compute --> Machine Sets\n![scalability_aws_1](./images/scalability_aws_1.png)\n\n3. Select MachineSet on the list and click it\n4. Click on X machine under the **Desired Count** on the **Overview** tab in the **Machine Set Details** page\n![scalability_aws_2](./images/scalability_aws_2.png)\n\n5. On the Edit Count window, type the replica number which you want to have and click on Save button\n![scalability_aws_3](./images/scalability_aws_3.png)\nIn our example, we updated the replica number from 1 to 2 for the csmo1aws-fsdjv-worker-us-west-1c Machine Set.\n\n6. Once it's done, then the worker nodes will be added/removed.  \n   ![scalability_aws_5](./images/scalability_aws_5.png)\n   As you see above, the Desired Count becomes 2 in our case.  \n   You can also find out that there is a new node created in the us-west-1c zone.\n   ![scalability_aws_6](./images/scalability_aws_6.png)   \n\n\n\n\n\n## Add Worker Nodes in UPI (VMware, Bare Metal)\nThe worker nodes can be added using the same steps as the control plane nodes. \n\nCreate a new VM with the RHCOS installer as normal, using the **worker.ign** file generated during the original install. During the machine's start up, it will sync with the existing OpenShift cluster's **Machine Config Operator** to be admitted into the cluster.\n\nNote: it may be required to manually approve the node's CSR (Certificate Signing Request) as described in the installation documentation of vSphere or Baremetal.   \n\n\n\n\n\n## Add Worker Nodes in IBM Cloud (a.k.a. ROKS)\nWith IBM Cloud, adding worker nodes in the OpenShift cluster will be done by IBM Cloud Console or by using the IBM Cloud CLI with commands such as \n  `ibmcloud ks worker-pool resize --cluster <cluster_name_or_ID> --worker-pool <pool_name>  --size-per-zone <number_of_workers_per_zone>`\nMore information can be found in the [ROKS documentation](https://cloud.ibm.com/docs/containers?topic=containers-add_workers).\n\n\n\n\n## Add Worker Nodes by AutoScaler  \nIn the previous section, we discussed how to add worker nodes manually.  In this section, we will show you how to add worker nodes by the AutoScaler. With the Auto Scaling capability in OpenShift, the worker nodes will be added / removed automatically based on the application deployment status. You need to understand two more resources such as **ClusterAutoscaler** and **MachineAutoscaler** in addition to Machine and MachineSet resources. \n\n\n\n\n### Create ClusterAutoscaler\nThe ClusterAutoscaler is a resource for automatically adjusting the size of OpenShift cluster. In the ClusterAutoscaler resource, it describes the maximum number of nodes, the possibility of scaling down, and the minimum and maximum values of CPU, memory, and GPU that can be used by the cluster. When it does the auto scaling (scale up), the upper limit defined by the ClusterAutoscaler will not be exceeded. Note that the ClusterAutoscaler is set for the OpenShift cluster wide, so that only one ClusterAutoscaler can be created per cluster. It is not tied to a specific project. The ClusterAutoscaler is managed by the Operator.  \n\nThe following example creates a YAML file that describes a resource named \"ca-sample\" and creates a ClusterAutoscaler resource with the oc create command. In this sample resource, the upper limit of the total number of nodes in the cluster is 10 and auto scale down is enabled.\n\n```\n$ cat << EOF  > cluster-autoscaler-sample.yaml\napiVersion: \"autoscaling.openshift.io/v1\"\nkind: \"ClusterAutoscaler\"\nmetadata:\n  name: \"ca-sample\"          ## Name of the ClusterAutoscaler resource\nspec:  \n  resourceLimits:    \n    maxNodesTotal: 10        ## Max number of nodes in the cluser  \n  scaleDown:    \n    enabled: true            ## Enable Scale Down\nEOF\n\n$ oc create -f cluster-autoscaler-sample.yaml\n```   \n\n\n\n### Create MachineAutoscaler\nThe MachineAutoscaler is a resource for automatically adjusting the number of Machines in the MachineSet. The number of Machines is adjusted so as not to exceed the upper limit defined by the ClusterAutoscaler. The MachineAutoscaler is required to determine which MachineSet the ClusterAutoscaler will adjust the number of replicas for.  \n\nThe following example creates a MachineAutoscaler resource named ma-sample01. The number of replicas is automatically adjusted within the range of 1 to 5 for the specified MachineSet. The MachineAutoscaler is created in the openshift-machine-api project as well as the Machine and the MachineSet. You can check the created the ClusterAutoscaler and the MachineAutoscaler information with the oc get command.\n\n```\n$ cat << EOF  > machine-autoscaler-sample01.yaml\napiVersion: \"autoscaling.openshift.io/v1beta1\"\nkind: \"MachineAutoscaler\"\nmetadata:\n  name: \"ma-sample01\"\n  namespace: \"openshift-machine-api\"\nspec:\n  minReplicas: 1                       ## Min replica number of MachineSet\n  maxReplicas: 5                       ## Max replica number of MachineSet\n  scaleTargetRef:    \n    apiVersion: machine.openshift.io/v1beta1\n    kind: MachineSet\n    name: <clusterID>-ap-northeast-1a   ## Specify an existing MachineSet name\nEOF\n$ oc create -f machine-autoscaler-sample01.yaml\n```  \n\nNote that the AutoScale function by ClusterAutoscaler will not work unless the following two conditions are met.\n- MachineAutoscaler is set for all MachineSets\n- The number of replicas of all MachineSets is set to 1 or more and one or more Machines are operating.  \n\n\nCheck to see if it works. Create a job like the following, and start a large number of containers at once that only execute the sleep command.\n\n```\n$ cat << EOF  > job-work-queue-sample.yaml\napiVersion: batch/v1\nkind: Jobmetadata:\n  generateName: work-queue-  \n  namespace: autoscale-demo01\nspec:  \n  template:    \n    spec:      \n      containers:      \n      - name: work        \n        image: busybox        \n        command: [\"sleep\",  \"300\"]        \n        resources:          \n          requests:            \n            memory: 500Mi            \n            cpu: 500m        \n        restartPolicy: Never  \n  completions: 20  \n  parallelism: 20\nEOF\n\n$ oc new-project autoscale-demo01; oc project autoscale-demo01\n$ oc create -f job-work-queue-sample.yaml\n```\n\nAssuming that your OpenShift cluster has the default configuration. With the above example, the resource is not enough to deploy pods at once.  Therefore, after a while, the worker node will be added automatically.  The pod status will be changed from **Pending** to **Running**.  Then the pod which was on hold will be deployed.\nYou can run **oc get nodes** command and/or oc get pods command to see those behavior.  You can also verify that the worker node will be delete automatically if you delete a newly created pod by issuing **oc delete project** which will also delete the project.\n\nNote that there are a few cases which the ClusterAutoscaler doesn't remove worker nodes.  For example, if the pod is using the local storage on the worker node, the worker node won't be removed by the ClusterAutoscaler.  Another example is that the pod which won't be move to other worker node due to the cluster resource shortage, then the worker will not be removed by the ClusterAutoscaler.  \n\n\n\n<a name=\"add-master-nodes\"></a>\n\n## Add Master Nodes\nThere should be at least 3 Master nodes deployed with OpenShift 4.x.  If you wish to add new masters due to load (on etcd, for example) then the procedure for adding a new master is the same as adding a regular worker node, except that the node must have the label `machineconfiguration.openshift.io/role: infra` instead of `worker`.\n\n\n\n\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/day2/Scalability/index.mdx"}}}}