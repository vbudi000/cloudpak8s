{"componentChunkName":"component---src-pages-day-2-backups-index-mdx","path":"/day2/Backups/","result":{"pageContext":{"frontmatter":{"title":"OpenShift Platform Day2 - Backup","description":"OpenShift Day2 Backup","keywords":"OpenShift, day2, backup"},"relativePagePath":"/day2/Backups/index.mdx","titleType":"page","MdxNode":{"id":"98b6045e-e9a3-57fa-97b9-64bd8e00d1ec","children":[],"parent":"1b248f8a-0a7e-51c3-b764-ff0e75ad6169","internal":{"content":"---\ntitle: OpenShift Platform Day2 - Backup\ndescription: OpenShift Day2 Backup\nkeywords: 'OpenShift, day2, backup'\n---\n\n## Backups Overview\nAll Kubernetes resources are described by data stored in a reliable database which is etcd. In the event of some Pods being  deleted, their supervising Deployment will re-create them from the spec held in the etcd database. In case of your worker node failure, then it will be recovered.\nSo, do we need a backup?  \n\nHere are a couple of examples. It will not prevent a misconfigured application from overwriting its data, or an operator from running a command with the wrong environment variables and accidentally dropping the production database instead of the development one. Another example is that it won’t protect you from accidentally deleting the storage volume by mis-clicking in a web console.  \n\n\nCloud Native DevOps with Kubernetes:  \nhttps://learning.oreilly.com/library/view/cloud-native-devops/9781492040750/ch11.html#  \n\n\nHowever, the major factor in running and managing a successful backup process is to understand the needs, SLOs and how services will be delivered to the platform users (i.e. the Platform Operators configure application backups, or the tools are provided and the Application teams configure backups and the platform team is only concerned with the Platform and Storage backups).  This split in responsibility is common and one of the major tenants of OpenShift - Self Service\n\nAs discussed below, you will find sections on \"Day 1\" (the installation) and \"Day 2\" (the daily operations) of the platform.  \n\n\n\nWhen you consider backup in OCP there are 2 important catagories to consider: **system backup** and **application data backup**.   \n- System backup means backup up your etcd data.  \n- Backup of Application Data includes both your deployments and data which your applications are accessing to.  \n\nHowever, one additional aspect that needs to taken into account is \"What is contained within the platform??\".  While we discuss within this document the use of the out-of-the-box registry for storing images, capturing build or configuration data, there are cases where this data is not hosted within the platform.  As an example, an external build tool and image repository (i.e. GitOps), where all data for the application is stored in Git and webhooks are used to orchestrate build, tests and deployments.  In those cases the Platform SRE and the DevOps Engineer would focus on how and what must be backed up from a platform perspective and what is handled via a different group/domain (i.e. the SRE team for GitHub).\n\nAs in any backup it is very important that you test your ability to restore on a regular basis.\n\nIf you are using OpenShift \"Managed\" service from the Cloud vendor such as [IBM Cloud](#Backups-on-IBM-Cloud), AWS, and Azure, your cloud provider may take care of the etcd and you may not be able to maintain the etcd yourself.  You need to check with your cloud provider to make sure whether you need to take care of the etcd or not.  \n\n\n\n\n\n## Day 1 Platform\n\nAs part of the platform installation, design decisions (Day 0), must be implemented or placed into the technical debt to be configured as part of Day 2.  As a starting point, the following Day 1 functionality/debt should be installed or captured as a set of initial tasks for Day 2.  While the common statement of \"Backup all Persistent Volumes and you're done\", would be nice, true platform and application backups require additional thought.\n\n- Selection and install of backup solutions for the platform.  This includes products, agents or agreed upon processes (i.e. if installed on VMware, how the backup functions of VMware will support the platform)  \n\n\n\n\n## Day 2 Platform\n\nHere are the list of Infrastructure-related backup items for day 2.  We will discuss each items in the later section on this page.\n- [Backup etcd (master hosts)](#Backing-up-etcd-data): The etcd persists the state of all resource objects and you make a back up etcd data regularly.\n- [Storage backup](#Backing-up-storage): Stateful apps save data, mostly attached on storage (persistent volumes), and it contains all the information that apps need in order to run properly making it a priority to backup.\n- [Images backup](#Backing-up-images): You can use either internal or external Image Registry to store your images. We discuss the image backup in a separate [document](../Image/).\n- [Certificate backup](#Backing-up-certificate): You have several TLS certificates in your OpenShift cluster. We discuss the certificate backup in a separate [document](../Security).\n- [Backing up services data](#backing-up-services-data): In case you are using the advanced features which comes with OpenShift, you need to consider the persistent storage backup.\n- [Backing up users](#Backing-up-users): If you are using LDAP for user authentication, your LDAP server administrator makes users backup.  If you are storing users in your OpenShift cluster with htpasswod for example, then you need to make a backup those.\n- [Backing up resource state](#backing-up-resource-state): You need to make a back up of the resource state in case you delete the wrong deployment or others.\n- [Recovering from lost master host(s)](#Recovering-from-lost-master-hosts): If one or more master permanently loses, you need to recover those. \n- [Restore cluster state](#restoring-to-a-previous-cluster-state): In case an administrator such as SRE or DevOps Engineer deleted something critical, we need to restore your cluster to the previous state.\n- [Recovering from expired control plane certificates](#recovering-from-expired-control-plane-certificates): If the TLS certificates are not valid on the cluster member being replaced, then you need to recover from expired control plane certificates.  \n- [Restore procedures and test](#Restore-procedures-and-test): You should write a detailed, step-by-step procedure describing how to restore data from backups, and make sure all staff know where to find this document.\n\n\n\n## Day 1 Application  \n\nAs Day 1, Application backup tasks will cover the following 3 areas:\n- Stateless applications and/or the pod images are not normally backed up (the orchestration engine will automatically recreate a failed pod/container, or a new deployment task is initiated)\n  - Apps can be redeployed from CI/CD\n  - Config YAMLs can be restored from version control (like Git)\n- Install, test and enable tools utilized for application backup types\n- There are needs to backup services provided for the DevOps Engineer to utilize or consume for persistent data.  This can/will include component specific agents needed for some items. You need to ensure that the data is consistent, sometimes this require locking of the application before taking the backup.\n\n\n\n\n## Day 2 Application\n\nAs Day 2, Application backup tasks can be broken down into 3 distinct areas:\n- [Backing up resource state](#backing-up-resource-state): Ensuring that the deployment configuration is backed up (roles and rolebindings, quotas, templates, Etc.), such that the project can be recovered.  This can be split between the platform and application team in that there are some resources (i.e. an external build tool, external image repository, Etc.) that are not within the scope of the platform SRE team.  In this case, it is the responsibility of the Application team to ensure that these non-platform tools are backed up.  \n- [Backing up your application data](#backing-up-your-application-data): Applications that require stateful data (i.e. data stored in a No/SQL database or StatefulSets) are provided and managed by the DevOps Engineer (and SRE).\n- [Repository backup/Images management](#repository-backup-images-management): Any images used within the application must be backed up to ensure that the latest revision deployed is captured and backed up. A repository backup will normally satisfy this requirement. The image management including backup and build process is out of scope of this document.  \n\n\n\n\n## Mapping to Personas\n\nNo | Persona | task\n--- | --- | ---\n1 | SRE | Backing up etcd data\n2 | SRE | Backing up storage\n3 | SRE | Backing up images\n4 | SRE | Backing up certificates\n5 | SRE | Backing up Services data\n6 | SRE | Backing up users\n7 | SRE | Backing up resource state\n8 | SRE | Recovering from lost master host(s)\n9 | SRE | Restoring to a previous cluster state\n10 | SRE | Recovering from expired control plane certificates\n11 | SRE, DevOps Engineer | Restore procedures and test\n12 | SRE, DevOps Engineer | Backing up resource state\n13 | SRE, DevOps Engineer | Backing up your application data\n14 | SRE, DevOps Engineer | Repository backup/images management\n\n\n\n\n\n<a name=\"Backing-up-etcd-data\"></a>\n\n## Backing up etcd data: [ SRE ]\nThe etcd is the key-value store for OpenShift Container Platform, which persists the state of all resource objects.  \nBack up your cluster’s etcd data regularly and store in a secure location ideally outside the OpenShift Container Platform environment.  \n\nNote that do NOT take an etcd backup before the first certificate rotation completes, which occurs 24 hours after installation, otherwise the backup will contain expired certificates.  \n\nIt is also recommended to take etcd backups during non-peak usage hours, as it is a blocking action.  \nOnce you have an etcd backup, you can recover from lost master hosts and restore to a previous cluster state.  \nYou can perform the etcd data backup process on any master host that has connectivity to the etcd cluster, where the proper certificates are provided.  \n\nYou can find out the procedure to back up etcd data in the official document.  \nhttps://docs.openshift.com/container-platform/4.3/backup_and_restore/backing-up-etcd.html  \n\n\n\n\n<a name=\"Backing-up-storage\"></a>\n\n## Backing up storage: [ SRE ]\nStateful apps save data, mostly attached on storage (persistent volumes), and it contains all the information that apps need in order to run properly making it a priority to backup. While\"Application Data\" is the norm for what needs to be backed up for applications (see \"Application Data Backups\", there are additional areas to be considered and they are lumped under the term \"Storage\".  These essential items such as secrets (normally part of the etcd backup), certificates (once again, normal part of the etcd backup).  However, this can also include specific items such as application specific Software Defined Network (SDN configuration), Etc.  In summary, all configuration items needed to restore the application (from image, to platform configuration, to application data), must be included in your backup solution and plan\n\n\n\n<a name=\"Backing-up-images\"></a>\n\n## Images backup: [ SRE ]\nYou can use either internal or external Image Registry to store your images. We discuss the image backup in a separate [document](../Image/).  \n\nWhen utilizing the internal/Red Hat provided Registry (installed via an Operator), the Registry is configured and utilizes a standard Persistent Volume.  However, while the standard \"backup all persistent volumes\", still applies, the Image Registry should have its own set of SLOs and processes for backups.  As an example:\n- Image Registry backups will occur more frequently than other Persistent Volumes\n- Image Registry backups will be triggered whenever there is a commit (Image added), to ensure minimal/no loss of data\n- Image Registry backups will allow for the restoration of a single image (instead of a complete Persistent Volume restoration).\n- Etc.  \n\nThe takeaway from the above is that the Image Registry is a shared service (versus an application level), that your platform consumers are depending upon the operations team to maintain and ensure data recovery in the case of failures.\n\n\n\n<a name=\"Backing-up-certificate\"></a>\n\n## Certificate backup: [ SRE ]\nYou have several TLS certificates in your OpenShift cluster. We discuss the certificate backup in a separate [document](../Security/).  \n\n\n<a name=\"backing-up-services-data\"></a>\n\n## Backing up Services data: [ SRE ]\nIn case you are using the following features which comes with OpenShift, you need to consider the persistent storage backup:\n- **Monitoring** (Prometheus and Alertmanager)\n- **Logging** (Elasticsearch)\n- **Metering** (Hive and Presto)\n- **Service Mesh** (Elasticsearch)   \n\n\n\n<a name=\"Backing-up-users\"></a>\n\n## Backing up users: [ SRE ]\nIf you are using LDAP for user authentication, your LDAP server administrator makes users backup.  If you are storing users in your OpenShift cluster with htpasswod for example, then you need to make a backup those.  \n\n\n\n<a name=\"backing-up-resource-state\"></a>\n\n## Backing up resource state: [ DevOps Engineer ]\nApart from etcd failures, there is also the question of saving the state of your individual resources. If you delete the wrong Deployment, for example, how would you re-create it?\n\nYou would manage your OpenShift resources declaratively, by applying YAML manifests stored in version control.  \nIn theory, then, to re-create (restore) the state of your resource, you should be able to check out the relevant version control repository, and apply all the resources in them. \n\nNote:  Some of these items (i.e. build tools, image repositories, Etc.), may not be hosted or under the control of the Platform SREs.  In those cases, it is the responsibility of the Development Engineer to ensure a backup process is in place and tested.\n\nCloud Native DevOps with Kubernetes:  \nhttps://learning.oreilly.com/library/view/cloud-native-devops/9781492040750/ch11.html#   \n\nWhen utilizing the internal/Red Hat provided Build and Pipeline tools (installed via an Operator), these tools are configured and utilize a standard set of Persistent Volumes.  However, while the standard \"backup all persistent volumes\", still applies, the Build & Pipeline tools should have its own set of SLOs and processes for backups.  As an example:\n- Backups will occur more frequently than other Persistent Volumes\n- Backups will allow for the restoration of a single Application set of data (instead of a complete Persistent Volume restoration).\n- Etc.  \n\nThe takeaway from the above is that the Build and Pipeline tools are a shared service (versus an application level), that your platform consumers are depending upon the operations team to maintain and ensure data recovery in the case of failures. \n\n\n\n<a name=\"Recovering-from-lost-master-hosts\"></a>\n\n## Recovering from lost master hosts: [ SRE ]\nWe talked about etcd database which we will make a backup. You need to understand a bit more detail about the etcd to prepare master hosts recovery including etcd.  \n\nThe **etcd** is designed to withstand machine failures. An etcd cluster automatically recovers from temporary failures (e.g., machine reboots) and tolerates up to (N-1)/2 permanent failures for a cluster of N members.   \nWhen a member permanently fails, whether due to hardware failure or disk corruption, it loses access to the cluster.   \nIf the cluster permanently loses more than (N-1)/2 members then it disastrously fails, irrevocably losing quorum. Once quorum is lost, the cluster cannot reach consensus and therefore cannot continue accepting updates.  \nhttps://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md   \n\n\nThere are a few situations you can think of:\n- A single etcd member is failed. It assumes that there is still an etcd quorum in the cluster.\n- A majority of your masters still available and have an etcd quorum, then follow the procedure to replace a single failed master host (etcd member).\n- A majority of master hosts have been lost, leading to etcd quorum loss and the cluster going offline. It assumes that you have at least one healthy master host.\n\nFor the fist and second scenarios above, your recovery procedure will be the same. For the 3rd scenario, you will have different recovery procedure.  See the official manual for procedures.  \nNote:OpenShift 4.x requires minimum 3 master nodes where the etcd runs.\n\nYou can find out the procedure to recover from lost master hosts in the official document.  \nhttps://docs.openshift.com/container-platform/4.3/backup_and_restore/replacing-failed-master.html\nhttps://docs.openshift.com/container-platform/4.3/backup_and_restore/disaster_recovery/scenario-1-infra-recovery.html#dr-infrastructure-recovery  \n\n\n\n<a name=\"restoring-to-a-previous-cluster-state\"></a>\n\n## Restoring to a previous cluster state: [ SRE ]\nTo restore the cluster to a previous state, you must have previously backed up etcd data by creating a snapshot.   \nYou will use the snapshot (a saved etcd backup) to restore back to a previous cluster state.  \n\nFor example, if an administrator (either SRE and/or DevOps Engineer) deletes something critical. As long as you have taken an etcd backup, you can restore your cluster to a previous state.\n\nYou can find out the procedure to restore to a previous cluster state in the official document.  \nhttps://docs.openshift.com/container-platform/4.3/backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.html\n\n\n\n\n<a name=\"recovering-from-expired-control-plane-certificates\"></a>\n\n## Recovering from expired control plane certificates: [ SRE ]\nIf the TLS certificates are not valid on the cluster member being replaced, then you need to recover from expired control plane certificates. \nAnother scenario is that if you shut down your cluster before the first certificate rotation, which occurs 24 hours after installation, your certificates will not be rotated and will expire. Then you need to recover from expired control plane certificates to access your cluster.  \n\nYou can find out the procedure to recover from expired control plane certificates in the official document.  \nhttps://docs.openshift.com/container-platform/4.3/backup_and_restore/disaster_recovery/scenario-3-expired-certs.html#dr-recovering-expired-certs   \n\n\n\n\n\n<a name=\"Restore-procedures-and-test\"></a>\n\n## Restore procedures and test: [ SRE / DevOps Engineer ]\nIt’s not very likely that you’d lose the whole cluster. What’s more likely is that you (or your newest team member) might delete a namespace by accident, shut down a Deployment without meaning to, or specify the wrong set of labels to a kubectl delete command, removing more than you intended. Whatever the cause, it does happen.  \n\nYou should write a detailed, step-by-step procedure describing how to restore data from backups, and make sure all staff know where to find this document. When a disaster happens, it’s usually at an inconvenient time, the key people aren’t available, everyone’s in a panic, and your procedure should be so clear and precise that it can be carried out by someone who isn’t familiar with Kubernetes.  \n\nEach month, run a restore test by having a different team member execute the restore procedure against a temporary cluster. This verifies both that your backups are good and that the restore procedure is correct, and makes sure everyone is familiar with how to do it.  \n\nCloud Native DevOps with Kubernetes:  \nhttps://learning.oreilly.com/library/view/cloud-native-devops/9781492040750/ch11.html#  \n\n\n\n\n<a name=\"backing-up-your-application-data\"></a>\n\n## Backing up your application data: [ SRE / DevOps Engineer ]\nBackup strategy for applications running on kubernetes depends on application design (stateless vs stateful) and deployment process (CI/CD).  \nStateless application does not store any state data and simple re-deployment from from CI/CD pipeline or from YAML definition files should be sufficient to restore the application.  \nStateful apps on the other hand save data, mostly attached on [persistent volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/), and it is these volumes that contain all the information that apps need in order to run properly making it a priority to backup. A persistent volume (PV) is a cluster-wide resource used to store data in a way that it persists beyond the lifetime of a pod. There are many [types of PVs](https://docs.openshift.com/container-platform/4.2/storage/understanding-persistent-storage.html#types-of-persistent-volumes_understanding-persistent-storage) and some of them are backed up by networked storage system such as EBS, NFS or Ceph and other are locally attached storage like Local Volume or HostPath. The storage space in a PV is requested via another type of kubernetes object - Persistent Volume Claim (PVC). The PVC requests a PV with desired specification (size, speed, etc.) from Kubernetes and binds it then to a pod, where it can be mounted as a volume.  \n\nKubernetes persistent volumes by example   \nhttp://kubernetesbyexample.com/pv/\n  \n\n\n\n### Application specific backup\nOther option for application backup is to use standard backup tools specific for the workload using persistent volumes like `pg_dump` for Postgresql or `mongodump` for MongoDB.\n\n\n\n<a name=\"repository-backup-images-management\"></a>\n\n## Repository backup/Images management: [ SRE / DevOps Engineer ]\nAny images used within the application must be backed up to ensure that the latest revision deployed is captured and backed up. A repository backup will normally satisfy this requirement. The image management including backup and build process is out of scope of this document.  \n\nAs stated earlier, backing up the Image Registry and the Red Hat Provided Build & Pipeline tools is a key concept that should have specific SLOs and processes.  This satisfies the requirements for a platform provided set of tools and image retention.  However, if using external backup product then this comes under the scope of the team provide the backup service, to ensure Images within the registrey and backed up and restored\nFrom a restoration perspective; restoring the Image Repository would ensure any existing build or deployment scripts work as expected.  The restoration of the Red Hat provided Build and Pipeline tools data (and the tools themselves if necessary), would enable any Development team to recreate and deploy an image using the original build files.\n\n## Implementing backup\n\n## Kubernetes out-of-the-box\nKubernetes is not providing any specific function for backup. However, backup can be implemented using Operator Framework.  \nValero is an open source tool typically used for backup (see reference for details).    \n\n\n\n## OpenShift out-of-the-box\n\nOpenShift does support etcd backup and restore.  \nhttps://docs.openshift.com/container-platform/4.3/backup_and_restore/backing-up-etcd.html  \n\nYou can implement your own operator to make a back up of your application by using Operator Framework.  \n\n\n## IBM Cloud Pak for MCM  \n\n\n\n## Backups on IBM or other Clouds\nWith OpenShift on IBM Cloud (and similar to the managed OCP platforms which are a managed services), you cannot access the Masters to manage it.  In other words, the vendor will maintain the etcd on Master nodes. However, these backups do not allow for single Object restoration (i.e. deleting a secret by accident).  For that reason, it is impposible to recover these items so rigid procesess must be utilized to ensure that these sort of actions are recoverable.    \n\n\n\n## Others\n\n**Velero**  \nVelero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes.  \nhttps://velero.io/   \nHow Velero works: https://velero.io/docs/v1.2.0/how-velero-works/  \n\n**Stash**\n[Stash](https://stash.run/)  \n\n\n\n## Other consideration\n\n- Platform backup SLOs and requirements.  This will include items such as etcd backed up weekly (since a new member can be installed and synched).  How often the repository will be backed up, how the change process will include the need to perform backups outside of schedule (i.e. Certificates updated, perform backup as part of that process such that a restoration will include the new certificates).  These requirements create the platform processes, SLOs and must meet the business continuity requirements for restoration due to a failure or data loss.  \n  \n\n- Mapping of the entire platform as to the technology and monitoring of backups.  I.e. Some Operators provide \"backup\" services embedded in the Operator and do not need external products.  Their configuration is performed via a CRD and the resulting backup can then be captured using additional products as necessary (i.e. Operator opens destination pool, performs backup but they pool may still be part of the platform storage system which will require additional technology to backup\n\n","type":"Mdx","contentDigest":"171745ec75a5a8876c94b64c251b4282","counter":542,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"OpenShift Platform Day2 - Backup","description":"OpenShift Day2 Backup","keywords":"OpenShift, day2, backup"},"exports":{},"rawBody":"---\ntitle: OpenShift Platform Day2 - Backup\ndescription: OpenShift Day2 Backup\nkeywords: 'OpenShift, day2, backup'\n---\n\n## Backups Overview\nAll Kubernetes resources are described by data stored in a reliable database which is etcd. In the event of some Pods being  deleted, their supervising Deployment will re-create them from the spec held in the etcd database. In case of your worker node failure, then it will be recovered.\nSo, do we need a backup?  \n\nHere are a couple of examples. It will not prevent a misconfigured application from overwriting its data, or an operator from running a command with the wrong environment variables and accidentally dropping the production database instead of the development one. Another example is that it won’t protect you from accidentally deleting the storage volume by mis-clicking in a web console.  \n\n\nCloud Native DevOps with Kubernetes:  \nhttps://learning.oreilly.com/library/view/cloud-native-devops/9781492040750/ch11.html#  \n\n\nHowever, the major factor in running and managing a successful backup process is to understand the needs, SLOs and how services will be delivered to the platform users (i.e. the Platform Operators configure application backups, or the tools are provided and the Application teams configure backups and the platform team is only concerned with the Platform and Storage backups).  This split in responsibility is common and one of the major tenants of OpenShift - Self Service\n\nAs discussed below, you will find sections on \"Day 1\" (the installation) and \"Day 2\" (the daily operations) of the platform.  \n\n\n\nWhen you consider backup in OCP there are 2 important catagories to consider: **system backup** and **application data backup**.   \n- System backup means backup up your etcd data.  \n- Backup of Application Data includes both your deployments and data which your applications are accessing to.  \n\nHowever, one additional aspect that needs to taken into account is \"What is contained within the platform??\".  While we discuss within this document the use of the out-of-the-box registry for storing images, capturing build or configuration data, there are cases where this data is not hosted within the platform.  As an example, an external build tool and image repository (i.e. GitOps), where all data for the application is stored in Git and webhooks are used to orchestrate build, tests and deployments.  In those cases the Platform SRE and the DevOps Engineer would focus on how and what must be backed up from a platform perspective and what is handled via a different group/domain (i.e. the SRE team for GitHub).\n\nAs in any backup it is very important that you test your ability to restore on a regular basis.\n\nIf you are using OpenShift \"Managed\" service from the Cloud vendor such as [IBM Cloud](#Backups-on-IBM-Cloud), AWS, and Azure, your cloud provider may take care of the etcd and you may not be able to maintain the etcd yourself.  You need to check with your cloud provider to make sure whether you need to take care of the etcd or not.  \n\n\n\n\n\n## Day 1 Platform\n\nAs part of the platform installation, design decisions (Day 0), must be implemented or placed into the technical debt to be configured as part of Day 2.  As a starting point, the following Day 1 functionality/debt should be installed or captured as a set of initial tasks for Day 2.  While the common statement of \"Backup all Persistent Volumes and you're done\", would be nice, true platform and application backups require additional thought.\n\n- Selection and install of backup solutions for the platform.  This includes products, agents or agreed upon processes (i.e. if installed on VMware, how the backup functions of VMware will support the platform)  \n\n\n\n\n## Day 2 Platform\n\nHere are the list of Infrastructure-related backup items for day 2.  We will discuss each items in the later section on this page.\n- [Backup etcd (master hosts)](#Backing-up-etcd-data): The etcd persists the state of all resource objects and you make a back up etcd data regularly.\n- [Storage backup](#Backing-up-storage): Stateful apps save data, mostly attached on storage (persistent volumes), and it contains all the information that apps need in order to run properly making it a priority to backup.\n- [Images backup](#Backing-up-images): You can use either internal or external Image Registry to store your images. We discuss the image backup in a separate [document](../Image/).\n- [Certificate backup](#Backing-up-certificate): You have several TLS certificates in your OpenShift cluster. We discuss the certificate backup in a separate [document](../Security).\n- [Backing up services data](#backing-up-services-data): In case you are using the advanced features which comes with OpenShift, you need to consider the persistent storage backup.\n- [Backing up users](#Backing-up-users): If you are using LDAP for user authentication, your LDAP server administrator makes users backup.  If you are storing users in your OpenShift cluster with htpasswod for example, then you need to make a backup those.\n- [Backing up resource state](#backing-up-resource-state): You need to make a back up of the resource state in case you delete the wrong deployment or others.\n- [Recovering from lost master host(s)](#Recovering-from-lost-master-hosts): If one or more master permanently loses, you need to recover those. \n- [Restore cluster state](#restoring-to-a-previous-cluster-state): In case an administrator such as SRE or DevOps Engineer deleted something critical, we need to restore your cluster to the previous state.\n- [Recovering from expired control plane certificates](#recovering-from-expired-control-plane-certificates): If the TLS certificates are not valid on the cluster member being replaced, then you need to recover from expired control plane certificates.  \n- [Restore procedures and test](#Restore-procedures-and-test): You should write a detailed, step-by-step procedure describing how to restore data from backups, and make sure all staff know where to find this document.\n\n\n\n## Day 1 Application  \n\nAs Day 1, Application backup tasks will cover the following 3 areas:\n- Stateless applications and/or the pod images are not normally backed up (the orchestration engine will automatically recreate a failed pod/container, or a new deployment task is initiated)\n  - Apps can be redeployed from CI/CD\n  - Config YAMLs can be restored from version control (like Git)\n- Install, test and enable tools utilized for application backup types\n- There are needs to backup services provided for the DevOps Engineer to utilize or consume for persistent data.  This can/will include component specific agents needed for some items. You need to ensure that the data is consistent, sometimes this require locking of the application before taking the backup.\n\n\n\n\n## Day 2 Application\n\nAs Day 2, Application backup tasks can be broken down into 3 distinct areas:\n- [Backing up resource state](#backing-up-resource-state): Ensuring that the deployment configuration is backed up (roles and rolebindings, quotas, templates, Etc.), such that the project can be recovered.  This can be split between the platform and application team in that there are some resources (i.e. an external build tool, external image repository, Etc.) that are not within the scope of the platform SRE team.  In this case, it is the responsibility of the Application team to ensure that these non-platform tools are backed up.  \n- [Backing up your application data](#backing-up-your-application-data): Applications that require stateful data (i.e. data stored in a No/SQL database or StatefulSets) are provided and managed by the DevOps Engineer (and SRE).\n- [Repository backup/Images management](#repository-backup-images-management): Any images used within the application must be backed up to ensure that the latest revision deployed is captured and backed up. A repository backup will normally satisfy this requirement. The image management including backup and build process is out of scope of this document.  \n\n\n\n\n## Mapping to Personas\n\nNo | Persona | task\n--- | --- | ---\n1 | SRE | Backing up etcd data\n2 | SRE | Backing up storage\n3 | SRE | Backing up images\n4 | SRE | Backing up certificates\n5 | SRE | Backing up Services data\n6 | SRE | Backing up users\n7 | SRE | Backing up resource state\n8 | SRE | Recovering from lost master host(s)\n9 | SRE | Restoring to a previous cluster state\n10 | SRE | Recovering from expired control plane certificates\n11 | SRE, DevOps Engineer | Restore procedures and test\n12 | SRE, DevOps Engineer | Backing up resource state\n13 | SRE, DevOps Engineer | Backing up your application data\n14 | SRE, DevOps Engineer | Repository backup/images management\n\n\n\n\n\n<a name=\"Backing-up-etcd-data\"></a>\n\n## Backing up etcd data: [ SRE ]\nThe etcd is the key-value store for OpenShift Container Platform, which persists the state of all resource objects.  \nBack up your cluster’s etcd data regularly and store in a secure location ideally outside the OpenShift Container Platform environment.  \n\nNote that do NOT take an etcd backup before the first certificate rotation completes, which occurs 24 hours after installation, otherwise the backup will contain expired certificates.  \n\nIt is also recommended to take etcd backups during non-peak usage hours, as it is a blocking action.  \nOnce you have an etcd backup, you can recover from lost master hosts and restore to a previous cluster state.  \nYou can perform the etcd data backup process on any master host that has connectivity to the etcd cluster, where the proper certificates are provided.  \n\nYou can find out the procedure to back up etcd data in the official document.  \nhttps://docs.openshift.com/container-platform/4.3/backup_and_restore/backing-up-etcd.html  \n\n\n\n\n<a name=\"Backing-up-storage\"></a>\n\n## Backing up storage: [ SRE ]\nStateful apps save data, mostly attached on storage (persistent volumes), and it contains all the information that apps need in order to run properly making it a priority to backup. While\"Application Data\" is the norm for what needs to be backed up for applications (see \"Application Data Backups\", there are additional areas to be considered and they are lumped under the term \"Storage\".  These essential items such as secrets (normally part of the etcd backup), certificates (once again, normal part of the etcd backup).  However, this can also include specific items such as application specific Software Defined Network (SDN configuration), Etc.  In summary, all configuration items needed to restore the application (from image, to platform configuration, to application data), must be included in your backup solution and plan\n\n\n\n<a name=\"Backing-up-images\"></a>\n\n## Images backup: [ SRE ]\nYou can use either internal or external Image Registry to store your images. We discuss the image backup in a separate [document](../Image/).  \n\nWhen utilizing the internal/Red Hat provided Registry (installed via an Operator), the Registry is configured and utilizes a standard Persistent Volume.  However, while the standard \"backup all persistent volumes\", still applies, the Image Registry should have its own set of SLOs and processes for backups.  As an example:\n- Image Registry backups will occur more frequently than other Persistent Volumes\n- Image Registry backups will be triggered whenever there is a commit (Image added), to ensure minimal/no loss of data\n- Image Registry backups will allow for the restoration of a single image (instead of a complete Persistent Volume restoration).\n- Etc.  \n\nThe takeaway from the above is that the Image Registry is a shared service (versus an application level), that your platform consumers are depending upon the operations team to maintain and ensure data recovery in the case of failures.\n\n\n\n<a name=\"Backing-up-certificate\"></a>\n\n## Certificate backup: [ SRE ]\nYou have several TLS certificates in your OpenShift cluster. We discuss the certificate backup in a separate [document](../Security/).  \n\n\n<a name=\"backing-up-services-data\"></a>\n\n## Backing up Services data: [ SRE ]\nIn case you are using the following features which comes with OpenShift, you need to consider the persistent storage backup:\n- **Monitoring** (Prometheus and Alertmanager)\n- **Logging** (Elasticsearch)\n- **Metering** (Hive and Presto)\n- **Service Mesh** (Elasticsearch)   \n\n\n\n<a name=\"Backing-up-users\"></a>\n\n## Backing up users: [ SRE ]\nIf you are using LDAP for user authentication, your LDAP server administrator makes users backup.  If you are storing users in your OpenShift cluster with htpasswod for example, then you need to make a backup those.  \n\n\n\n<a name=\"backing-up-resource-state\"></a>\n\n## Backing up resource state: [ DevOps Engineer ]\nApart from etcd failures, there is also the question of saving the state of your individual resources. If you delete the wrong Deployment, for example, how would you re-create it?\n\nYou would manage your OpenShift resources declaratively, by applying YAML manifests stored in version control.  \nIn theory, then, to re-create (restore) the state of your resource, you should be able to check out the relevant version control repository, and apply all the resources in them. \n\nNote:  Some of these items (i.e. build tools, image repositories, Etc.), may not be hosted or under the control of the Platform SREs.  In those cases, it is the responsibility of the Development Engineer to ensure a backup process is in place and tested.\n\nCloud Native DevOps with Kubernetes:  \nhttps://learning.oreilly.com/library/view/cloud-native-devops/9781492040750/ch11.html#   \n\nWhen utilizing the internal/Red Hat provided Build and Pipeline tools (installed via an Operator), these tools are configured and utilize a standard set of Persistent Volumes.  However, while the standard \"backup all persistent volumes\", still applies, the Build & Pipeline tools should have its own set of SLOs and processes for backups.  As an example:\n- Backups will occur more frequently than other Persistent Volumes\n- Backups will allow for the restoration of a single Application set of data (instead of a complete Persistent Volume restoration).\n- Etc.  \n\nThe takeaway from the above is that the Build and Pipeline tools are a shared service (versus an application level), that your platform consumers are depending upon the operations team to maintain and ensure data recovery in the case of failures. \n\n\n\n<a name=\"Recovering-from-lost-master-hosts\"></a>\n\n## Recovering from lost master hosts: [ SRE ]\nWe talked about etcd database which we will make a backup. You need to understand a bit more detail about the etcd to prepare master hosts recovery including etcd.  \n\nThe **etcd** is designed to withstand machine failures. An etcd cluster automatically recovers from temporary failures (e.g., machine reboots) and tolerates up to (N-1)/2 permanent failures for a cluster of N members.   \nWhen a member permanently fails, whether due to hardware failure or disk corruption, it loses access to the cluster.   \nIf the cluster permanently loses more than (N-1)/2 members then it disastrously fails, irrevocably losing quorum. Once quorum is lost, the cluster cannot reach consensus and therefore cannot continue accepting updates.  \nhttps://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md   \n\n\nThere are a few situations you can think of:\n- A single etcd member is failed. It assumes that there is still an etcd quorum in the cluster.\n- A majority of your masters still available and have an etcd quorum, then follow the procedure to replace a single failed master host (etcd member).\n- A majority of master hosts have been lost, leading to etcd quorum loss and the cluster going offline. It assumes that you have at least one healthy master host.\n\nFor the fist and second scenarios above, your recovery procedure will be the same. For the 3rd scenario, you will have different recovery procedure.  See the official manual for procedures.  \nNote:OpenShift 4.x requires minimum 3 master nodes where the etcd runs.\n\nYou can find out the procedure to recover from lost master hosts in the official document.  \nhttps://docs.openshift.com/container-platform/4.3/backup_and_restore/replacing-failed-master.html\nhttps://docs.openshift.com/container-platform/4.3/backup_and_restore/disaster_recovery/scenario-1-infra-recovery.html#dr-infrastructure-recovery  \n\n\n\n<a name=\"restoring-to-a-previous-cluster-state\"></a>\n\n## Restoring to a previous cluster state: [ SRE ]\nTo restore the cluster to a previous state, you must have previously backed up etcd data by creating a snapshot.   \nYou will use the snapshot (a saved etcd backup) to restore back to a previous cluster state.  \n\nFor example, if an administrator (either SRE and/or DevOps Engineer) deletes something critical. As long as you have taken an etcd backup, you can restore your cluster to a previous state.\n\nYou can find out the procedure to restore to a previous cluster state in the official document.  \nhttps://docs.openshift.com/container-platform/4.3/backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.html\n\n\n\n\n<a name=\"recovering-from-expired-control-plane-certificates\"></a>\n\n## Recovering from expired control plane certificates: [ SRE ]\nIf the TLS certificates are not valid on the cluster member being replaced, then you need to recover from expired control plane certificates. \nAnother scenario is that if you shut down your cluster before the first certificate rotation, which occurs 24 hours after installation, your certificates will not be rotated and will expire. Then you need to recover from expired control plane certificates to access your cluster.  \n\nYou can find out the procedure to recover from expired control plane certificates in the official document.  \nhttps://docs.openshift.com/container-platform/4.3/backup_and_restore/disaster_recovery/scenario-3-expired-certs.html#dr-recovering-expired-certs   \n\n\n\n\n\n<a name=\"Restore-procedures-and-test\"></a>\n\n## Restore procedures and test: [ SRE / DevOps Engineer ]\nIt’s not very likely that you’d lose the whole cluster. What’s more likely is that you (or your newest team member) might delete a namespace by accident, shut down a Deployment without meaning to, or specify the wrong set of labels to a kubectl delete command, removing more than you intended. Whatever the cause, it does happen.  \n\nYou should write a detailed, step-by-step procedure describing how to restore data from backups, and make sure all staff know where to find this document. When a disaster happens, it’s usually at an inconvenient time, the key people aren’t available, everyone’s in a panic, and your procedure should be so clear and precise that it can be carried out by someone who isn’t familiar with Kubernetes.  \n\nEach month, run a restore test by having a different team member execute the restore procedure against a temporary cluster. This verifies both that your backups are good and that the restore procedure is correct, and makes sure everyone is familiar with how to do it.  \n\nCloud Native DevOps with Kubernetes:  \nhttps://learning.oreilly.com/library/view/cloud-native-devops/9781492040750/ch11.html#  \n\n\n\n\n<a name=\"backing-up-your-application-data\"></a>\n\n## Backing up your application data: [ SRE / DevOps Engineer ]\nBackup strategy for applications running on kubernetes depends on application design (stateless vs stateful) and deployment process (CI/CD).  \nStateless application does not store any state data and simple re-deployment from from CI/CD pipeline or from YAML definition files should be sufficient to restore the application.  \nStateful apps on the other hand save data, mostly attached on [persistent volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/), and it is these volumes that contain all the information that apps need in order to run properly making it a priority to backup. A persistent volume (PV) is a cluster-wide resource used to store data in a way that it persists beyond the lifetime of a pod. There are many [types of PVs](https://docs.openshift.com/container-platform/4.2/storage/understanding-persistent-storage.html#types-of-persistent-volumes_understanding-persistent-storage) and some of them are backed up by networked storage system such as EBS, NFS or Ceph and other are locally attached storage like Local Volume or HostPath. The storage space in a PV is requested via another type of kubernetes object - Persistent Volume Claim (PVC). The PVC requests a PV with desired specification (size, speed, etc.) from Kubernetes and binds it then to a pod, where it can be mounted as a volume.  \n\nKubernetes persistent volumes by example   \nhttp://kubernetesbyexample.com/pv/\n  \n\n\n\n### Application specific backup\nOther option for application backup is to use standard backup tools specific for the workload using persistent volumes like `pg_dump` for Postgresql or `mongodump` for MongoDB.\n\n\n\n<a name=\"repository-backup-images-management\"></a>\n\n## Repository backup/Images management: [ SRE / DevOps Engineer ]\nAny images used within the application must be backed up to ensure that the latest revision deployed is captured and backed up. A repository backup will normally satisfy this requirement. The image management including backup and build process is out of scope of this document.  \n\nAs stated earlier, backing up the Image Registry and the Red Hat Provided Build & Pipeline tools is a key concept that should have specific SLOs and processes.  This satisfies the requirements for a platform provided set of tools and image retention.  However, if using external backup product then this comes under the scope of the team provide the backup service, to ensure Images within the registrey and backed up and restored\nFrom a restoration perspective; restoring the Image Repository would ensure any existing build or deployment scripts work as expected.  The restoration of the Red Hat provided Build and Pipeline tools data (and the tools themselves if necessary), would enable any Development team to recreate and deploy an image using the original build files.\n\n## Implementing backup\n\n## Kubernetes out-of-the-box\nKubernetes is not providing any specific function for backup. However, backup can be implemented using Operator Framework.  \nValero is an open source tool typically used for backup (see reference for details).    \n\n\n\n## OpenShift out-of-the-box\n\nOpenShift does support etcd backup and restore.  \nhttps://docs.openshift.com/container-platform/4.3/backup_and_restore/backing-up-etcd.html  \n\nYou can implement your own operator to make a back up of your application by using Operator Framework.  \n\n\n## IBM Cloud Pak for MCM  \n\n\n\n## Backups on IBM or other Clouds\nWith OpenShift on IBM Cloud (and similar to the managed OCP platforms which are a managed services), you cannot access the Masters to manage it.  In other words, the vendor will maintain the etcd on Master nodes. However, these backups do not allow for single Object restoration (i.e. deleting a secret by accident).  For that reason, it is impposible to recover these items so rigid procesess must be utilized to ensure that these sort of actions are recoverable.    \n\n\n\n## Others\n\n**Velero**  \nVelero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes.  \nhttps://velero.io/   \nHow Velero works: https://velero.io/docs/v1.2.0/how-velero-works/  \n\n**Stash**\n[Stash](https://stash.run/)  \n\n\n\n## Other consideration\n\n- Platform backup SLOs and requirements.  This will include items such as etcd backed up weekly (since a new member can be installed and synched).  How often the repository will be backed up, how the change process will include the need to perform backups outside of schedule (i.e. Certificates updated, perform backup as part of that process such that a restoration will include the new certificates).  These requirements create the platform processes, SLOs and must meet the business continuity requirements for restoration due to a failure or data loss.  \n  \n\n- Mapping of the entire platform as to the technology and monitoring of backups.  I.e. Some Operators provide \"backup\" services embedded in the Operator and do not need external products.  Their configuration is performed via a CRD and the resulting backup can then be captured using additional products as necessary (i.e. Operator opens destination pool, performs backup but they pool may still be part of the platform storage system which will require additional technology to backup\n\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/day2/Backups/index.mdx"}}}}