{"componentChunkName":"component---src-pages-day-2-logging-efk-aws-index-mdx","path":"/day2/Logging/EFK_AWS/","result":{"pageContext":{"frontmatter":{"title":"OpenShift Platform Day2 - EFK installation on AWS","description":"OpenShift Day2 EFK on AWS","keywords":"OpenShift, day2, supplement, efk"},"relativePagePath":"/day2/Logging/EFK_AWS/index.mdx","titleType":"page","MdxNode":{"id":"f259968a-46ef-5c56-a0e7-a8bd09ca1a7e","children":[],"parent":"69757caf-0b6d-50e4-81fc-f9bbdedcbe89","internal":{"content":"---\ntitle: OpenShift Platform Day2 - EFK installation on AWS\ndescription: OpenShift Day2 EFK on AWS\nkeywords: 'OpenShift, day2, supplement, efk'\n---\n\n## Steps for EFK installation on OpenShift 4 on AWS\nWe will demonstrate how to install the EFK on OpenShift 4 cluster. As we described, the Operator is a key technology for the day 2 operation in OpenShift 4. We use the Operators to enable the EFK stack. For demonstration purpose, we install the Elasticsearch Operator via the oc CLI and the Cluster Logging Operator via Web Console.  \n\n## Install the Elasticsearch Operator\nAs mentioned, we will show you how to install the Elasticsearch Operator via the CLI.\n\n## Login to the OpenShift\nLog in to the OpenShift cluster with the oc login command. In the following execution example, enter _openshift api url_ according to your environment.\n\n```\n$ oc login https://<openshift api url>:6443/\n```\n\nHere is what we did in our cluster. Note that the value of -p which is the password of the user is not the actual one since I shouldn't put the real one here.  We also use the user \"kubeadmin\" in our example but it is not recommended. Instead, you should use your own user.\n\n```\n$ oc login -u kubeadmin -p abcabcabc https://api.csmo1aws.privatedomain.ml:6443\nLogin successful.\n\nYou have access to 51 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n$ \n```\n\n## Create a namespace to create an Elasticsearch Operator\nFirst, prepare a manifest file to create a namespace for creating an Elasticsearch Operator. As an example, create the file as follow.\n\n```\n$ vi eo-namespace.yaml\n$ cat eo-namespace.yaml \napiVersion: v1\nkind: Namespace\nmetadata:\n  name: openshift-operators-redhat\n  annotations:\n    openshift.io/node-selector: \"\"\n  labels:\n    openshift.io/cluster-logging: \"true\"\n    openshift.io/cluster-monitoring: \"true\"\n$  \n```\n\nNote that you can specify any namespace name for the value of \"name:\".  In our example, we specify \"openshift-operators-redhat\".\n\nThen create a namespace with the yaml file by runing the oc create command as follow.\n\n```\n$ oc create -f eo-namespace.yaml\nnamespace/openshift-operators-redhat created\n$  \n```\n\n## Create a namespace to create a Cluster Logging Operator\nPrepare a manifest file to create a namespace to create a Cluster Logging Operator. As an example, create the file as follow.\n\n```\n$ vi clo-namespace.yaml\n$ cat clo-namespace.yaml \napiVersion: v1\nkind: Namespace\nmetadata:\n  name: openshift-logging\n  annotations:\n    openshift.io/node-selector: \"\"\n  labels:\n    openshift.io/cluster-logging: \"true\"\n    openshift.io/cluster-monitoring: \"true\"\n$ \n```\n\nNote that you can specify any namespace name for the value of \"name:\".  In our example, we specify \"openshift-logging\".\n\nThen create a namespace with the yaml file by runing the oc create command as follow.\n\n```\n$ oc create -f clo-namespace.yaml\nnamespace/openshift-logging created\n$ \n```\n\n## Install the Elasticsearch Operator\nPrepare an Operator Group resource for Elasticsearch as follow.\n\n```\n$ vi eo-og.yaml\n$ cat eo-og.yaml \napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: openshift-operators-redhat\n  namespace: openshift-operators-redhat\nspec: {}\n$ \n```\nNote that for the value of the \"namespace:\", specify the namespace created for the Elasticsearch Operator namespace in the previous step.  In our example, we specify \"openshift-operators-redhat\".\n\nThen, create an Operator Group via oc create command as follow.\n\n```\n$ oc create -f eo-og.yaml\noperatorgroup.operators.coreos.com/openshift-operators-redhat created\n$ \n```\n\nUse the oc get command to check the channel name for subscribing the Operator. In the following execution example, you can confirm that the channel name is 4.2.\n\n```\n$ oc get packagemanifest elasticsearch-operator -n openshift-marketplace -o jsonpath='{.status.channels[].name}'\n4.2\n$\n```\n\nThe next step is to prepare the Subscription of the operator. Create a yaml file as follow.\n\n```\n$ vi eo-sub.yaml\n$ cat eo-sub.yaml \napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  generateName: \"elasticsearch-\"\n  namespace: \"openshift-operators-redhat\"\nspec:\n  channel: \"4.2\"\n  installPlanApproval: \"Automatic\"\n  source: \"redhat-operators\"\n  sourceNamespace: \"openshift-marketplace\"\n  name: \"elasticsearch-operator\"\n$ \n```\n\nNote that for the value of \"namespace:\", specify the one created in the previous step. In our example, we specify \"openshift-operators-redhat\".\n\nWith the yaml file, you create the subscription by issuing the oc create command as follow.\n\n```\n$ oc create -f eo-sub.yaml\nsubscription.operators.coreos.com/elasticsearch-724cq created\n$ \n```\n\nChange the current namespace by oc project command as follow.\n\n```\n$ oc project openshift-operators-redhat\nNow using project \"openshift-operators-redhat\" on server \"https://api.csmo1aws.privatedomain.ml:6443\".\n$  \n```\n\nAs the final step for the Elasticsearch operator installation, prepare an Operator RBAC object. Create the eo-rbac.yaml file as follow.\n\n```\n$ vi eo-rbac.yaml\n$ cat eo-rbac.yaml \napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: prometheus-k8s\n  namespace: openshift-operators-redhat\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: prometheus-k8s\n  namespace: openshift-operators-redhat\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: prometheus-k8s\nsubjects:\n- kind: ServiceAccount\n  name: prometheus-k8s\n  namespace: openshift-operators-redhat\n$ \n```\n\nThen, create an RBAC object using the oc create command as follow.\n\n```\n$ oc create -f eo-rbac.yaml\nrole.rbac.authorization.k8s.io/prometheus-k8s created\nrolebinding.rbac.authorization.k8s.io/prometheus-k8s created\n$ \n```\nNow, the Elasticsearch Operator installation is complete. Confirm that it is installed with the oc get command.\n```\n$ oc get pod -n openshift-operators-redhat\nNAME                                      READY   STATUS    RESTARTS   AGE\nelasticsearch-operator-8664584cbc-wq57s   1/1     Running   0          2m16s\n$ \n```\n\n\n## Install the Cluster Logging Operator  \n\nLogin with the OpenShift Web console and expand the **Operator** → **OperatorHub** menu.\nSearch for and select the Operator named “Cluster Logging” as shown in the following figure.  \n\n![cluster_logging_operator_1](./images/cluster_logging_operator_1_aws.png)\n\nClick the **Install** button in the **Cluster Logging** dialog displayed on the screen as shown in below.  \n\n![cluster_logging_operator_2](./images/cluster_logging_operator_2_aws.png)\n\nOn the **Create Operator Subscription** screen, select **openshift-logging** created in the previous step from the namespace drop-down list, and click the **Subscribe** button.\n\n![cluster_logging_operator_3](./images/cluster_logging_operator_3_aws.png)\n\nNow, we complete the **Cluster Logging Operator** installation steps.\n\n\n\n## Checking Operator installation status\nUp to this point, installation of Elasticsearch Operator and Cluster Logging Operator has been completed. Check if each Operator has been installed correctly.\n\n### Check Cluster Logging Operator\n\nSelect **Operators** -> **Installed Operators** from the Web console. At this time, perform the operation with the Project **openshift-logging** that the project where the **Cluster Logging** Operator is installed.\n\n![cluster_logging_operator_4](./images/cluster_logging_operator_4_aws.png)\n\nOn the Installed Operators page, check the **Cluster Logging** operator and find out its status which is **InstallSucceeded**.\n\n\n### Check Elasticsearch Operator\n\nChange the Project selection at the top of the center of the screen to the namespace where the **Elasticsearch Operator** is installed.  In our example, it is **openshift-opeartors-redhat** as shown in below.\n\n![cluster_logging_operator_5](./images/cluster_logging_operator_5_aws.png)\n\nOn the Installed Operators page, check the **Elasticsearch** operator and find out its status which is **InstallSucceeded**.\n\nIf there is no problem in any of the displays, you have confirmed the installation of Operators.\n\n## Creating a Cluster Logging instance\nFrom here, we will create a Pod that will be the substance of Cluster Logging.\n\n\n### Define Custom Resource Definition（CRD）\n\nSelect **Administration** -> **Custom Resource Definitions** from the Web console.\nEnter a string such as **ClusterLogging** in the filter at the top right of the screen. The **ClusterLogging** CRD is displayed. Then, click on **ClusterLogging**.\n\n![cluster_logging_operator_6](./images/cluster_logging_operator_6_aws.png)\n\n\nOn the **Custom Resource Definition Details** page, the **Overview** tab of the CRD **clusterloggings.logging.openshift.io** is displayed.\nNext, select **View Instances** from the **Actions** drop-down list in the upper right.\n\n![cluster_logging_operator_7](./images/cluster_logging_operator_7_aws.png)\n\nThe **Cluster Loggings** page is displayed. \n\n![cluster_logging_operator_8](./images/cluster_logging_operator_8_aws.png)\n\nClick the **Create Cluster Logging** button to open the edit screen of yaml for creating an instance and enter the following contents.\n\n```\napiVersion: \"logging.openshift.io/v1\"\nkind: \"ClusterLogging\"\nmetadata:\n  name: \"instance\"\n  namespace: \"openshift-logging\"\nspec:\n  managementState: \"Managed\"\n  logStore:\n    type: \"elasticsearch\"\n    elasticsearch:\n      nodeCount: 3\n      storage:\n        storageClassName: gp2\n        size: 200G\n      redundancyPolicy: \"SingleRedundancy\"\n  visualization:\n    type: \"kibana\"\n    kibana:\n      replicas: 1\n  curation:\n    type: \"curator\"\n    curator:\n      schedule: \"30 3 * * *\"\n  collection:\n    logs:\n      type: \"fluentd\"\n      fluentd: {}\n```\n\n\nAfter entering, press the **Create** button.  \n\n![cluster_logging_operator_9](./images/cluster_logging_operator_9_aws.png)\n\nNote that the default OpenShift cluster installation settings on AWS (IPI) does not have enough resources on workers. Therefore, we got the following errors on the OpenShift dashboard.\n![cluster_logging_operator_10](./images/cluster_logging_operator_10_aws.png)\nYou may also see that the PVC for the Elasticsearch logging are pending on the dashboard.\n\nTo solve this resouce shortage issues, we changed the worker nodes size from m4.large to m4.2xlarge. \nFYI. We tried m4.xlarge but it didn't large enough.\nHere are the commands we used to upgrade the worker nodes size.\n\n```\n$ oc get machines -n openshift-machine-api\nNAME                                     STATE     TYPE        REGION      ZONE         AGE\ncsmo1aws-fsdjv-master-0                  running   m4.xlarge   us-west-1   us-west-1b   5d12h\ncsmo1aws-fsdjv-master-1                  running   m4.xlarge   us-west-1   us-west-1c   5d12h\ncsmo1aws-fsdjv-master-2                  running   m4.xlarge   us-west-1   us-west-1b   5d12h\ncsmo1aws-fsdjv-worker-us-west-1b-gx8d2   running   m4.xlarge   us-west-1   us-west-1b   2d23h\ncsmo1aws-fsdjv-worker-us-west-1c-5sr2w   running   m4.xlarge   us-west-1   us-west-1c   2d23h\ncsmo1aws-fsdjv-worker-us-west-1c-x8gxr   running   m4.xlarge   us-west-1   us-west-1c   2d23h\n$\n\n$ oc patch machineset csmo1aws-fsdjv-worker-us-west-1b --type='merge' --patch='{\"spec\": { \"template\": { \"spec\": { \"providerSpec\": { \"value\": { \"instanceType\": \"m4.2xlarge\"}}}}}}' -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1b patched\n$ \n\n$ oc patch machineset csmo1aws-fsdjv-worker-us-west-1c --type='merge' --patch='{\"spec\": { \"template\": { \"spec\": { \"providerSpec\": { \"value\": { \"instanceType\": \"m4.2xlarge\"}}}}}}' -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1c patched\n$ \n\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1b --replicas=0 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1b scaled\n$ \n\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1b --replicas=1 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1b scaled\n$\n\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1c --replicas=0 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1c scaled\n$ \n\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1b --replicas=2 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1b scaled\n$\n\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1c --replicas=1 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1c scaled\n$\n\n$ oc get machinesets -n openshift-machine-api\nNAME                               DESIRED   CURRENT   READY   AVAILABLE   AGE\ncsmo1aws-fsdjv-worker-us-west-1b   2         2         2       2           5d13h\ncsmo1aws-fsdjv-worker-us-west-1c   1         1         1       1           5d13h\n$\n\n$ oc get machines -n openshift-machine-api\nNAME                                     STATE     TYPE         REGION      ZONE         AGE\ncsmo1aws-fsdjv-master-0                  running   m4.xlarge    us-west-1   us-west-1b   5d13h\ncsmo1aws-fsdjv-master-1                  running   m4.xlarge    us-west-1   us-west-1c   5d13h\ncsmo1aws-fsdjv-master-2                  running   m4.xlarge    us-west-1   us-west-1b   5d13h\ncsmo1aws-fsdjv-worker-us-west-1b-bwxw7   running   m4.2xlarge   us-west-1   us-west-1b   18m\ncsmo1aws-fsdjv-worker-us-west-1b-s6wsb   running   m4.2xlarge   us-west-1   us-west-1b   7m45s\ncsmo1aws-fsdjv-worker-us-west-1c-cf9r2   running   m4.2xlarge   us-west-1   us-west-1c   7m\n$ \n\n$ oc get nodes\nNAME                                         STATUS   ROLES    AGE     VERSION\nip-10-0-129-227.us-west-1.compute.internal   Ready    master   5d13h   v1.14.6+cebabbf4a\nip-10-0-138-212.us-west-1.compute.internal   Ready    worker   4m23s   v1.14.6+cebabbf4a\nip-10-0-139-19.us-west-1.compute.internal    Ready    master   5d13h   v1.14.6+cebabbf4a\nip-10-0-141-88.us-west-1.compute.internal    Ready    worker   15m     v1.14.6+cebabbf4a\nip-10-0-151-108.us-west-1.compute.internal   Ready    master   5d13h   v1.14.6+cebabbf4a\nip-10-0-158-42.us-west-1.compute.internal    Ready    worker   4m11s   v1.14.6+cebabbf4a\n$\n```\n\nAs a result, several components of EFK stack are created inside OpenShift.\nExecute the oc get command to confirm that the pod STATUS is **Running**.\n\n```\n$ oc get pod -n openshift-logging\nNAME                                            READY   STATUS    RESTARTS   AGE\ncluster-logging-operator-697dbb7bf9-dhjpc       1/1     Running   0          23m\nelasticsearch-cdm-9whip7ih-1-5846879467-mlmdw   2/2     Running   0          15m\nelasticsearch-cdm-9whip7ih-2-7b8f4bd55f-cb5cg   2/2     Running   0          14m\nelasticsearch-cdm-9whip7ih-3-b4c87df67-znw9x    2/2     Running   0          13m\nfluentd-2smx6                                   1/1     Running   6          4d23h\nfluentd-fgkhp                                   1/1     Running   6          4d23h\nfluentd-j6bnb                                   1/1     Running   0          30m\nfluentd-m82q2                                   1/1     Running   0          18m\nfluentd-td72t                                   1/1     Running   6          4d23h\nfluentd-z5h4j                                   1/1     Running   0          18m\nkibana-bc9b88996-7fh5r                          2/2     Running   0          23m\n$ \n```\nYou can find that the status of PVC on the dashboard became **Bound** as shown in the following picture.\n![cluster_logging_operator_11](./images/cluster_logging_operator_11_aws.png)\n\n## Access Kibana Dashboard\n\nAccess the created Kibana web console. You can check the public URL of Kibata from the [Networking]-> [Routes] menu of the OpenShift web console or by using the oc get route command.\n\n```\n$ oc get route -n openshift-logging\nNAME     HOST/PORT                                                 PATH   SERVICES   PORT    TERMINATION          WILDCARD\nkibana   kibana-openshift-logging.apps.csmo1aws.privatedomain.ml          kibana     <all>   reencrypt/Redirect   None\n$ \n```\n\nThe HOST / PORT for the **oc get route** command output will be the URL of the Kibana web console. You can also access the Kibana web console from OpenShift dashboard by navigating Administrator --> Monitoring --> Logging.  Note that the **Logging** option will not be shown until you configured the EFK on your OpenShift. In other words, by default, you don't see it on the menu.\nIf you can access it normally, the Kibana dashboard will be displayed and you can see that the log messages output on the OpenShiftCluster node are being collected as shown in below.\n\n![cluster_logging_operator_12](./images/cluster_logging_operator_12_aws.png)\n\n","type":"Mdx","contentDigest":"6b0b90c972f5d2f89729ee9fa48b1052","counter":554,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"OpenShift Platform Day2 - EFK installation on AWS","description":"OpenShift Day2 EFK on AWS","keywords":"OpenShift, day2, supplement, efk"},"exports":{},"rawBody":"---\ntitle: OpenShift Platform Day2 - EFK installation on AWS\ndescription: OpenShift Day2 EFK on AWS\nkeywords: 'OpenShift, day2, supplement, efk'\n---\n\n## Steps for EFK installation on OpenShift 4 on AWS\nWe will demonstrate how to install the EFK on OpenShift 4 cluster. As we described, the Operator is a key technology for the day 2 operation in OpenShift 4. We use the Operators to enable the EFK stack. For demonstration purpose, we install the Elasticsearch Operator via the oc CLI and the Cluster Logging Operator via Web Console.  \n\n## Install the Elasticsearch Operator\nAs mentioned, we will show you how to install the Elasticsearch Operator via the CLI.\n\n## Login to the OpenShift\nLog in to the OpenShift cluster with the oc login command. In the following execution example, enter _openshift api url_ according to your environment.\n\n```\n$ oc login https://<openshift api url>:6443/\n```\n\nHere is what we did in our cluster. Note that the value of -p which is the password of the user is not the actual one since I shouldn't put the real one here.  We also use the user \"kubeadmin\" in our example but it is not recommended. Instead, you should use your own user.\n\n```\n$ oc login -u kubeadmin -p abcabcabc https://api.csmo1aws.privatedomain.ml:6443\nLogin successful.\n\nYou have access to 51 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n$ \n```\n\n## Create a namespace to create an Elasticsearch Operator\nFirst, prepare a manifest file to create a namespace for creating an Elasticsearch Operator. As an example, create the file as follow.\n\n```\n$ vi eo-namespace.yaml\n$ cat eo-namespace.yaml \napiVersion: v1\nkind: Namespace\nmetadata:\n  name: openshift-operators-redhat\n  annotations:\n    openshift.io/node-selector: \"\"\n  labels:\n    openshift.io/cluster-logging: \"true\"\n    openshift.io/cluster-monitoring: \"true\"\n$  \n```\n\nNote that you can specify any namespace name for the value of \"name:\".  In our example, we specify \"openshift-operators-redhat\".\n\nThen create a namespace with the yaml file by runing the oc create command as follow.\n\n```\n$ oc create -f eo-namespace.yaml\nnamespace/openshift-operators-redhat created\n$  \n```\n\n## Create a namespace to create a Cluster Logging Operator\nPrepare a manifest file to create a namespace to create a Cluster Logging Operator. As an example, create the file as follow.\n\n```\n$ vi clo-namespace.yaml\n$ cat clo-namespace.yaml \napiVersion: v1\nkind: Namespace\nmetadata:\n  name: openshift-logging\n  annotations:\n    openshift.io/node-selector: \"\"\n  labels:\n    openshift.io/cluster-logging: \"true\"\n    openshift.io/cluster-monitoring: \"true\"\n$ \n```\n\nNote that you can specify any namespace name for the value of \"name:\".  In our example, we specify \"openshift-logging\".\n\nThen create a namespace with the yaml file by runing the oc create command as follow.\n\n```\n$ oc create -f clo-namespace.yaml\nnamespace/openshift-logging created\n$ \n```\n\n## Install the Elasticsearch Operator\nPrepare an Operator Group resource for Elasticsearch as follow.\n\n```\n$ vi eo-og.yaml\n$ cat eo-og.yaml \napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: openshift-operators-redhat\n  namespace: openshift-operators-redhat\nspec: {}\n$ \n```\nNote that for the value of the \"namespace:\", specify the namespace created for the Elasticsearch Operator namespace in the previous step.  In our example, we specify \"openshift-operators-redhat\".\n\nThen, create an Operator Group via oc create command as follow.\n\n```\n$ oc create -f eo-og.yaml\noperatorgroup.operators.coreos.com/openshift-operators-redhat created\n$ \n```\n\nUse the oc get command to check the channel name for subscribing the Operator. In the following execution example, you can confirm that the channel name is 4.2.\n\n```\n$ oc get packagemanifest elasticsearch-operator -n openshift-marketplace -o jsonpath='{.status.channels[].name}'\n4.2\n$\n```\n\nThe next step is to prepare the Subscription of the operator. Create a yaml file as follow.\n\n```\n$ vi eo-sub.yaml\n$ cat eo-sub.yaml \napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  generateName: \"elasticsearch-\"\n  namespace: \"openshift-operators-redhat\"\nspec:\n  channel: \"4.2\"\n  installPlanApproval: \"Automatic\"\n  source: \"redhat-operators\"\n  sourceNamespace: \"openshift-marketplace\"\n  name: \"elasticsearch-operator\"\n$ \n```\n\nNote that for the value of \"namespace:\", specify the one created in the previous step. In our example, we specify \"openshift-operators-redhat\".\n\nWith the yaml file, you create the subscription by issuing the oc create command as follow.\n\n```\n$ oc create -f eo-sub.yaml\nsubscription.operators.coreos.com/elasticsearch-724cq created\n$ \n```\n\nChange the current namespace by oc project command as follow.\n\n```\n$ oc project openshift-operators-redhat\nNow using project \"openshift-operators-redhat\" on server \"https://api.csmo1aws.privatedomain.ml:6443\".\n$  \n```\n\nAs the final step for the Elasticsearch operator installation, prepare an Operator RBAC object. Create the eo-rbac.yaml file as follow.\n\n```\n$ vi eo-rbac.yaml\n$ cat eo-rbac.yaml \napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: prometheus-k8s\n  namespace: openshift-operators-redhat\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: prometheus-k8s\n  namespace: openshift-operators-redhat\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: prometheus-k8s\nsubjects:\n- kind: ServiceAccount\n  name: prometheus-k8s\n  namespace: openshift-operators-redhat\n$ \n```\n\nThen, create an RBAC object using the oc create command as follow.\n\n```\n$ oc create -f eo-rbac.yaml\nrole.rbac.authorization.k8s.io/prometheus-k8s created\nrolebinding.rbac.authorization.k8s.io/prometheus-k8s created\n$ \n```\nNow, the Elasticsearch Operator installation is complete. Confirm that it is installed with the oc get command.\n```\n$ oc get pod -n openshift-operators-redhat\nNAME                                      READY   STATUS    RESTARTS   AGE\nelasticsearch-operator-8664584cbc-wq57s   1/1     Running   0          2m16s\n$ \n```\n\n\n## Install the Cluster Logging Operator  \n\nLogin with the OpenShift Web console and expand the **Operator** → **OperatorHub** menu.\nSearch for and select the Operator named “Cluster Logging” as shown in the following figure.  \n\n![cluster_logging_operator_1](./images/cluster_logging_operator_1_aws.png)\n\nClick the **Install** button in the **Cluster Logging** dialog displayed on the screen as shown in below.  \n\n![cluster_logging_operator_2](./images/cluster_logging_operator_2_aws.png)\n\nOn the **Create Operator Subscription** screen, select **openshift-logging** created in the previous step from the namespace drop-down list, and click the **Subscribe** button.\n\n![cluster_logging_operator_3](./images/cluster_logging_operator_3_aws.png)\n\nNow, we complete the **Cluster Logging Operator** installation steps.\n\n\n\n## Checking Operator installation status\nUp to this point, installation of Elasticsearch Operator and Cluster Logging Operator has been completed. Check if each Operator has been installed correctly.\n\n### Check Cluster Logging Operator\n\nSelect **Operators** -> **Installed Operators** from the Web console. At this time, perform the operation with the Project **openshift-logging** that the project where the **Cluster Logging** Operator is installed.\n\n![cluster_logging_operator_4](./images/cluster_logging_operator_4_aws.png)\n\nOn the Installed Operators page, check the **Cluster Logging** operator and find out its status which is **InstallSucceeded**.\n\n\n### Check Elasticsearch Operator\n\nChange the Project selection at the top of the center of the screen to the namespace where the **Elasticsearch Operator** is installed.  In our example, it is **openshift-opeartors-redhat** as shown in below.\n\n![cluster_logging_operator_5](./images/cluster_logging_operator_5_aws.png)\n\nOn the Installed Operators page, check the **Elasticsearch** operator and find out its status which is **InstallSucceeded**.\n\nIf there is no problem in any of the displays, you have confirmed the installation of Operators.\n\n## Creating a Cluster Logging instance\nFrom here, we will create a Pod that will be the substance of Cluster Logging.\n\n\n### Define Custom Resource Definition（CRD）\n\nSelect **Administration** -> **Custom Resource Definitions** from the Web console.\nEnter a string such as **ClusterLogging** in the filter at the top right of the screen. The **ClusterLogging** CRD is displayed. Then, click on **ClusterLogging**.\n\n![cluster_logging_operator_6](./images/cluster_logging_operator_6_aws.png)\n\n\nOn the **Custom Resource Definition Details** page, the **Overview** tab of the CRD **clusterloggings.logging.openshift.io** is displayed.\nNext, select **View Instances** from the **Actions** drop-down list in the upper right.\n\n![cluster_logging_operator_7](./images/cluster_logging_operator_7_aws.png)\n\nThe **Cluster Loggings** page is displayed. \n\n![cluster_logging_operator_8](./images/cluster_logging_operator_8_aws.png)\n\nClick the **Create Cluster Logging** button to open the edit screen of yaml for creating an instance and enter the following contents.\n\n```\napiVersion: \"logging.openshift.io/v1\"\nkind: \"ClusterLogging\"\nmetadata:\n  name: \"instance\"\n  namespace: \"openshift-logging\"\nspec:\n  managementState: \"Managed\"\n  logStore:\n    type: \"elasticsearch\"\n    elasticsearch:\n      nodeCount: 3\n      storage:\n        storageClassName: gp2\n        size: 200G\n      redundancyPolicy: \"SingleRedundancy\"\n  visualization:\n    type: \"kibana\"\n    kibana:\n      replicas: 1\n  curation:\n    type: \"curator\"\n    curator:\n      schedule: \"30 3 * * *\"\n  collection:\n    logs:\n      type: \"fluentd\"\n      fluentd: {}\n```\n\n\nAfter entering, press the **Create** button.  \n\n![cluster_logging_operator_9](./images/cluster_logging_operator_9_aws.png)\n\nNote that the default OpenShift cluster installation settings on AWS (IPI) does not have enough resources on workers. Therefore, we got the following errors on the OpenShift dashboard.\n![cluster_logging_operator_10](./images/cluster_logging_operator_10_aws.png)\nYou may also see that the PVC for the Elasticsearch logging are pending on the dashboard.\n\nTo solve this resouce shortage issues, we changed the worker nodes size from m4.large to m4.2xlarge. \nFYI. We tried m4.xlarge but it didn't large enough.\nHere are the commands we used to upgrade the worker nodes size.\n\n```\n$ oc get machines -n openshift-machine-api\nNAME                                     STATE     TYPE        REGION      ZONE         AGE\ncsmo1aws-fsdjv-master-0                  running   m4.xlarge   us-west-1   us-west-1b   5d12h\ncsmo1aws-fsdjv-master-1                  running   m4.xlarge   us-west-1   us-west-1c   5d12h\ncsmo1aws-fsdjv-master-2                  running   m4.xlarge   us-west-1   us-west-1b   5d12h\ncsmo1aws-fsdjv-worker-us-west-1b-gx8d2   running   m4.xlarge   us-west-1   us-west-1b   2d23h\ncsmo1aws-fsdjv-worker-us-west-1c-5sr2w   running   m4.xlarge   us-west-1   us-west-1c   2d23h\ncsmo1aws-fsdjv-worker-us-west-1c-x8gxr   running   m4.xlarge   us-west-1   us-west-1c   2d23h\n$\n\n$ oc patch machineset csmo1aws-fsdjv-worker-us-west-1b --type='merge' --patch='{\"spec\": { \"template\": { \"spec\": { \"providerSpec\": { \"value\": { \"instanceType\": \"m4.2xlarge\"}}}}}}' -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1b patched\n$ \n\n$ oc patch machineset csmo1aws-fsdjv-worker-us-west-1c --type='merge' --patch='{\"spec\": { \"template\": { \"spec\": { \"providerSpec\": { \"value\": { \"instanceType\": \"m4.2xlarge\"}}}}}}' -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1c patched\n$ \n\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1b --replicas=0 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1b scaled\n$ \n\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1b --replicas=1 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1b scaled\n$\n\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1c --replicas=0 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1c scaled\n$ \n\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1b --replicas=2 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1b scaled\n$\n\n$ oc scale machineset csmo1aws-fsdjv-worker-us-west-1c --replicas=1 -n openshift-machine-api\nmachineset.machine.openshift.io/csmo1aws-fsdjv-worker-us-west-1c scaled\n$\n\n$ oc get machinesets -n openshift-machine-api\nNAME                               DESIRED   CURRENT   READY   AVAILABLE   AGE\ncsmo1aws-fsdjv-worker-us-west-1b   2         2         2       2           5d13h\ncsmo1aws-fsdjv-worker-us-west-1c   1         1         1       1           5d13h\n$\n\n$ oc get machines -n openshift-machine-api\nNAME                                     STATE     TYPE         REGION      ZONE         AGE\ncsmo1aws-fsdjv-master-0                  running   m4.xlarge    us-west-1   us-west-1b   5d13h\ncsmo1aws-fsdjv-master-1                  running   m4.xlarge    us-west-1   us-west-1c   5d13h\ncsmo1aws-fsdjv-master-2                  running   m4.xlarge    us-west-1   us-west-1b   5d13h\ncsmo1aws-fsdjv-worker-us-west-1b-bwxw7   running   m4.2xlarge   us-west-1   us-west-1b   18m\ncsmo1aws-fsdjv-worker-us-west-1b-s6wsb   running   m4.2xlarge   us-west-1   us-west-1b   7m45s\ncsmo1aws-fsdjv-worker-us-west-1c-cf9r2   running   m4.2xlarge   us-west-1   us-west-1c   7m\n$ \n\n$ oc get nodes\nNAME                                         STATUS   ROLES    AGE     VERSION\nip-10-0-129-227.us-west-1.compute.internal   Ready    master   5d13h   v1.14.6+cebabbf4a\nip-10-0-138-212.us-west-1.compute.internal   Ready    worker   4m23s   v1.14.6+cebabbf4a\nip-10-0-139-19.us-west-1.compute.internal    Ready    master   5d13h   v1.14.6+cebabbf4a\nip-10-0-141-88.us-west-1.compute.internal    Ready    worker   15m     v1.14.6+cebabbf4a\nip-10-0-151-108.us-west-1.compute.internal   Ready    master   5d13h   v1.14.6+cebabbf4a\nip-10-0-158-42.us-west-1.compute.internal    Ready    worker   4m11s   v1.14.6+cebabbf4a\n$\n```\n\nAs a result, several components of EFK stack are created inside OpenShift.\nExecute the oc get command to confirm that the pod STATUS is **Running**.\n\n```\n$ oc get pod -n openshift-logging\nNAME                                            READY   STATUS    RESTARTS   AGE\ncluster-logging-operator-697dbb7bf9-dhjpc       1/1     Running   0          23m\nelasticsearch-cdm-9whip7ih-1-5846879467-mlmdw   2/2     Running   0          15m\nelasticsearch-cdm-9whip7ih-2-7b8f4bd55f-cb5cg   2/2     Running   0          14m\nelasticsearch-cdm-9whip7ih-3-b4c87df67-znw9x    2/2     Running   0          13m\nfluentd-2smx6                                   1/1     Running   6          4d23h\nfluentd-fgkhp                                   1/1     Running   6          4d23h\nfluentd-j6bnb                                   1/1     Running   0          30m\nfluentd-m82q2                                   1/1     Running   0          18m\nfluentd-td72t                                   1/1     Running   6          4d23h\nfluentd-z5h4j                                   1/1     Running   0          18m\nkibana-bc9b88996-7fh5r                          2/2     Running   0          23m\n$ \n```\nYou can find that the status of PVC on the dashboard became **Bound** as shown in the following picture.\n![cluster_logging_operator_11](./images/cluster_logging_operator_11_aws.png)\n\n## Access Kibana Dashboard\n\nAccess the created Kibana web console. You can check the public URL of Kibata from the [Networking]-> [Routes] menu of the OpenShift web console or by using the oc get route command.\n\n```\n$ oc get route -n openshift-logging\nNAME     HOST/PORT                                                 PATH   SERVICES   PORT    TERMINATION          WILDCARD\nkibana   kibana-openshift-logging.apps.csmo1aws.privatedomain.ml          kibana     <all>   reencrypt/Redirect   None\n$ \n```\n\nThe HOST / PORT for the **oc get route** command output will be the URL of the Kibana web console. You can also access the Kibana web console from OpenShift dashboard by navigating Administrator --> Monitoring --> Logging.  Note that the **Logging** option will not be shown until you configured the EFK on your OpenShift. In other words, by default, you don't see it on the menu.\nIf you can access it normally, the Kibana dashboard will be displayed and you can see that the log messages output on the OpenShiftCluster node are being collected as shown in below.\n\n![cluster_logging_operator_12](./images/cluster_logging_operator_12_aws.png)\n\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/day2/Logging/EFK_AWS/index.mdx"}}}}