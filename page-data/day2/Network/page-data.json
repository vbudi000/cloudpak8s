{"componentChunkName":"component---src-pages-day-2-network-index-mdx","path":"/day2/Network/","result":{"pageContext":{"frontmatter":{"title":"OpenShift Platform Day2 - Network","description":"OpenShift Day2 Network","keywords":"OpenShift, day2, network"},"relativePagePath":"/day2/Network/index.mdx","titleType":"page","MdxNode":{"id":"baaf3a31-7c5d-514e-aeb9-f2cccf57a9f9","children":[],"parent":"3d8ef277-539f-5b27-96eb-6ad4fc10f773","internal":{"content":"---\ntitle: OpenShift Platform Day2 - Network\ndescription: OpenShift Day2 Network\nkeywords: 'OpenShift, day2, network'\n---\n\n## Network Overview\nOnce an OpenShift Container Platform cluster was installed, from a networking point of view, things would just work as expected in a Kubernetes environment. If you create and deploy an application through the out-of-the-box OpenShift's _Project_, _BuildConfig_, and _DeployConfig_ resources, then most resources will be deployed for you by OpenShift. Your application should be accessible from the internal network through the service that OpenShift created for you.  If you want to expose the service outside of the cluster, then you create the route resource. In other words, things just work. Most OpenShift users do not need to spend time fiddling with the network setup of OpenShift.\n\nAs you expand the usage of OpenShift, especially by putting more of the enterprise workload onto it, there is normally a requirement to start modifying your network configuration.  For example, you might have a multi-tenancy requirement. The security team may demand that certain applications need to be run on its own network.  You are running a multi-cluster environment, and the cluster is hosted at a different cloud provider with different network configurations, and you want to standardize them.  All these are an example that necessitates you to modify the network configuration of OpenShift.  The redesign and implementation of these is Day 0 and Day 1 activities.  However, as Day 2 now, you are left with a more complex network configuration to support.  \n\n## Day 1 Platform\nKeep the design (Day 0) and installation note (Day 1) handy to support the Day 2 operations.  In particular:\n- The architecture of the cluster.  How many master and worker nodes?\n- Sizing consideration, especially those involving network utilization.\n- Is there any network policies defined? Has all the network policies been implemented?\n\n> To be able to perform the Day 2 activities on the OpenShift cluster, you need a good understanding of how the OpenShift network is implemented.  There are 3 Operators responsible for managing your OpenShift cluster.  \n\n## Day 2 Platform\nThe following lists the sections that will be covered in this Networking chapter:\n- [Implementing network policies](#net-policies)\n  - [Setting up your project default to close of external traffic](#project-net-policies)\n  - [Using multitenant isolation mode of OpenshiftSDN](#project-sdn-isolation)\n- [Configuring OpenShift router](#router-config)\n  - [Configuring the ingress operator to use a particular certificate](#certificates)\n  - [Scaling the number of ingress controller replicas](#ingress-replica)\n  - [Configuring route persistance cookie's name](#cookie)\n- [Troubleshooting: Network packet fragmentation due to incorrect MTU size](#mtu)\n- [Using other network operator from OperatorHub](#operatorhub)\n\n## Day 1 Application\nThe out of the box Prometheus configuration concentrates on providing metrics on the compute components of OpenShift such as CPU, Memory, and Persistent Volume Claim.  If the business requires it, such as you need to provide metering data on network usage, then you might need to __set up Prometheus to collect network metrics__.\n\n## Day 2 Application\nNetwork configuration is mostly a platform related activity. To support the Operational Requirement or Application requirement, you may need to configure the OpenShift's network operators.  These are:\n- [Operational Requirement of needing a specific egress IP address](#egress)\n- [Operational Requirement of more than one networks](#more-than-one)\n\n\n## Mapping to Personas\n\nPersona | task\n--- | ---\nSRE, DevOps Engineer | Implementing network policies\nSRE, DevOps Engineer | Configuring OpenShift router\nSRE, DevOps Engineer | Using other network operator from OperatorHub\nSRE | Troubleshooting: Network packet fragmentation due to incorrect MTU size\nSRE, DevOps Engineer | Operational Requirement of needing a specific egress IP address\nSRE, DevOps Engineer | Operational Requirement of more than one networks\n\n\n## Platform tasks\n\n<a name=\"net-policies\"></a>\n\n## Implementing network policies: [SRE, DevOps Engineer]\nIn a fresh newly-installed OpenShift, pods are non-isolated.  Pods accept traffic from any source that has the route to it, so that means pretty much every other pod in the cluster.\n\nYou can isolate a pod by defining a NetworkPolicy.  Before you configure any network policies, you need to make sure that your OpenShift network plugin supports it.  You implement network policy by specifying the policy into the resource, however: \n> Creating a NetworkPolicy specification without a network plugin that implements it, will have no effect.\n\nDuring the installation of OpenShift, the `network.config` controller has defined the `OpenShiftSDN` network plugin, so we can use this NetworkPolicy.   \n\nNetwork policies are additive; it means the order of evaluation does not affect the policy result.\n\nNetwork policy creation and deletion are detailed in the OpenShift Manual on Configuring network policy with OpenShift SDN:  \nhttps://docs.openshift.com/container-platform/4.3/networking/configuring-networkpolicy.html    \n\n\n<a name=\"project-net-policies\"></a>\n\n### Setting up your project default to close of external traffic using network policy\n\nThe chapter on [Build and Deploy](../BuildDeploy) mentioned that OpenShift recommends the user to build their application using the project construct.  The project provides you with pre-configured resources, such as BuildConfig, DeployConfig, and Services.  The default configuration of the project is not to expose the service to the outside world.  To do that, you need to add the route to the service.  \n\nYou might want to add standard network policies to a project's automation. One example will be to implement a network policy to close the project build result for external traffic. Doing this will ensure that any project created in the environment will be isolated and secured. \n\nThe OpenShift documentation provides the steps for Creating default network policies for a new project:  https://docs.openshift.com/container-platform/4.3/networking/configuring-networkpolicy.html#nw-networkpolicy-creating-default-networkpolicy-objects-for-a-new-project  \n\n\n\n<a name=\"project-sdn-isolation\"></a>\n\n### Using multitenant isolation mode of OpenshiftSDN\nWe have just seen that you can create project isolation using network policy.  OpenshiftSDN network plugin also offers a `multitenant isolation` mode.  In this mode, by default, the pods of the project are isolated to the pods from other projects.\n\nTo configure network isolation on a project:\n```\n$ oc adm pod-network isolate-projects <project1>\n```\n\nTo disable network isolation:\n```\noc adm pod-network make-projects-global <project1>\n```\nMore information can be found in the OpenShift documentation:  \nhttps://docs.openshift.com/container-platform/4.3/networking/openshift_sdn/multitenant-isolation.html   \n\n\n\n<a name=\"router-config\"></a>\n\n## Configuring OpenShift router: [SRE, DevOps Engineer]\nSimilar to other networking components, the router function provided by the Ingress operator will work without too much configuration.  However, the operational requirement may necessitate further configuration on the router.  The following are some of the common ones.\n\n<a name=\"certificates\"></a>\n\n### Configuring the ingress operator to use a particular certificate\nThe operation requirement may dictate to use of a particular certificate for your ingress controller.  The following are steps to import the certificate into the ingress operator:\n- Get the certificate into your workspace where you run the oc command.  Assumes the filename for this example is tls.crt and tls.key.\n- Create a `secret` resource in the `openshift-ingress` namespace providing the certificate file.\n```\n$ oc --namespace openshift-ingress create secret tls custom-certs-default --cert=tls.crt --key=tls.key\n```\n- Update the Ingress controller using the `oc patch.`\n\n```\n$ oc patch --type=merge --namespace openshift-ingress-operator ingresscontrollers/default \\\n  --patch '{\"spec\":{\"defaultCertificate\":{\"name\":\"custom-certs-default\"}}}'\n```\n\n- Verify\n\n```\n$ oc get --namespace openshift-ingress-operator ingresscontrollers/default \\\n  --output jsonpath='{.spec.defaultCertificate}'\n```  \n\nMore information can be found in the OpenShift documentation:  \nhttps://docs.openshift.com/container-platform/4.3/networking/ingress-operator.html#nw-ingress-setting-a-custom-default-certificate_configuring-ingress     \n\n\n<a name=\"ingress-replica\"></a>\n\n### Scaling the number of ingress controller replicas\nYou can scale up or down the number of ingress controller.\n\n- View the number of replicas\n```\n$ oc get -n openshift-ingress-operator ingresscontrollers/default -o jsonpath='{$.status.availableReplicas}'\n```\n- Change the number of replicas, for example to set it to 3 you can perform the following command.\n```\n$ oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{\"spec\":{\"replicas\": 3}}' --type=merge\n\n```   \n\nMore information can be found in the OpenShift documentation:  \nhttps://docs.openshift.com/container-platform/4.3/networking/ingress-operator.html#nw-ingress-controller-configuration_configuring-ingress   \n\n\n<a name=\"cookie\"></a>\n\n### Configuring route persistance cookie's name\nOne of the common requirements in setting up a clustered client-server architecture with a load-balancer is to configure the route persistence on the load-balancer.  In OpenShift, from an external client software connecting to a server that is an application running inside OpenShift, this load balancer is the ingress controller/router.  The client software needs to be able to access the same application's replica for continuous client-server operation.  This is normally called a _sticky session_.\n\nOpenShift implement this by using a session cookie with a randomly generated cookie name.  A cookie is generated by the ingress controller and is sent back to the external application.  The next time the external application communicates to OpenShift, it needs to include the cookie so the ingress controller can route the traffic correctly.  For security reasons, the client software might require the interface to use a certain cookie name.  To setup OpenShift to use a certain cookie name, you can perform the following:\n\n```\n$ oc annotate route <route_name> router.openshift.io/<cookie_name>=\"-<cookie_annotation>\"\n```   \n\nMore information can be found in the OpenShift documentation:  \nhttps://docs.openshift.com/container-platform/4.3/networking/routes/route-configuration.html#nw-using-cookies-keep-route-statefulness_route-configuration   \n\n\n<a name=\"operatorhub\"></a>\n\n## Using other network operator from OperatorHub: [SRE, DevOps Engineer]\nYou can change or extend your network configuration by making use of some of the operators available at the OperatorHub.  To access this login to your web console as a user with cluster-admin privilege and access the menu `Operators > Operator Hub > Networking`.  The following shows the available operators provided by Red Hat:\n\n<a href=\"./images/net_ops.png\"><img src=\"./images/net_ops.png\" alt=\"Networking Operator\" title=\"Network Operator\" width=\"700\" align=\"center\"/></a>\n\nLet's say that you have a requirement to extend your network configuration that allows Pods to attach to a virtual function (VF) interface on SR-IOV capable hardware on the host system.  One way to support this requirement is to install the `SR-IOV Network Operator` from the OperatorHub catalog above.\n\n<a name=\"mtu\"></a>\n\n## Troubleshooting: Network packet fragmentation due to incorrect MTU size: [SRE]\nYou suppose to have a good network, yet you feel that your OpenShift network performance is sluggish, and this is supported by the Prometheus network metrics history, what could be the issue?  One possible cause is network packet fragmentation due to incorrect MTU size.  Setting up the MTU is a Day 0 and Day 1 activity; however, if you have more than one cluster hosted at a different cloud provider, there is a chance that the MTU is not set correctly as different cloud providers might need you to set the MTU differently.  If you change your network overlay from VXLAN to IP-in-IP, then you will also need to change your MTU size.\n\n\nYou can change the MTU by changing the `cluster` instance of the `network.config` resources by using the `oc patch` command.  \n  \nThe following is an article from an older version of OpenShift documentation about [potential issue on accessing SSL certificate that might be caused by incorrect MTU setting](https://docs.openshift.com/container-platform/3.7/day_two_guide/environment_health_checks.html#day-two-guide-verifying_mtu).  The article can provide you with additional troubleshooting options when you encounter similar problems.\n\n\n## Application tasks\n\n\n<a name=\"egress\"></a>\n\n## Operational Requirement of needing a specific egress IP address: [SRE, DevOps Engineer]\nThere are many ways that applications communicate with each other.  For security reasons, as part of the interface specifications/agreement, an external application may expect traffic coming from a certain IP address.  Thus the requirement is, can we configure our project so that the traffic coming from our pods is seen by the external application as coming from a specific IP address?  To do this, you need to configure the egress IP address of the project.\n\nThere are two steps involved in assigning an egress IP address, and this is similar in a way on how a pod gets an IP address from the node assigned IP address range:\n- First, you assigned an egress IP address range to the node.\n- Then you specify the egress IP address from that range to the pod.\n\nExample:\n```\n# Assign the IP address range using CIDR notation to the node.\n$ oc patch hostsubnet node1 --type=merge -p '{\"egressCIDRs\": [\"192.168.1.0/24\"]}'\n\n# Assign the IP address range to the project from the range of IP address of the node.\n$ oc patch netnamespace project1 --type=merge -p '{\"egressIPs\": [\"192.168.1.100\"]}'\n\n```\n\nThe above is the OpenShift preferred approach.  There are other options available, and you can read them from the OpenShift documentation:  \nhttps://docs.openshift.com/container-platform/4.3/networking/openshift_sdn/assigning-egress-ips.html  \n\n\n\n<a name=\"more-than-one\"></a>\n\n## Operational Requirement of more than one network: [SRE, DevOps Engineer]\nSome enterprises might have a requirement to run some pods on a separate network outside the default network.  For performance reasons, some enterprises might also have a requirement for a set of pods to run on a separate network.  If you have this requirement, then you need to introduce the additional network to your cluster.\n\nDepending on the requirement, you can add an additional network through the network plugin. To provide multiple networks, OpenShift uses [the Multus Container Network Interface(CNI) plugin](https://github.com/intel/multus-cni). You can add the additional network of the following types:\n- __Bridge__.\n- __Host-device__.\n- __MAC-VLAN__, where a single physical interface of the host can have multiple mac and ip addresses.\n- __IP-VLAN__, where each endpoint gets the same mac address but different ip address.\n- __SR-IOV__ (Single-Root Input/Output Virtualization) \n\nEach of these additional networks is described in the OpenShift documentation on multiple networks:  \nhttps://docs.openshift.com/container-platform/4.3/networking/multiple_networks/understanding-multiple-networks.html\n\n\n\n## Implementing Networking\n\nThe following are links to more information on implementing networking.\n\n## Kubernetes\n- [Kubernetes network policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)\n\n## OpenShift\n- The sample OC CLI commands and Web console screen shots in the previous sections are for OpenShift implementation.\n- [Cluster Network Operator on Git](https://github.com/openshift/cluster-network-operator)\n\n## On IBM Cloud (Managed OpenShift)\nIf you are using Managed OpenShift on IBM Cloud, you will see some differences between what we have described in the previous sections and how IBM Cloud implemented the network.  Please read IBM Cloud official documents to understand those differences.\n- [Configuring subnets and IP addresses](https://cloud.ibm.com/docs/openshift?topic=openshift-subnets)\n- [Changing service endpoints or VLAN connections](https://cloud.ibm.com/docs/openshift?topic=openshift-cs_network_cluster)\n- [Opening required ports and IP addresses in your firewall](https://cloud.ibm.com/docs/openshift?topic=openshift-firewall)\n- [About Ingress in OpenShift version 4.3](https://cloud.ibm.com/docs/openshift?topic=openshift-ingress-about-roks4)\n- [Restricting network traffic to edge worker nodes](https://cloud.ibm.com/docs/openshift?topic=openshift-edge)\n- [Controlling traffic with network policies](https://cloud.ibm.com/docs/openshift?topic=openshift-network_policies)\n\n## With IBM Cloud Pak for MCM\n\n- [Specifying policy using IBM CloudPak for Multi-Cluster Management](../../mcm/cp4mcm_governance_risk/)\n\n## Others\n\nThe following are some of the Kubernetes network implementations:\n\n- AWS VPC CNI for Kubernetes. [AWS VPC CNI](https://github.com/aws/amazon-vpc-cni-k8s) is a networking plugin repository for pod networking in Kubernetes using Elastic Network Interfaces on AWS (Amazon Web Service).\n- Azure CNI [Azure CNI](https://docs.microsoft.com/en-us/azure/virtual-network/container-networking-overview) is an open-source plugin that enables containers to use Azure Virtual Network capabilities\n- Calico.  [Project Calico](https://docs.projectcalico.org/introduction) is an open-source networking and network security solution for containers, virtual machines, and native host-based workloads. It was used by the IBM Container Platform 2.x and 3.x for its networking component.\n- kube-ovn.  In its git repository [kube-ovn](https://github.com/alauda/kube-ovn) describes itself as a Kubernetes Network Fabric for Enterprises that Rich in Functions and Easy in Operations \n- Open vSwitch. [Open vSwitch](https://www.openvswitch.org/) is a multilayer virtual switch licensed under the open-source Apache 2.0 license.\n- Open Virtual Networking (OVN). It is an open source network virtualization solution developed by the Open vSwitch community.  Please refer to [the Kubernetes plugin and its documentation for OVN](https://github.com/ovn-org/ovn-kubernetes)\n\n## Other consideration\n\n- [OpenShift and Network Security Zones: Coexistence Approaches](https://blog.openshift.com/openshift-and-network-security-zones-coexistence-approaches/)\n","type":"Mdx","contentDigest":"3832407afe2adb7cdb5a9a59903d34b8","counter":546,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"OpenShift Platform Day2 - Network","description":"OpenShift Day2 Network","keywords":"OpenShift, day2, network"},"exports":{},"rawBody":"---\ntitle: OpenShift Platform Day2 - Network\ndescription: OpenShift Day2 Network\nkeywords: 'OpenShift, day2, network'\n---\n\n## Network Overview\nOnce an OpenShift Container Platform cluster was installed, from a networking point of view, things would just work as expected in a Kubernetes environment. If you create and deploy an application through the out-of-the-box OpenShift's _Project_, _BuildConfig_, and _DeployConfig_ resources, then most resources will be deployed for you by OpenShift. Your application should be accessible from the internal network through the service that OpenShift created for you.  If you want to expose the service outside of the cluster, then you create the route resource. In other words, things just work. Most OpenShift users do not need to spend time fiddling with the network setup of OpenShift.\n\nAs you expand the usage of OpenShift, especially by putting more of the enterprise workload onto it, there is normally a requirement to start modifying your network configuration.  For example, you might have a multi-tenancy requirement. The security team may demand that certain applications need to be run on its own network.  You are running a multi-cluster environment, and the cluster is hosted at a different cloud provider with different network configurations, and you want to standardize them.  All these are an example that necessitates you to modify the network configuration of OpenShift.  The redesign and implementation of these is Day 0 and Day 1 activities.  However, as Day 2 now, you are left with a more complex network configuration to support.  \n\n## Day 1 Platform\nKeep the design (Day 0) and installation note (Day 1) handy to support the Day 2 operations.  In particular:\n- The architecture of the cluster.  How many master and worker nodes?\n- Sizing consideration, especially those involving network utilization.\n- Is there any network policies defined? Has all the network policies been implemented?\n\n> To be able to perform the Day 2 activities on the OpenShift cluster, you need a good understanding of how the OpenShift network is implemented.  There are 3 Operators responsible for managing your OpenShift cluster.  \n\n## Day 2 Platform\nThe following lists the sections that will be covered in this Networking chapter:\n- [Implementing network policies](#net-policies)\n  - [Setting up your project default to close of external traffic](#project-net-policies)\n  - [Using multitenant isolation mode of OpenshiftSDN](#project-sdn-isolation)\n- [Configuring OpenShift router](#router-config)\n  - [Configuring the ingress operator to use a particular certificate](#certificates)\n  - [Scaling the number of ingress controller replicas](#ingress-replica)\n  - [Configuring route persistance cookie's name](#cookie)\n- [Troubleshooting: Network packet fragmentation due to incorrect MTU size](#mtu)\n- [Using other network operator from OperatorHub](#operatorhub)\n\n## Day 1 Application\nThe out of the box Prometheus configuration concentrates on providing metrics on the compute components of OpenShift such as CPU, Memory, and Persistent Volume Claim.  If the business requires it, such as you need to provide metering data on network usage, then you might need to __set up Prometheus to collect network metrics__.\n\n## Day 2 Application\nNetwork configuration is mostly a platform related activity. To support the Operational Requirement or Application requirement, you may need to configure the OpenShift's network operators.  These are:\n- [Operational Requirement of needing a specific egress IP address](#egress)\n- [Operational Requirement of more than one networks](#more-than-one)\n\n\n## Mapping to Personas\n\nPersona | task\n--- | ---\nSRE, DevOps Engineer | Implementing network policies\nSRE, DevOps Engineer | Configuring OpenShift router\nSRE, DevOps Engineer | Using other network operator from OperatorHub\nSRE | Troubleshooting: Network packet fragmentation due to incorrect MTU size\nSRE, DevOps Engineer | Operational Requirement of needing a specific egress IP address\nSRE, DevOps Engineer | Operational Requirement of more than one networks\n\n\n## Platform tasks\n\n<a name=\"net-policies\"></a>\n\n## Implementing network policies: [SRE, DevOps Engineer]\nIn a fresh newly-installed OpenShift, pods are non-isolated.  Pods accept traffic from any source that has the route to it, so that means pretty much every other pod in the cluster.\n\nYou can isolate a pod by defining a NetworkPolicy.  Before you configure any network policies, you need to make sure that your OpenShift network plugin supports it.  You implement network policy by specifying the policy into the resource, however: \n> Creating a NetworkPolicy specification without a network plugin that implements it, will have no effect.\n\nDuring the installation of OpenShift, the `network.config` controller has defined the `OpenShiftSDN` network plugin, so we can use this NetworkPolicy.   \n\nNetwork policies are additive; it means the order of evaluation does not affect the policy result.\n\nNetwork policy creation and deletion are detailed in the OpenShift Manual on Configuring network policy with OpenShift SDN:  \nhttps://docs.openshift.com/container-platform/4.3/networking/configuring-networkpolicy.html    \n\n\n<a name=\"project-net-policies\"></a>\n\n### Setting up your project default to close of external traffic using network policy\n\nThe chapter on [Build and Deploy](../BuildDeploy) mentioned that OpenShift recommends the user to build their application using the project construct.  The project provides you with pre-configured resources, such as BuildConfig, DeployConfig, and Services.  The default configuration of the project is not to expose the service to the outside world.  To do that, you need to add the route to the service.  \n\nYou might want to add standard network policies to a project's automation. One example will be to implement a network policy to close the project build result for external traffic. Doing this will ensure that any project created in the environment will be isolated and secured. \n\nThe OpenShift documentation provides the steps for Creating default network policies for a new project:  https://docs.openshift.com/container-platform/4.3/networking/configuring-networkpolicy.html#nw-networkpolicy-creating-default-networkpolicy-objects-for-a-new-project  \n\n\n\n<a name=\"project-sdn-isolation\"></a>\n\n### Using multitenant isolation mode of OpenshiftSDN\nWe have just seen that you can create project isolation using network policy.  OpenshiftSDN network plugin also offers a `multitenant isolation` mode.  In this mode, by default, the pods of the project are isolated to the pods from other projects.\n\nTo configure network isolation on a project:\n```\n$ oc adm pod-network isolate-projects <project1>\n```\n\nTo disable network isolation:\n```\noc adm pod-network make-projects-global <project1>\n```\nMore information can be found in the OpenShift documentation:  \nhttps://docs.openshift.com/container-platform/4.3/networking/openshift_sdn/multitenant-isolation.html   \n\n\n\n<a name=\"router-config\"></a>\n\n## Configuring OpenShift router: [SRE, DevOps Engineer]\nSimilar to other networking components, the router function provided by the Ingress operator will work without too much configuration.  However, the operational requirement may necessitate further configuration on the router.  The following are some of the common ones.\n\n<a name=\"certificates\"></a>\n\n### Configuring the ingress operator to use a particular certificate\nThe operation requirement may dictate to use of a particular certificate for your ingress controller.  The following are steps to import the certificate into the ingress operator:\n- Get the certificate into your workspace where you run the oc command.  Assumes the filename for this example is tls.crt and tls.key.\n- Create a `secret` resource in the `openshift-ingress` namespace providing the certificate file.\n```\n$ oc --namespace openshift-ingress create secret tls custom-certs-default --cert=tls.crt --key=tls.key\n```\n- Update the Ingress controller using the `oc patch.`\n\n```\n$ oc patch --type=merge --namespace openshift-ingress-operator ingresscontrollers/default \\\n  --patch '{\"spec\":{\"defaultCertificate\":{\"name\":\"custom-certs-default\"}}}'\n```\n\n- Verify\n\n```\n$ oc get --namespace openshift-ingress-operator ingresscontrollers/default \\\n  --output jsonpath='{.spec.defaultCertificate}'\n```  \n\nMore information can be found in the OpenShift documentation:  \nhttps://docs.openshift.com/container-platform/4.3/networking/ingress-operator.html#nw-ingress-setting-a-custom-default-certificate_configuring-ingress     \n\n\n<a name=\"ingress-replica\"></a>\n\n### Scaling the number of ingress controller replicas\nYou can scale up or down the number of ingress controller.\n\n- View the number of replicas\n```\n$ oc get -n openshift-ingress-operator ingresscontrollers/default -o jsonpath='{$.status.availableReplicas}'\n```\n- Change the number of replicas, for example to set it to 3 you can perform the following command.\n```\n$ oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{\"spec\":{\"replicas\": 3}}' --type=merge\n\n```   \n\nMore information can be found in the OpenShift documentation:  \nhttps://docs.openshift.com/container-platform/4.3/networking/ingress-operator.html#nw-ingress-controller-configuration_configuring-ingress   \n\n\n<a name=\"cookie\"></a>\n\n### Configuring route persistance cookie's name\nOne of the common requirements in setting up a clustered client-server architecture with a load-balancer is to configure the route persistence on the load-balancer.  In OpenShift, from an external client software connecting to a server that is an application running inside OpenShift, this load balancer is the ingress controller/router.  The client software needs to be able to access the same application's replica for continuous client-server operation.  This is normally called a _sticky session_.\n\nOpenShift implement this by using a session cookie with a randomly generated cookie name.  A cookie is generated by the ingress controller and is sent back to the external application.  The next time the external application communicates to OpenShift, it needs to include the cookie so the ingress controller can route the traffic correctly.  For security reasons, the client software might require the interface to use a certain cookie name.  To setup OpenShift to use a certain cookie name, you can perform the following:\n\n```\n$ oc annotate route <route_name> router.openshift.io/<cookie_name>=\"-<cookie_annotation>\"\n```   \n\nMore information can be found in the OpenShift documentation:  \nhttps://docs.openshift.com/container-platform/4.3/networking/routes/route-configuration.html#nw-using-cookies-keep-route-statefulness_route-configuration   \n\n\n<a name=\"operatorhub\"></a>\n\n## Using other network operator from OperatorHub: [SRE, DevOps Engineer]\nYou can change or extend your network configuration by making use of some of the operators available at the OperatorHub.  To access this login to your web console as a user with cluster-admin privilege and access the menu `Operators > Operator Hub > Networking`.  The following shows the available operators provided by Red Hat:\n\n<a href=\"./images/net_ops.png\"><img src=\"./images/net_ops.png\" alt=\"Networking Operator\" title=\"Network Operator\" width=\"700\" align=\"center\"/></a>\n\nLet's say that you have a requirement to extend your network configuration that allows Pods to attach to a virtual function (VF) interface on SR-IOV capable hardware on the host system.  One way to support this requirement is to install the `SR-IOV Network Operator` from the OperatorHub catalog above.\n\n<a name=\"mtu\"></a>\n\n## Troubleshooting: Network packet fragmentation due to incorrect MTU size: [SRE]\nYou suppose to have a good network, yet you feel that your OpenShift network performance is sluggish, and this is supported by the Prometheus network metrics history, what could be the issue?  One possible cause is network packet fragmentation due to incorrect MTU size.  Setting up the MTU is a Day 0 and Day 1 activity; however, if you have more than one cluster hosted at a different cloud provider, there is a chance that the MTU is not set correctly as different cloud providers might need you to set the MTU differently.  If you change your network overlay from VXLAN to IP-in-IP, then you will also need to change your MTU size.\n\n\nYou can change the MTU by changing the `cluster` instance of the `network.config` resources by using the `oc patch` command.  \n  \nThe following is an article from an older version of OpenShift documentation about [potential issue on accessing SSL certificate that might be caused by incorrect MTU setting](https://docs.openshift.com/container-platform/3.7/day_two_guide/environment_health_checks.html#day-two-guide-verifying_mtu).  The article can provide you with additional troubleshooting options when you encounter similar problems.\n\n\n## Application tasks\n\n\n<a name=\"egress\"></a>\n\n## Operational Requirement of needing a specific egress IP address: [SRE, DevOps Engineer]\nThere are many ways that applications communicate with each other.  For security reasons, as part of the interface specifications/agreement, an external application may expect traffic coming from a certain IP address.  Thus the requirement is, can we configure our project so that the traffic coming from our pods is seen by the external application as coming from a specific IP address?  To do this, you need to configure the egress IP address of the project.\n\nThere are two steps involved in assigning an egress IP address, and this is similar in a way on how a pod gets an IP address from the node assigned IP address range:\n- First, you assigned an egress IP address range to the node.\n- Then you specify the egress IP address from that range to the pod.\n\nExample:\n```\n# Assign the IP address range using CIDR notation to the node.\n$ oc patch hostsubnet node1 --type=merge -p '{\"egressCIDRs\": [\"192.168.1.0/24\"]}'\n\n# Assign the IP address range to the project from the range of IP address of the node.\n$ oc patch netnamespace project1 --type=merge -p '{\"egressIPs\": [\"192.168.1.100\"]}'\n\n```\n\nThe above is the OpenShift preferred approach.  There are other options available, and you can read them from the OpenShift documentation:  \nhttps://docs.openshift.com/container-platform/4.3/networking/openshift_sdn/assigning-egress-ips.html  \n\n\n\n<a name=\"more-than-one\"></a>\n\n## Operational Requirement of more than one network: [SRE, DevOps Engineer]\nSome enterprises might have a requirement to run some pods on a separate network outside the default network.  For performance reasons, some enterprises might also have a requirement for a set of pods to run on a separate network.  If you have this requirement, then you need to introduce the additional network to your cluster.\n\nDepending on the requirement, you can add an additional network through the network plugin. To provide multiple networks, OpenShift uses [the Multus Container Network Interface(CNI) plugin](https://github.com/intel/multus-cni). You can add the additional network of the following types:\n- __Bridge__.\n- __Host-device__.\n- __MAC-VLAN__, where a single physical interface of the host can have multiple mac and ip addresses.\n- __IP-VLAN__, where each endpoint gets the same mac address but different ip address.\n- __SR-IOV__ (Single-Root Input/Output Virtualization) \n\nEach of these additional networks is described in the OpenShift documentation on multiple networks:  \nhttps://docs.openshift.com/container-platform/4.3/networking/multiple_networks/understanding-multiple-networks.html\n\n\n\n## Implementing Networking\n\nThe following are links to more information on implementing networking.\n\n## Kubernetes\n- [Kubernetes network policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)\n\n## OpenShift\n- The sample OC CLI commands and Web console screen shots in the previous sections are for OpenShift implementation.\n- [Cluster Network Operator on Git](https://github.com/openshift/cluster-network-operator)\n\n## On IBM Cloud (Managed OpenShift)\nIf you are using Managed OpenShift on IBM Cloud, you will see some differences between what we have described in the previous sections and how IBM Cloud implemented the network.  Please read IBM Cloud official documents to understand those differences.\n- [Configuring subnets and IP addresses](https://cloud.ibm.com/docs/openshift?topic=openshift-subnets)\n- [Changing service endpoints or VLAN connections](https://cloud.ibm.com/docs/openshift?topic=openshift-cs_network_cluster)\n- [Opening required ports and IP addresses in your firewall](https://cloud.ibm.com/docs/openshift?topic=openshift-firewall)\n- [About Ingress in OpenShift version 4.3](https://cloud.ibm.com/docs/openshift?topic=openshift-ingress-about-roks4)\n- [Restricting network traffic to edge worker nodes](https://cloud.ibm.com/docs/openshift?topic=openshift-edge)\n- [Controlling traffic with network policies](https://cloud.ibm.com/docs/openshift?topic=openshift-network_policies)\n\n## With IBM Cloud Pak for MCM\n\n- [Specifying policy using IBM CloudPak for Multi-Cluster Management](../../mcm/cp4mcm_governance_risk/)\n\n## Others\n\nThe following are some of the Kubernetes network implementations:\n\n- AWS VPC CNI for Kubernetes. [AWS VPC CNI](https://github.com/aws/amazon-vpc-cni-k8s) is a networking plugin repository for pod networking in Kubernetes using Elastic Network Interfaces on AWS (Amazon Web Service).\n- Azure CNI [Azure CNI](https://docs.microsoft.com/en-us/azure/virtual-network/container-networking-overview) is an open-source plugin that enables containers to use Azure Virtual Network capabilities\n- Calico.  [Project Calico](https://docs.projectcalico.org/introduction) is an open-source networking and network security solution for containers, virtual machines, and native host-based workloads. It was used by the IBM Container Platform 2.x and 3.x for its networking component.\n- kube-ovn.  In its git repository [kube-ovn](https://github.com/alauda/kube-ovn) describes itself as a Kubernetes Network Fabric for Enterprises that Rich in Functions and Easy in Operations \n- Open vSwitch. [Open vSwitch](https://www.openvswitch.org/) is a multilayer virtual switch licensed under the open-source Apache 2.0 license.\n- Open Virtual Networking (OVN). It is an open source network virtualization solution developed by the Open vSwitch community.  Please refer to [the Kubernetes plugin and its documentation for OVN](https://github.com/ovn-org/ovn-kubernetes)\n\n## Other consideration\n\n- [OpenShift and Network Security Zones: Coexistence Approaches](https://blog.openshift.com/openshift-and-network-security-zones-coexistence-approaches/)\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/day2/Network/index.mdx"}}}}