{"componentChunkName":"component---src-pages-day-2-security-index-mdx","path":"/day2/Security/","result":{"pageContext":{"frontmatter":{"title":"OpenShift Platform Day2 - Security","description":"OpenShift Day2 Security","keywords":"OpenShift, day2, security"},"relativePagePath":"/day2/Security/index.mdx","titleType":"page","MdxNode":{"id":"408dfe40-e4bb-57ac-a356-6e9d458a6dc1","children":[],"parent":"ee2c283f-56ad-5088-9811-cc21e2ff9478","internal":{"content":"---\ntitle: OpenShift Platform Day2 - Security\ndescription: OpenShift Day2 Security\nkeywords: 'OpenShift, day2, security'\n---\n\n\n## Security Overview\n\nIn the Security lifecycle of Assess, Prevent, Detect and Respond, Day2 Operational Security generally focuses on Detect and Respond. It does, however, also depend of strong preventative measures being in place, from Day 1.\n\nThroughout the infrastructure - the Operating system, Container Platform, Pods, Container, Images etc, a ZeroTrust model must be adopted. This means to ensure that by default the most restrictive security controls are in place, and access is only grated explicitly when needed.\n\nIn some cases, where higher-level administrative access is required, extended user privileges are necessary to allow administrators to perform their jobs. In these cases it is critical that all access and actions performed by these administrators are logged and monitored for bad behavior, either malicious or accidental.  \n\n\n\n## Day 1 Platform (Pre-requisites)  \nThere is sometimes an overlap between what constitutes Day 1 versus Day 2 activities. It is generally assumed that the following tasks have been performed as Day 1 activities, and are therefore considered pre-requisites. If they have not been completed, they should be considered Day 2 activities.\n\n- [Enable Role Based Access Control (RBAC)](#RBAC)\n- [Securely Manage All Platform Secrets](#platformsecrets)\n- Define and Enable Audit Logging\n- Host Operating System hardening. (e.g. https://coreos.com/os/docs/latest/hardening-guide.html)\n- [Create and Define Cluster Network Polices](#NetworkSecurity)\n- [Create Cluster-wide Pod Security Policy](#PodSecurity)\n- [Setup Platform Certificates (Administrator console etc.)](#manage-certificates)  \n\n\n\n## Day 2 Platform\n- [Rotate Administrative credentials](#Rotate-Administrative-credentials)\n- [Update Platform (and Host) security patches (Rolling Updates)](#Update-Platform-security-patches)\n- [Perform Regular Vulnerability Scans](#Regular-scans)\n- [Create and Manage a Namespaces for Application(s)](#Create-new-Namespaces)\n- [Container Security (signing container images etc.)](#Container-security)\n- [Repeat Pen Testing (6months - one year)](#Repeat-Pen-testing)\n- [Security Threat Management](#Security-Threat-management)\n- [Manage Certificates](#manage-certificates)\n\n\n\n## Day 1 Application\n\n- Secure Coding Practices (Application Code scanning. Security TDD. Validating Input/Outputs etc.)\n- Application Hardening – ensure all backdoor accesses are removed! (local users / default passwords etc.)\n- External Application Penetration Testing\n- Threat Modeling\n- Application Continue Delivery Pipeline setup\n\n\n\n## Day 2 Application\n- [Application Security Policy](#Application-User-Access-Security-Policy)\n- [Validate Onboard Application to Threat Management Platform](#validate-Security-Threat-Monitoring)\n- [Setup External Certificates (application)](#Setup-external-Certificates)\n- [Application Users Access Channels Policies Setup: authentication, Firewalls, NATs, Load-balance, Network security, routes](#Application-Users-access-channels-policies-setup)\n\n\n\n\n## Mapping to Personas\n\nPersona | task\n--- | ---\nSRE | Rotate Administrative credentials\nSRE | Update Platform (and Host) security patches (Rolling Updates)\nSRE | Create new Namespaces for new application(s)\nSecurity Expert, SRE | Container security (signing container images etc.)\nSecurity Expert, SRE | Manage certificates\nSecurity Expert | Perform regular (daily/weekly) vulnerability scans\nSecurity Expert | Repeat Pen testing (6 months - one year)\nSecurity Expert | Security Threat Monitoring with integrated Incident Response\nSecurity Expert, SRE | Application Security Policy\nSecurity Expert, SRE | Validate onboard application to threat platform\nSecurity Expert, SRE | Setup external Certificates (application)\nSecurity Expert, SRE | Application Users access channels policies setup: authentication, Firewalls, NATs, Load-balance, Network security, routes  \n\n\n\n<a name=\"Rotate-Administrative-credentials\"></a>\n\n## Rotate Administrative Credentials: [ SRE ]\nThe shorter the lifetime of a secret or credential the harder it is for an attacker to make use of that credential. Set short lifetimes on certificates and automate their rotation. Use an authentication provider that can control how long issued tokens are available and use short lifetimes where possible. If you use service account tokens in external integrations, plan to rotate those tokens frequently. For example, once the bootstrap phase is complete, a bootstrap token used for setting up nodes should be revoked or its authorization removed.\nIt's also recommended that ownership of resources are rotated, making it hard for a single person to always have access to a single resource indefinitely makes it harder for a compromise to happen.\n\n- [Setting up authentication and credential rotation](https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html)  \n\n\n\n<a name=\"Regular-scans\"></a>\n\n## Perform Regular Vulnerability Scans: [ Security Expert ]\nWith the increase in adoption of Kubernetes among companies, there is an ever-increasing risk of exposure of the hosted business applications.With the increasing number of complex configuration controls, chances of human error in the administration are gradually increasing, leaving the entire environment of an organization vulnerable to cybersecurity incidents.\nWith that in mind, the Center of Internet Security (CIS) Provided companies with a set of guidelines into security their Kubernetes environment as best practices into securing it, one of them being running regular platform scans to help mapping where the platform can be vulnerable and required to be patched. One of the tools used in openshift to map vulnerabilities can be openscap, which is an open-source tool that maps container vulnerabilities or black-duck.\nDefining a schedule for such task can be a challenge as well as become overwhelming if done in a short period of time. The recommended approach is to use an automated tool to execute at least a weekly scan and create a pipeline of vulnerabilities to be fixed on a prioritized order so that high score vulnerabilities will be prioritized sooner than low score ones but in a way that all of them will be tackled in future releases.\n\n- [How and why of conducting vulnerability management for OpenShift](https://www.openshift.com/blog/container-vulnerability-management-openshift-commons-briefing-47)\n\n\n\n<a name=\"Update-Platform-security-patches\"></a>\n\n## Update Platform (and Host) Security Patches (Rolling Updates): [ SRE ]\nA major step into preventing vulnerabilities to allow your system to be compromised is to map the current vulnerabilities and based on them, apply updates that will address such vulnerabilities. The update policy shifts from a purely feature based update into a security-based update that will keep your system healthier from a risk perspective. Evaluating what update makes sense is also critical during this phase because a newer version might introduce a higher risk-vulnerability to your system. The recommendation is that each version is evaluated from a risk perspective and that updates, after being vetted, are applied through execution of rolling updates. Rolling updates allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones.\n\n- [How to perform rolling updates](https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/)\n\n\n\n<a name=\"Create-new-Namespaces\"></a>\n\n## Create and Manage a Namespaces for application(s) : [ SRE ]\nIn order to improve access-control to applications a namespaced access control setup can be leveraged so that policies are applied on a namespace basis, allowing for a given user or application to have or not access to a certain action, such as viewing secrets or updating a deployment.\n\n- [Setting up namespaced access control](https://learning.oreilly.com/library/view/kubernetes-cookbook/9781491979679/ch10.html#service_accounts) under section 10.3\n\n\n\n<a name=\"Container-security\"></a>\n\n## Container security (signing container images): [ Security Expert, SRE ]\nWhen building images that will ultimately run in a production Kubernetes cluster, be sure to follow best practices for packaging and distributing applications. For example, don’t build containers with passwords baked in—and this includes not just in the final layer, but any layers in the image. One of the counterintuitive problems introduced by container layers is that deleting a file in one layer doesn’t delete that file from preceding layers. It still takes up space, and it can be accessed by anyone with the right tools—an enterprising attacker can simply create an image that only consists of the layers that contain the password.  \nSecrets and images should never be mixed. If you do so, you will be hacked, and you will bring shame to your entire company or department.\n\nJust like an RPM signing, container image singing can be used to verify authenticity of a container image created by users/developers throughout image life-cycle. Container image signing on Red Hat Enterprise Linux (RHEL) systems provides a means of:\n1. Validating where a container image came from,\n2. Checking that the image has not been tampered with, and\n3. Setting policies to determine which validated images can be pulled to a host.\n\n- [Kubernetes: Up and Running, 2nd Edition](https://learning.oreilly.com/library/view/kubernetes-up-and/9781492046523/ch02.html#idm45561624327768)\n\nWe explain **Images** in more detail in [here](/day2/Image/).\n\nMore on **how to sign container images** in [here](https://access.redhat.com/documentation/en-us/openshift_container_platform/3.9/html/cluster_administration/admin-guide-image-signatures) and [here](https://www.techbeatly.com/2019/03/devsecops-with-openshift-image-signing.html#.Xou0QlB7nko).  \n\n\n\n<a name=\"Security-Threat-management\"></a>\n\n## Security Threat Management: [ Security Expert ]\n**Threat management** can be defined by:\n**1. Monitor:** Enabling log and flow (network) data generation from applications to be fed into a centralized system that can correlate information and provide actionable insight and context of a given information, often called SIEM (**Security Information Event Management**). This platform can be a log aggregator or a more robust platform that allows for detection of threats such as the [IBM QRadar](https://www.ibm.com/security/security-intelligence/qradar).\n**2. Respond:** System with pre-configured set of actions to be executed in case of an incident detection. Such system has the capabilities to automate specific tasks as well as collect artifacts and help on incident response. A system with such capabilities is [IBM Resilient](https://www.ibm.com/products/resilient-soar-platform)\n**3. Investigate:** Capabilities provided by a combination of the **Monitor** and **Respond** systems that allows for a user to search and investigate through data providing insights over a particular and possible threat such as Behavior analytics, AI, Reporting and more.\n\n- [Security Threat management](https://www.ibm.com/security/threat-management)\n\n\n\n<a name=\"Repeat-Pen-testing\"></a>\n\n## Execute periodically penetration testing: [ Security Expert ]\nAlthough scans will provide a holistic view on current security issues that need to be addressed in the platform, there needs to be proof that a vulnerable system or code can be exploited and cause harm to a company such as leaking sensitive content. In any scenario, a threat exploits a vulnerability that leads to risk of exposure. The way of preventing it is to apply a countermeasure and the way of mapping the most effective countermeasure is by asking our own penetration testing team to exploit the vulnerabilities found in the scanners regularly to define which has the highest risk of leading to exposure. Penetration testing has to generate accurate reports on how the exploits took place and which vulnerabilities were exploited so that countermeasures can be defined effectively afterwards. The recommended approach is that at least once a quarter this is executed as well as after the release of a major version so that responses and hardening policies are applied more frequently as well as more tactically.\n\n\n\n<a name=\"Application-User-Access-Security-Policy\"></a>\n\n## Application Security Policy: [ Security Expert, SRE ]\nIn this activity we setup the Applications User ID, and the Application's Administrative User.\n\nYou would provide an application with a unique identity in order to control access to resources on a fine-grained level. You can create a service account and use it in a pod specification.\n\n- [Kubernetes Cookbook](https://learning.oreilly.com/library/view/kubernetes-cookbook/9781491979679/ch10.html#service_accounts)\n\nWe explain **Service Account** in [here](/day2/User/).\n\nCluster administrators can use the cluster roles and bindings to control who has various access levels to the OpenShift Container Platform platform itself and all projects.\nDevelopers can use local roles and bindings to control who has access to their projects. Note that authorization is a separate step from authentication, which is more about determining the identity of who is taking the action and keep track of such user.\n\nMore on **how to set up RBAC** in [here](https://docs.openshift.com/container-platform/4.3/authentication/using-rbac.html) and [here](https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/).\n\nWe also explain **Role-Based Access Control** in [here](/day2/User/).  \n\n\n\n<a name=\"Setup-external-Certificates\"></a>\n\n## Setup external Certificates (application): [ Security Expert, SRE ]\nWhen setting up applications facing public web, it's recommended that a good PKI (Public Key Infrastructure) is in place to guarantee that only trusted certificates and certificate authorities are in place, applying strong reliability to the application via digital certificates.\nIn order to setup certificates in a Kubernetes cluster, Kubernetes provides a certificates.k8s.io API, which lets you provision TLS certificates signed by a Certificate Authority (CA) that you control. These CA and certificates can be used by your workloads to establish trust.\nIn order to get an external certificate, first it's necessary to contact a valid, external certificate authority. Each customer tends to hire services from a determined certificate authority, the application expert needs to contact the customer to find which team handles requests of certificates and then, after this is done, create a CSR (Certificate Signing Request). A CSR will allow the certificate authority to sign the certificate for the cluster and then return the certificate to be installed in the application.\n\n- [PKI Definition](https://en.wikipedia.org/wiki/Public_key_infrastructure)\n- [PKI certificates and requirements for kubernetes](https://kubernetes.io/docs/setup/best-practices/certificates/)\n- [Managing certificates in a cluster](https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/)\n\n\n\n<a name=\"Application-Users-access-channels-policies-setup\"></a>\n\n## Application Users access channels policies setup: authentication, Firewalls, NATs, Load-balance, Network security, routes: [ Security Expert, SRE ]\nLet's define **chanel** first:\n- It's component in a media that a user has to cross in order to reach a resource.\n\nFor this particular example, in order to make it more clear we will use the following:\n\n|Web-application: Port A| ----|\n                              |--|Load-balancer|--|Switch|--|Firewall|--|Switch|--|User|\n|Web-application: Port B| ----|\n\nThe topology above has the following components:\n- **Application web cluster**: A number of application instances that are a part of a load-balancer farm\n- [**Load-Balancer**](https://www.citrix.com/glossary/load-balancing.html): A device responsible for managing multiple requests and balance them through the configured instances in a farm\n- [**Farm**](https://www.itprotoday.com/compute-engines/load-balancing-down-web-farm): A combination of application instances\n- [**Switch**](https://www.cisco.com/c/en_ca/solutions/small-business/resource-center/networking/network-switch-what.html): Physical connection (network) in an infrastructure\n- [**Firewall**](https://www.cisco.com/c/en/us/products/security/firewalls/what-is-a-firewall.html): Controls inbound/outbound traffic through filters, sometimes can be a stateful or stateless firewall\n- **User**: Individual trying to access a resource\n\nIn summary, each of these are a media and have their own channel policies, for example, the application that resides in the )penShift container will have its [own firewall rules](https://docs.openshift.com/container-platform/4.3/installing/install_config/configuring-firewall.html), the Load-balance will have their own load-balancing policies as well as access control policies as well as the switches and even the user-machine has its own firewalls and access policies.\n\nIts imperative that each channel has its access policies well defined and applied, creating what's called a layered security approach, protecting the application that is in the last layer of the channels from being compromised. Each component in the channel can act as a compensative control for a vulnerability if necessary so policies also have to be updated as the need arises from a vulnerability scanner or a penetration testing. Keeping those channel policies up to date and well defined and documented help securing the application even more.\n\nThe recommendation is that each of the policies is tested by a security professional during day-2 to guarantee that there's no opened policies that would allow for a compromise and that during day-2, teams are ready to update policies in the necessary channels as they arise.\n\n- [Understanding layered security and defense in depth](https://www.techrepublic.com/blog/it-security/understanding-layered-security-and-defense-in-depth/)\n\n\n\n<a name=\"validate-Security-Threat-Monitoring\"></a>\n\n## Validate Onboard Application to Threat Management Platform: [ Security Expert, SRE ]\nDuring the deployment phase, applications have to be onboard into the monitoring component of the **Threat Management** system, allowing for analysts to monitor its behavior and patterns in order to be able to report into security incidents in case those are detected. it's recommended that all applications are onboarded through a secure connection and send syslog data to the monitoring system. Once the application is deployed it requires validation that it has been onboarded in both parties, the monitoring system and the application itself. Start by checking if the application [has this configuration](https://docs.openshift.com/container-platform/4.1/logging/config/efk-logging-external.html). Move onto accessing the monitoring system and validating it has syslog data for the onboarded application by creating a filter in the monitoring application, in this case we will use an example of the IBM QRadar filter demonstrated in [here](https://www.ibm.com/support/pages/qradar-how-can-you-find-out-what-log-sources-are-generating-most-events).\n\n- [Searching Data Efficiently in QRadar Using Quick Filters](https://www.google.com/search?client=firefox-b-e&q=creating+filters+on+qradar#kpvalbx=_ahSaXpvHBcm-tQXgx6uoBw20)\n\n\n\n<a name=\"RBAC\"></a>\n\n## RBAC: [ ]\nSecurity default Polices for Administrators with just enough access to do their job. Cluster administrators can use the cluster roles and bindings to control who has various access levels to the OpenShift Container Platform platform itself and all projects.\n\nMore on **how to set up RBAC** in [here](https://docs.openshift.com/container-platform/4.3/authentication/using-rbac.html) and [here](https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/).\n\nWe also explain **Role-Based Access Control** in [here](/day2/User/).  \n\n\n<a name=\"PodSecurity\"></a>\n\n## Create Cluster-wide Pod Security Policy\n\n### Securing Pods: [ ]\nYou can define the **Security Context** for an application on the pod level. For example, you want to run the application as a nonpriviledged process or restrict the types of volumes the application can access.  \nYou can use the **securityContext** field in a pod specification to enforce policies on the pod level in OpenShift cluster.\n\n- [Kubernetes Cookbook](https://learning.oreilly.com/library/view/kubernetes-cookbook/9781491979679/ch10.html#securing-pods)\n- [Managing Security Context Constraints](https://docs.openshift.com/container-platform/4.3/authentication/managing-security-context-constraints.html)\n\n\n\n### PodSecuirtyPolicy: [ ]\nBy using Pod's **SecurityContext** function, Pod security can be enhanced. However, we cannot guarantee that the **SecurityContext** is correctly specified for all pods deployed in the cluster. **PodSecurityPolicy** is a function that prohibits the deployment of Pod that does not satisfy the policy by defining the policy that the setting of the Pod should satisfy. Multiple policies can be defined and different policies can be applied depending on the user who creates the pod. By using this feature, you can maintain a consistent quality of Pod security settings across the cluster.  \n\n- [Pod Security Policies](https://kubernetes.io/docs/concepts/policy/pod-security-policy/)\n\n\n\n<a name=\"platformsecrets\"></a>\n\n## Securely Manage All Platform Secrets\n\n### Encryption Secrets: [ ]\nYou can encrypt data stored in etcd. Enabling etcd encryption for your cluster provides an additional layer of data security.\nWhen you enable etcd encryption, the following OpenShift API server and Kubernetes API server resources are encrypted:\n- Secrets\n- ConfigMaps\n- Routes\n- OAuth access tokens\n- OAuth authorize tokens  \n\n### Set minimum privilege with seccomp profiles: [ ]\nSeccomp is a system call filtering facility in the Linux kernel which lets applications define limits on system calls they may make, and what should happen when system calls are made. Seccomp is used to reduce the attack surface available to applications.  \n\nMore on Seccomp on [here](https://github.com/kubernetes/kubernetes/blob/release-1.4/docs/design/seccomp.md) and [here](https://docs.openshift.com/container-platform/4.3/authentication/managing-security-context-constraints.html).\n\n\n\n<a name=\"NetworkSecurity\"></a>\n\n## NetworkPolicy: [ ]\nIf a container is taken over by an attacker, it is an effective measure to limit the containers that can communicate in advance in order to prevent the damage from spreading to other containers deployed in the cluster. OpenShift has a function called NetworkPolicy to restrict communication between pods.  \n\nBy default, all Pods in a project are accessible from other Pods and network endpoints. To isolate one or more Pods in a project, you can create NetworkPolicy objects in that project to indicate the allowed incoming connections. Project administrators can create and delete NetworkPolicy objects within their own project.  \n\n- [Network policy](https://docs.openshift.com/container-platform/4.3/networking/configuring-networkpolicy.html)\n\n\n\n\n<a name=\"manage-certificates\"></a>\n\n## Setup Platform Certificates\nThe Ignition config files that the installation program generates contain certificates that expire after 24 hours. You must complete your cluster installation and keep the cluster running for 24 hours in a non-degraded state to ensure that the first certificate rotation has finished.\n\n### Certificate Management: [ SRE ]\nOpenShift Container Platform creates certificates by default for the Ingress Operator, the API server, and for services needed by complex middleware applications that require encryption. At some point, you may need to change, add, and rotate these certificates.\n\nCertificates are used to provide secure connections to master and nodes, Ingress controller and registry, etcd.  \n\nAutomated service CA rotation is available. In previous versions, the service CA did not automatically renew, leading to service disruption and requiring manual intervention. The service CA and signing key now auto-rotate before expiration. This allows administrators to plan for their environments in advance, avoiding service disruption.  \n\nOptionally configure external endpoints to use custom certificates.  \nYou cannot replace the Root CA of an OpenShift 4 cluster. There are currently no plans to change this.  \n\nHere is a list of tasks for Day 2 Security domain:\n- [Understanding and replacing the default ingress certificate](#Understanding-and-replacing-the-default-ingress-certificate)\n- [Adding API server certificates](#adding-api-server-certificates)\n- [Securing service traffic using service serving certificate secrets](#securing-service-traffic-using-service-serving-certificate-secrets)\n- [Setting up additional trusted certificate authorities for builds](#setting-up-additional-trusted-certificate-authorities-for-builds)\n- [Creating a secret from a .gitconfig file for secured git](#creating-a-secret-from-a-gitconfig-file-for-secured-git)\n- [Creating a secret from source code trusted certificate authorities](#creating-a-secret-from-source-code-trusted-certificate-authorities)\n- [Adding TLS certificates for authenticating DataVolume imports](#adding-tls-certificates-for-authenticating-datavolume-imports)\n- [Adding TLS certificates for sending logs to external devices](#adding-tls-certificates-for-sending-logs-to-external-devices)\n- [Certificate signing requests management](#certificate-signing-requests-management)  \n\n\n<a name=\"Understanding-and-replacing-the-default-ingress-certificate\"></a>\n\n### Understanding and replacing the default ingress certificate: [ SRE ]\nBy default OpenShift Container Platform uses the Ingress Operator to create an internal CA and issue a wildcard certificate that is valid for applications under the .apps sub-domain. Both the web console and CLI use this certificate as well.\n\nThe internal infrastructure CA certificates are self-signed. While this process might be perceived as bad practice by some security or PKI teams, any risk here is minimal. The only clients that implicitly trust these certificates are other components within the cluster.  \n\nReplacing the default wildcard certificate with one that is issued by a public CA already included in the CA bundle as provided by the container userspace allows external clients to connect securely to applications running under the .apps sub-domain.  \nAfter you replace the certificate, all applications, including the web console and CLI, will have encryption provided by specified certificate.  \n\n- [Replacing default ingress certificate](https://docs.openshift.com/container-platform/4.3/authentication/certificates/replacing-default-ingress-certificate.html)\n- [Replacing default ingress certificate 2](https://docs.openshift.com/container-platform/4.3/authentication/certificates/replacing-default-ingress-certificate.html#replacing-default-ingress_replacing-default-ingress)\n\n\n\n<a name=\"adding-api-server-certificates\"></a>\n\n### Adding API server certificates: [ SRE ]\nThe default API server certificate is issued by an internal OpenShift Container Platform cluster CA. Clients outside of the cluster will not be able to verify the API server’s certificate by default. This certificate can be replaced by one that is issued by a CA that clients trust.  \nYou can add additional certificates to the API server to send based on the client’s requested URL, such as when a reverse proxy or load balancer is used.  \n\n- [Adding API server certificates](https://docs.openshift.com/container-platform/4.3/authentication/certificates/api-server.html)\n\n\n\n<a name=\"securing-service-traffic-using-service-serving-certificate-secrets\"></a>\n\n### Securing service traffic using service serving certificate secrets: [ DevOps Engineer ]\nService serving certificates are intended to support complex middleware applications that require encryption (in other words, it needs out-of-the-box certificates). These certificates are issued as TLS web server certificates which means that it has the same settings as the server certificates generated by the administrator tooling for nodes and masters.  \n\nThe service-ca controller uses the x509.SHA256WithRSA signature algorithm to generate service certificates.\n\nThe generated certificate and key are in PEM format, stored in tls.crt and tls.key respectively, within a created secret.  \n\nThe service CA certificate, which signs the service certificates, is only valid for one year after OpenShift Container Platform is installed.  \nThe certificate and key are automatically replaced when they get close to expiration.  \nYou can rotate the service certificate by deleting the associated secret. Deleting the secret results in a new one being automatically created, resulting in a new certificate.  \n\n- [Service serving certificate](https://docs.openshift.com/container-platform/4.3/authentication/certificates/service-serving-certificate.html)\n- [About nodes/pods secrets](https://docs.openshift.com/container-platform/4.3/nodes/pods/nodes-pods-secrets.html#nodes-pods-secrets-certificates-about_nodes-pods-secrets)\n\n\n\n<a name=\"setting-up-additional-trusted-certificate-authorities-for-builds\"></a>\n\n### Setting up additional trusted certificate authorities for builds: [ DevOps Engineer ]  \nYou can add certificate authorities (CA) to be trusted by builds when pulling images from an image registry.  \n\n- [Setting up trusted ca](https://docs.openshift.com/container-platform/4.3/builds/setting-up-trusted-ca.html)\n\n\n\n<a name=\"creating-a-secret-from-a-gitconfig-file-for-secured-git\"></a>\n\n### Creating a secret from a .gitconfig file for secured git: [ DevOps Engineer ]\nIf your Git server is secured with two-way SSL and user name with password, you must add the certificate files to your source build and add references to the certificate files in the .gitconfig file.  \n\n- [How to create Gitconfig file for secured Git](https://docs.openshift.com/container-platform/4.3/builds/creating-build-inputs.html#source-secrets-gitconfig-file-for-secured-git_creating-build-inputs)  \n\n\n\n<a name=\"creating-a-secret-from-source-code-trusted-certificate-authorities\"></a>\n\n### Creating a secret from source code trusted certificate authorities: [ ]\nThe set of TLS certificate authorities (CA) that are trusted during a git clone operation are built into the OpenShift Container Platform infrastructure images. If your Git server uses a self-signed certificate or one signed by an authority not trusted by the image, you can create a secret that contains the certificate or disable TLS verification.  \n\nIf you create a secret for the CA certificate, OpenShift Container Platform uses it to access your Git server during the git clone operation. Using this method is significantly more secure than disabling Git’s SSL verification, which accepts any TLS certificate that is presented.  \n\n- [How to create a secret from source code trusted CAs](https://docs.openshift.com/container-platform/4.3/builds/creating-build-inputs.html#source-secrets-trusted-ca_creating-build-inputs)\n\n\n\n<a name=\"adding-tls-certificates-for-authenticating-datavolume-imports\"></a>\n\n### Adding TLS certificates for authenticating DataVolume imports: [ ]\nTLS certificates for registry or HTTPS endpoints must be added to a ConfigMap in order to import data from these sources. This ConfigMap must be present in the namespace of the destination DataVolume.  \n\n- [How to add TLS certificates for authenticating DataVolume imports](https://docs.openshift.com/container-platform/4.3/cnv/cnv_virtual_machines/cnv_importing_vms/cnv-tls-certificates-for-dv-imports.html)\n\n\n\n\n<a name=\"#adding-tls-certificates-for-sending-logs-to-external-devices\"></a>\n\n### Adding TLS certificates for sending logs to external devices: [ ]\nYou can configure Fluentd to send a copy of its logs to an external log aggregator, and not the default Elasticsearch, using the forward plug-in.  \nYou can add any certificates required by your external devices (external log aggregator) to a secret, called secure-forward, which is mounted to the Fluentd Pods.  \n\n- [How to add TLS certificate for sending logs to external devices](https://docs.openshift.com/container-platform/4.3/logging/config/cluster-logging-external.html#cluster-logging-fluentd-external_cluster-logging-external)\n\n\n\n\n<a name=\"certificate-signing-requests-management\"></a>\n\n### Certificate signing requests management: [ ]\nBecause your cluster has limited access to automatic machine management when you use infrastructure that you provision, you must provide a mechanism for approving cluster certificate signing requests (CSRs) after installation. The kube-controller-manager only approves the kubelet client CSRs. The machine-approver cannot guarantee the validity of a serving certificate that is requested by using kubelet credentials because it cannot confirm that the correct machine issued the request. You must determine and implement a method of verifying the validity of the kubelet serving certificate requests and approving them.  \n\n- [How to generate a CSR](https://docs.openshift.com/container-platform/4.3/machine_management/more-rhel-compute.html#csr-management-rhel_more-rhel-compute)  \n\nhttps://docs.openshift.com/container-platform/4.3/networking/ingress-operator.html#nw-ingress-setting-a-custom-default-certificate_configuring-ingress   \nhttps://docs.openshift.com/container-platform/4.3/networking/routes/secured-routes.html  \n\nWeb Console is accessed via the ingress controller   \nhttps://docs.openshift.com/container-platform/4.3/authentication/certificates/api-server.html\nhttps://docs.openshift.com/container-platform/4.3/registry/configuring-registry-operator.html\nhttps://blog.openshift.com/requesting-and-installing-lets-encrypt-certificates-for-openshift-4/   \n\nFor Application cert management, see:\nhttps://blog.openshift.com/self-serviced-end-to-end-encryption-approaches-for-applications-deployed-in-openshift/\nhttps://blog.openshift.com/self-serviced-end-to-end-encryption-for-kubernetes-applications-part-2-a-practical-example/\n\n\n\n## Implementing Security Management\n**TBD**  \n\n\n## Kubernetes\n**TBD**  \n\n\n\n## OpenShift\n**TBD**  \n\n\n\n\n## On IBM Cloud\nSecurity for Red Hat OpenShift on IBM Cloud:  \nhttps://cloud.ibm.com/docs/openshift?topic=openshift-security\n\n\n\n\n## With IBM Cloud Pak for MCM\n**TBD**\n\n\n\n## Others\n\n\n\n\n## Other consideration\n\n\n\n\n## Reference  \n\n**9 Kubernetes Security Best Practices Everyone Must Follow**  \nAs organizations accelerate their adoption of containers and container orchestrators, they will need to take necessary steps to protect such a critical part of their compute infrastructure. To help in this endeavor, check out these nine Kubernetes security best practices, based on customer input, you should follow to help protect your infrastructure.\n1. Upgrade to the Latest Version\n2. Enable Role-Based Access Control (RBAC)\n3. Use Namespaces to Establish Security Boundaries\n4. Separate Sensitive Workloads\n5. Secure Cloud Metadata Access\n6. Create and Define Cluster Network Policies\n7. Run a Cluster-wide Pod Security Policy\n8. Harden Node Security\n9. Turn on Audit Logging  \n\nhttps://www.cncf.io/blog/2019/01/14/9-kubernetes-security-best-practices-everyone-must-follow/  \n\n\n**Securing a Cluster**  \n- Controlling access to the Kubernetes API\n- Controlling access to the Kubelet\n- Controlling the capabilities of a workload or user at runtime\n- Protecting cluster components from compromise\n\nhttps://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/  \n\n\n**Overview of Cloud Native Security**   \nKubernetes Security (and security in general) is an immense topic that has many highly interrelated parts. In today’s era where open source software is integrated into many of the systems that help web applications run, there are some overarching concepts that can help guide your intuition about how you can think about security holistically.  \n\nhttps://kubernetes.io/docs/concepts/security/  \n\n\n**6 Kubernetes Security Best Practices: Secure Your Workloads**\n1. Enable Role-Based Access Control (RBAC)\n2. Keep Your Secrets Secret\n3. Restrict Pod-to-Pod Traffic With a Kubernetes Network Policy\n4. Enhance Pod Security\n5. Kubernetes Rolling Updates\n6. Define Audit Policies\n\nhttps://phoenixnap.com/kb/kubernetes-security-best-practices\n\n","type":"Mdx","contentDigest":"f32e2ebed90192335dacbaca9ad38448","counter":570,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"OpenShift Platform Day2 - Security","description":"OpenShift Day2 Security","keywords":"OpenShift, day2, security"},"exports":{},"rawBody":"---\ntitle: OpenShift Platform Day2 - Security\ndescription: OpenShift Day2 Security\nkeywords: 'OpenShift, day2, security'\n---\n\n\n## Security Overview\n\nIn the Security lifecycle of Assess, Prevent, Detect and Respond, Day2 Operational Security generally focuses on Detect and Respond. It does, however, also depend of strong preventative measures being in place, from Day 1.\n\nThroughout the infrastructure - the Operating system, Container Platform, Pods, Container, Images etc, a ZeroTrust model must be adopted. This means to ensure that by default the most restrictive security controls are in place, and access is only grated explicitly when needed.\n\nIn some cases, where higher-level administrative access is required, extended user privileges are necessary to allow administrators to perform their jobs. In these cases it is critical that all access and actions performed by these administrators are logged and monitored for bad behavior, either malicious or accidental.  \n\n\n\n## Day 1 Platform (Pre-requisites)  \nThere is sometimes an overlap between what constitutes Day 1 versus Day 2 activities. It is generally assumed that the following tasks have been performed as Day 1 activities, and are therefore considered pre-requisites. If they have not been completed, they should be considered Day 2 activities.\n\n- [Enable Role Based Access Control (RBAC)](#RBAC)\n- [Securely Manage All Platform Secrets](#platformsecrets)\n- Define and Enable Audit Logging\n- Host Operating System hardening. (e.g. https://coreos.com/os/docs/latest/hardening-guide.html)\n- [Create and Define Cluster Network Polices](#NetworkSecurity)\n- [Create Cluster-wide Pod Security Policy](#PodSecurity)\n- [Setup Platform Certificates (Administrator console etc.)](#manage-certificates)  \n\n\n\n## Day 2 Platform\n- [Rotate Administrative credentials](#Rotate-Administrative-credentials)\n- [Update Platform (and Host) security patches (Rolling Updates)](#Update-Platform-security-patches)\n- [Perform Regular Vulnerability Scans](#Regular-scans)\n- [Create and Manage a Namespaces for Application(s)](#Create-new-Namespaces)\n- [Container Security (signing container images etc.)](#Container-security)\n- [Repeat Pen Testing (6months - one year)](#Repeat-Pen-testing)\n- [Security Threat Management](#Security-Threat-management)\n- [Manage Certificates](#manage-certificates)\n\n\n\n## Day 1 Application\n\n- Secure Coding Practices (Application Code scanning. Security TDD. Validating Input/Outputs etc.)\n- Application Hardening – ensure all backdoor accesses are removed! (local users / default passwords etc.)\n- External Application Penetration Testing\n- Threat Modeling\n- Application Continue Delivery Pipeline setup\n\n\n\n## Day 2 Application\n- [Application Security Policy](#Application-User-Access-Security-Policy)\n- [Validate Onboard Application to Threat Management Platform](#validate-Security-Threat-Monitoring)\n- [Setup External Certificates (application)](#Setup-external-Certificates)\n- [Application Users Access Channels Policies Setup: authentication, Firewalls, NATs, Load-balance, Network security, routes](#Application-Users-access-channels-policies-setup)\n\n\n\n\n## Mapping to Personas\n\nPersona | task\n--- | ---\nSRE | Rotate Administrative credentials\nSRE | Update Platform (and Host) security patches (Rolling Updates)\nSRE | Create new Namespaces for new application(s)\nSecurity Expert, SRE | Container security (signing container images etc.)\nSecurity Expert, SRE | Manage certificates\nSecurity Expert | Perform regular (daily/weekly) vulnerability scans\nSecurity Expert | Repeat Pen testing (6 months - one year)\nSecurity Expert | Security Threat Monitoring with integrated Incident Response\nSecurity Expert, SRE | Application Security Policy\nSecurity Expert, SRE | Validate onboard application to threat platform\nSecurity Expert, SRE | Setup external Certificates (application)\nSecurity Expert, SRE | Application Users access channels policies setup: authentication, Firewalls, NATs, Load-balance, Network security, routes  \n\n\n\n<a name=\"Rotate-Administrative-credentials\"></a>\n\n## Rotate Administrative Credentials: [ SRE ]\nThe shorter the lifetime of a secret or credential the harder it is for an attacker to make use of that credential. Set short lifetimes on certificates and automate their rotation. Use an authentication provider that can control how long issued tokens are available and use short lifetimes where possible. If you use service account tokens in external integrations, plan to rotate those tokens frequently. For example, once the bootstrap phase is complete, a bootstrap token used for setting up nodes should be revoked or its authorization removed.\nIt's also recommended that ownership of resources are rotated, making it hard for a single person to always have access to a single resource indefinitely makes it harder for a compromise to happen.\n\n- [Setting up authentication and credential rotation](https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html)  \n\n\n\n<a name=\"Regular-scans\"></a>\n\n## Perform Regular Vulnerability Scans: [ Security Expert ]\nWith the increase in adoption of Kubernetes among companies, there is an ever-increasing risk of exposure of the hosted business applications.With the increasing number of complex configuration controls, chances of human error in the administration are gradually increasing, leaving the entire environment of an organization vulnerable to cybersecurity incidents.\nWith that in mind, the Center of Internet Security (CIS) Provided companies with a set of guidelines into security their Kubernetes environment as best practices into securing it, one of them being running regular platform scans to help mapping where the platform can be vulnerable and required to be patched. One of the tools used in openshift to map vulnerabilities can be openscap, which is an open-source tool that maps container vulnerabilities or black-duck.\nDefining a schedule for such task can be a challenge as well as become overwhelming if done in a short period of time. The recommended approach is to use an automated tool to execute at least a weekly scan and create a pipeline of vulnerabilities to be fixed on a prioritized order so that high score vulnerabilities will be prioritized sooner than low score ones but in a way that all of them will be tackled in future releases.\n\n- [How and why of conducting vulnerability management for OpenShift](https://www.openshift.com/blog/container-vulnerability-management-openshift-commons-briefing-47)\n\n\n\n<a name=\"Update-Platform-security-patches\"></a>\n\n## Update Platform (and Host) Security Patches (Rolling Updates): [ SRE ]\nA major step into preventing vulnerabilities to allow your system to be compromised is to map the current vulnerabilities and based on them, apply updates that will address such vulnerabilities. The update policy shifts from a purely feature based update into a security-based update that will keep your system healthier from a risk perspective. Evaluating what update makes sense is also critical during this phase because a newer version might introduce a higher risk-vulnerability to your system. The recommendation is that each version is evaluated from a risk perspective and that updates, after being vetted, are applied through execution of rolling updates. Rolling updates allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones.\n\n- [How to perform rolling updates](https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/)\n\n\n\n<a name=\"Create-new-Namespaces\"></a>\n\n## Create and Manage a Namespaces for application(s) : [ SRE ]\nIn order to improve access-control to applications a namespaced access control setup can be leveraged so that policies are applied on a namespace basis, allowing for a given user or application to have or not access to a certain action, such as viewing secrets or updating a deployment.\n\n- [Setting up namespaced access control](https://learning.oreilly.com/library/view/kubernetes-cookbook/9781491979679/ch10.html#service_accounts) under section 10.3\n\n\n\n<a name=\"Container-security\"></a>\n\n## Container security (signing container images): [ Security Expert, SRE ]\nWhen building images that will ultimately run in a production Kubernetes cluster, be sure to follow best practices for packaging and distributing applications. For example, don’t build containers with passwords baked in—and this includes not just in the final layer, but any layers in the image. One of the counterintuitive problems introduced by container layers is that deleting a file in one layer doesn’t delete that file from preceding layers. It still takes up space, and it can be accessed by anyone with the right tools—an enterprising attacker can simply create an image that only consists of the layers that contain the password.  \nSecrets and images should never be mixed. If you do so, you will be hacked, and you will bring shame to your entire company or department.\n\nJust like an RPM signing, container image singing can be used to verify authenticity of a container image created by users/developers throughout image life-cycle. Container image signing on Red Hat Enterprise Linux (RHEL) systems provides a means of:\n1. Validating where a container image came from,\n2. Checking that the image has not been tampered with, and\n3. Setting policies to determine which validated images can be pulled to a host.\n\n- [Kubernetes: Up and Running, 2nd Edition](https://learning.oreilly.com/library/view/kubernetes-up-and/9781492046523/ch02.html#idm45561624327768)\n\nWe explain **Images** in more detail in [here](/day2/Image/).\n\nMore on **how to sign container images** in [here](https://access.redhat.com/documentation/en-us/openshift_container_platform/3.9/html/cluster_administration/admin-guide-image-signatures) and [here](https://www.techbeatly.com/2019/03/devsecops-with-openshift-image-signing.html#.Xou0QlB7nko).  \n\n\n\n<a name=\"Security-Threat-management\"></a>\n\n## Security Threat Management: [ Security Expert ]\n**Threat management** can be defined by:\n**1. Monitor:** Enabling log and flow (network) data generation from applications to be fed into a centralized system that can correlate information and provide actionable insight and context of a given information, often called SIEM (**Security Information Event Management**). This platform can be a log aggregator or a more robust platform that allows for detection of threats such as the [IBM QRadar](https://www.ibm.com/security/security-intelligence/qradar).\n**2. Respond:** System with pre-configured set of actions to be executed in case of an incident detection. Such system has the capabilities to automate specific tasks as well as collect artifacts and help on incident response. A system with such capabilities is [IBM Resilient](https://www.ibm.com/products/resilient-soar-platform)\n**3. Investigate:** Capabilities provided by a combination of the **Monitor** and **Respond** systems that allows for a user to search and investigate through data providing insights over a particular and possible threat such as Behavior analytics, AI, Reporting and more.\n\n- [Security Threat management](https://www.ibm.com/security/threat-management)\n\n\n\n<a name=\"Repeat-Pen-testing\"></a>\n\n## Execute periodically penetration testing: [ Security Expert ]\nAlthough scans will provide a holistic view on current security issues that need to be addressed in the platform, there needs to be proof that a vulnerable system or code can be exploited and cause harm to a company such as leaking sensitive content. In any scenario, a threat exploits a vulnerability that leads to risk of exposure. The way of preventing it is to apply a countermeasure and the way of mapping the most effective countermeasure is by asking our own penetration testing team to exploit the vulnerabilities found in the scanners regularly to define which has the highest risk of leading to exposure. Penetration testing has to generate accurate reports on how the exploits took place and which vulnerabilities were exploited so that countermeasures can be defined effectively afterwards. The recommended approach is that at least once a quarter this is executed as well as after the release of a major version so that responses and hardening policies are applied more frequently as well as more tactically.\n\n\n\n<a name=\"Application-User-Access-Security-Policy\"></a>\n\n## Application Security Policy: [ Security Expert, SRE ]\nIn this activity we setup the Applications User ID, and the Application's Administrative User.\n\nYou would provide an application with a unique identity in order to control access to resources on a fine-grained level. You can create a service account and use it in a pod specification.\n\n- [Kubernetes Cookbook](https://learning.oreilly.com/library/view/kubernetes-cookbook/9781491979679/ch10.html#service_accounts)\n\nWe explain **Service Account** in [here](/day2/User/).\n\nCluster administrators can use the cluster roles and bindings to control who has various access levels to the OpenShift Container Platform platform itself and all projects.\nDevelopers can use local roles and bindings to control who has access to their projects. Note that authorization is a separate step from authentication, which is more about determining the identity of who is taking the action and keep track of such user.\n\nMore on **how to set up RBAC** in [here](https://docs.openshift.com/container-platform/4.3/authentication/using-rbac.html) and [here](https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/).\n\nWe also explain **Role-Based Access Control** in [here](/day2/User/).  \n\n\n\n<a name=\"Setup-external-Certificates\"></a>\n\n## Setup external Certificates (application): [ Security Expert, SRE ]\nWhen setting up applications facing public web, it's recommended that a good PKI (Public Key Infrastructure) is in place to guarantee that only trusted certificates and certificate authorities are in place, applying strong reliability to the application via digital certificates.\nIn order to setup certificates in a Kubernetes cluster, Kubernetes provides a certificates.k8s.io API, which lets you provision TLS certificates signed by a Certificate Authority (CA) that you control. These CA and certificates can be used by your workloads to establish trust.\nIn order to get an external certificate, first it's necessary to contact a valid, external certificate authority. Each customer tends to hire services from a determined certificate authority, the application expert needs to contact the customer to find which team handles requests of certificates and then, after this is done, create a CSR (Certificate Signing Request). A CSR will allow the certificate authority to sign the certificate for the cluster and then return the certificate to be installed in the application.\n\n- [PKI Definition](https://en.wikipedia.org/wiki/Public_key_infrastructure)\n- [PKI certificates and requirements for kubernetes](https://kubernetes.io/docs/setup/best-practices/certificates/)\n- [Managing certificates in a cluster](https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/)\n\n\n\n<a name=\"Application-Users-access-channels-policies-setup\"></a>\n\n## Application Users access channels policies setup: authentication, Firewalls, NATs, Load-balance, Network security, routes: [ Security Expert, SRE ]\nLet's define **chanel** first:\n- It's component in a media that a user has to cross in order to reach a resource.\n\nFor this particular example, in order to make it more clear we will use the following:\n\n|Web-application: Port A| ----|\n                              |--|Load-balancer|--|Switch|--|Firewall|--|Switch|--|User|\n|Web-application: Port B| ----|\n\nThe topology above has the following components:\n- **Application web cluster**: A number of application instances that are a part of a load-balancer farm\n- [**Load-Balancer**](https://www.citrix.com/glossary/load-balancing.html): A device responsible for managing multiple requests and balance them through the configured instances in a farm\n- [**Farm**](https://www.itprotoday.com/compute-engines/load-balancing-down-web-farm): A combination of application instances\n- [**Switch**](https://www.cisco.com/c/en_ca/solutions/small-business/resource-center/networking/network-switch-what.html): Physical connection (network) in an infrastructure\n- [**Firewall**](https://www.cisco.com/c/en/us/products/security/firewalls/what-is-a-firewall.html): Controls inbound/outbound traffic through filters, sometimes can be a stateful or stateless firewall\n- **User**: Individual trying to access a resource\n\nIn summary, each of these are a media and have their own channel policies, for example, the application that resides in the )penShift container will have its [own firewall rules](https://docs.openshift.com/container-platform/4.3/installing/install_config/configuring-firewall.html), the Load-balance will have their own load-balancing policies as well as access control policies as well as the switches and even the user-machine has its own firewalls and access policies.\n\nIts imperative that each channel has its access policies well defined and applied, creating what's called a layered security approach, protecting the application that is in the last layer of the channels from being compromised. Each component in the channel can act as a compensative control for a vulnerability if necessary so policies also have to be updated as the need arises from a vulnerability scanner or a penetration testing. Keeping those channel policies up to date and well defined and documented help securing the application even more.\n\nThe recommendation is that each of the policies is tested by a security professional during day-2 to guarantee that there's no opened policies that would allow for a compromise and that during day-2, teams are ready to update policies in the necessary channels as they arise.\n\n- [Understanding layered security and defense in depth](https://www.techrepublic.com/blog/it-security/understanding-layered-security-and-defense-in-depth/)\n\n\n\n<a name=\"validate-Security-Threat-Monitoring\"></a>\n\n## Validate Onboard Application to Threat Management Platform: [ Security Expert, SRE ]\nDuring the deployment phase, applications have to be onboard into the monitoring component of the **Threat Management** system, allowing for analysts to monitor its behavior and patterns in order to be able to report into security incidents in case those are detected. it's recommended that all applications are onboarded through a secure connection and send syslog data to the monitoring system. Once the application is deployed it requires validation that it has been onboarded in both parties, the monitoring system and the application itself. Start by checking if the application [has this configuration](https://docs.openshift.com/container-platform/4.1/logging/config/efk-logging-external.html). Move onto accessing the monitoring system and validating it has syslog data for the onboarded application by creating a filter in the monitoring application, in this case we will use an example of the IBM QRadar filter demonstrated in [here](https://www.ibm.com/support/pages/qradar-how-can-you-find-out-what-log-sources-are-generating-most-events).\n\n- [Searching Data Efficiently in QRadar Using Quick Filters](https://www.google.com/search?client=firefox-b-e&q=creating+filters+on+qradar#kpvalbx=_ahSaXpvHBcm-tQXgx6uoBw20)\n\n\n\n<a name=\"RBAC\"></a>\n\n## RBAC: [ ]\nSecurity default Polices for Administrators with just enough access to do their job. Cluster administrators can use the cluster roles and bindings to control who has various access levels to the OpenShift Container Platform platform itself and all projects.\n\nMore on **how to set up RBAC** in [here](https://docs.openshift.com/container-platform/4.3/authentication/using-rbac.html) and [here](https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/).\n\nWe also explain **Role-Based Access Control** in [here](/day2/User/).  \n\n\n<a name=\"PodSecurity\"></a>\n\n## Create Cluster-wide Pod Security Policy\n\n### Securing Pods: [ ]\nYou can define the **Security Context** for an application on the pod level. For example, you want to run the application as a nonpriviledged process or restrict the types of volumes the application can access.  \nYou can use the **securityContext** field in a pod specification to enforce policies on the pod level in OpenShift cluster.\n\n- [Kubernetes Cookbook](https://learning.oreilly.com/library/view/kubernetes-cookbook/9781491979679/ch10.html#securing-pods)\n- [Managing Security Context Constraints](https://docs.openshift.com/container-platform/4.3/authentication/managing-security-context-constraints.html)\n\n\n\n### PodSecuirtyPolicy: [ ]\nBy using Pod's **SecurityContext** function, Pod security can be enhanced. However, we cannot guarantee that the **SecurityContext** is correctly specified for all pods deployed in the cluster. **PodSecurityPolicy** is a function that prohibits the deployment of Pod that does not satisfy the policy by defining the policy that the setting of the Pod should satisfy. Multiple policies can be defined and different policies can be applied depending on the user who creates the pod. By using this feature, you can maintain a consistent quality of Pod security settings across the cluster.  \n\n- [Pod Security Policies](https://kubernetes.io/docs/concepts/policy/pod-security-policy/)\n\n\n\n<a name=\"platformsecrets\"></a>\n\n## Securely Manage All Platform Secrets\n\n### Encryption Secrets: [ ]\nYou can encrypt data stored in etcd. Enabling etcd encryption for your cluster provides an additional layer of data security.\nWhen you enable etcd encryption, the following OpenShift API server and Kubernetes API server resources are encrypted:\n- Secrets\n- ConfigMaps\n- Routes\n- OAuth access tokens\n- OAuth authorize tokens  \n\n### Set minimum privilege with seccomp profiles: [ ]\nSeccomp is a system call filtering facility in the Linux kernel which lets applications define limits on system calls they may make, and what should happen when system calls are made. Seccomp is used to reduce the attack surface available to applications.  \n\nMore on Seccomp on [here](https://github.com/kubernetes/kubernetes/blob/release-1.4/docs/design/seccomp.md) and [here](https://docs.openshift.com/container-platform/4.3/authentication/managing-security-context-constraints.html).\n\n\n\n<a name=\"NetworkSecurity\"></a>\n\n## NetworkPolicy: [ ]\nIf a container is taken over by an attacker, it is an effective measure to limit the containers that can communicate in advance in order to prevent the damage from spreading to other containers deployed in the cluster. OpenShift has a function called NetworkPolicy to restrict communication between pods.  \n\nBy default, all Pods in a project are accessible from other Pods and network endpoints. To isolate one or more Pods in a project, you can create NetworkPolicy objects in that project to indicate the allowed incoming connections. Project administrators can create and delete NetworkPolicy objects within their own project.  \n\n- [Network policy](https://docs.openshift.com/container-platform/4.3/networking/configuring-networkpolicy.html)\n\n\n\n\n<a name=\"manage-certificates\"></a>\n\n## Setup Platform Certificates\nThe Ignition config files that the installation program generates contain certificates that expire after 24 hours. You must complete your cluster installation and keep the cluster running for 24 hours in a non-degraded state to ensure that the first certificate rotation has finished.\n\n### Certificate Management: [ SRE ]\nOpenShift Container Platform creates certificates by default for the Ingress Operator, the API server, and for services needed by complex middleware applications that require encryption. At some point, you may need to change, add, and rotate these certificates.\n\nCertificates are used to provide secure connections to master and nodes, Ingress controller and registry, etcd.  \n\nAutomated service CA rotation is available. In previous versions, the service CA did not automatically renew, leading to service disruption and requiring manual intervention. The service CA and signing key now auto-rotate before expiration. This allows administrators to plan for their environments in advance, avoiding service disruption.  \n\nOptionally configure external endpoints to use custom certificates.  \nYou cannot replace the Root CA of an OpenShift 4 cluster. There are currently no plans to change this.  \n\nHere is a list of tasks for Day 2 Security domain:\n- [Understanding and replacing the default ingress certificate](#Understanding-and-replacing-the-default-ingress-certificate)\n- [Adding API server certificates](#adding-api-server-certificates)\n- [Securing service traffic using service serving certificate secrets](#securing-service-traffic-using-service-serving-certificate-secrets)\n- [Setting up additional trusted certificate authorities for builds](#setting-up-additional-trusted-certificate-authorities-for-builds)\n- [Creating a secret from a .gitconfig file for secured git](#creating-a-secret-from-a-gitconfig-file-for-secured-git)\n- [Creating a secret from source code trusted certificate authorities](#creating-a-secret-from-source-code-trusted-certificate-authorities)\n- [Adding TLS certificates for authenticating DataVolume imports](#adding-tls-certificates-for-authenticating-datavolume-imports)\n- [Adding TLS certificates for sending logs to external devices](#adding-tls-certificates-for-sending-logs-to-external-devices)\n- [Certificate signing requests management](#certificate-signing-requests-management)  \n\n\n<a name=\"Understanding-and-replacing-the-default-ingress-certificate\"></a>\n\n### Understanding and replacing the default ingress certificate: [ SRE ]\nBy default OpenShift Container Platform uses the Ingress Operator to create an internal CA and issue a wildcard certificate that is valid for applications under the .apps sub-domain. Both the web console and CLI use this certificate as well.\n\nThe internal infrastructure CA certificates are self-signed. While this process might be perceived as bad practice by some security or PKI teams, any risk here is minimal. The only clients that implicitly trust these certificates are other components within the cluster.  \n\nReplacing the default wildcard certificate with one that is issued by a public CA already included in the CA bundle as provided by the container userspace allows external clients to connect securely to applications running under the .apps sub-domain.  \nAfter you replace the certificate, all applications, including the web console and CLI, will have encryption provided by specified certificate.  \n\n- [Replacing default ingress certificate](https://docs.openshift.com/container-platform/4.3/authentication/certificates/replacing-default-ingress-certificate.html)\n- [Replacing default ingress certificate 2](https://docs.openshift.com/container-platform/4.3/authentication/certificates/replacing-default-ingress-certificate.html#replacing-default-ingress_replacing-default-ingress)\n\n\n\n<a name=\"adding-api-server-certificates\"></a>\n\n### Adding API server certificates: [ SRE ]\nThe default API server certificate is issued by an internal OpenShift Container Platform cluster CA. Clients outside of the cluster will not be able to verify the API server’s certificate by default. This certificate can be replaced by one that is issued by a CA that clients trust.  \nYou can add additional certificates to the API server to send based on the client’s requested URL, such as when a reverse proxy or load balancer is used.  \n\n- [Adding API server certificates](https://docs.openshift.com/container-platform/4.3/authentication/certificates/api-server.html)\n\n\n\n<a name=\"securing-service-traffic-using-service-serving-certificate-secrets\"></a>\n\n### Securing service traffic using service serving certificate secrets: [ DevOps Engineer ]\nService serving certificates are intended to support complex middleware applications that require encryption (in other words, it needs out-of-the-box certificates). These certificates are issued as TLS web server certificates which means that it has the same settings as the server certificates generated by the administrator tooling for nodes and masters.  \n\nThe service-ca controller uses the x509.SHA256WithRSA signature algorithm to generate service certificates.\n\nThe generated certificate and key are in PEM format, stored in tls.crt and tls.key respectively, within a created secret.  \n\nThe service CA certificate, which signs the service certificates, is only valid for one year after OpenShift Container Platform is installed.  \nThe certificate and key are automatically replaced when they get close to expiration.  \nYou can rotate the service certificate by deleting the associated secret. Deleting the secret results in a new one being automatically created, resulting in a new certificate.  \n\n- [Service serving certificate](https://docs.openshift.com/container-platform/4.3/authentication/certificates/service-serving-certificate.html)\n- [About nodes/pods secrets](https://docs.openshift.com/container-platform/4.3/nodes/pods/nodes-pods-secrets.html#nodes-pods-secrets-certificates-about_nodes-pods-secrets)\n\n\n\n<a name=\"setting-up-additional-trusted-certificate-authorities-for-builds\"></a>\n\n### Setting up additional trusted certificate authorities for builds: [ DevOps Engineer ]  \nYou can add certificate authorities (CA) to be trusted by builds when pulling images from an image registry.  \n\n- [Setting up trusted ca](https://docs.openshift.com/container-platform/4.3/builds/setting-up-trusted-ca.html)\n\n\n\n<a name=\"creating-a-secret-from-a-gitconfig-file-for-secured-git\"></a>\n\n### Creating a secret from a .gitconfig file for secured git: [ DevOps Engineer ]\nIf your Git server is secured with two-way SSL and user name with password, you must add the certificate files to your source build and add references to the certificate files in the .gitconfig file.  \n\n- [How to create Gitconfig file for secured Git](https://docs.openshift.com/container-platform/4.3/builds/creating-build-inputs.html#source-secrets-gitconfig-file-for-secured-git_creating-build-inputs)  \n\n\n\n<a name=\"creating-a-secret-from-source-code-trusted-certificate-authorities\"></a>\n\n### Creating a secret from source code trusted certificate authorities: [ ]\nThe set of TLS certificate authorities (CA) that are trusted during a git clone operation are built into the OpenShift Container Platform infrastructure images. If your Git server uses a self-signed certificate or one signed by an authority not trusted by the image, you can create a secret that contains the certificate or disable TLS verification.  \n\nIf you create a secret for the CA certificate, OpenShift Container Platform uses it to access your Git server during the git clone operation. Using this method is significantly more secure than disabling Git’s SSL verification, which accepts any TLS certificate that is presented.  \n\n- [How to create a secret from source code trusted CAs](https://docs.openshift.com/container-platform/4.3/builds/creating-build-inputs.html#source-secrets-trusted-ca_creating-build-inputs)\n\n\n\n<a name=\"adding-tls-certificates-for-authenticating-datavolume-imports\"></a>\n\n### Adding TLS certificates for authenticating DataVolume imports: [ ]\nTLS certificates for registry or HTTPS endpoints must be added to a ConfigMap in order to import data from these sources. This ConfigMap must be present in the namespace of the destination DataVolume.  \n\n- [How to add TLS certificates for authenticating DataVolume imports](https://docs.openshift.com/container-platform/4.3/cnv/cnv_virtual_machines/cnv_importing_vms/cnv-tls-certificates-for-dv-imports.html)\n\n\n\n\n<a name=\"#adding-tls-certificates-for-sending-logs-to-external-devices\"></a>\n\n### Adding TLS certificates for sending logs to external devices: [ ]\nYou can configure Fluentd to send a copy of its logs to an external log aggregator, and not the default Elasticsearch, using the forward plug-in.  \nYou can add any certificates required by your external devices (external log aggregator) to a secret, called secure-forward, which is mounted to the Fluentd Pods.  \n\n- [How to add TLS certificate for sending logs to external devices](https://docs.openshift.com/container-platform/4.3/logging/config/cluster-logging-external.html#cluster-logging-fluentd-external_cluster-logging-external)\n\n\n\n\n<a name=\"certificate-signing-requests-management\"></a>\n\n### Certificate signing requests management: [ ]\nBecause your cluster has limited access to automatic machine management when you use infrastructure that you provision, you must provide a mechanism for approving cluster certificate signing requests (CSRs) after installation. The kube-controller-manager only approves the kubelet client CSRs. The machine-approver cannot guarantee the validity of a serving certificate that is requested by using kubelet credentials because it cannot confirm that the correct machine issued the request. You must determine and implement a method of verifying the validity of the kubelet serving certificate requests and approving them.  \n\n- [How to generate a CSR](https://docs.openshift.com/container-platform/4.3/machine_management/more-rhel-compute.html#csr-management-rhel_more-rhel-compute)  \n\nhttps://docs.openshift.com/container-platform/4.3/networking/ingress-operator.html#nw-ingress-setting-a-custom-default-certificate_configuring-ingress   \nhttps://docs.openshift.com/container-platform/4.3/networking/routes/secured-routes.html  \n\nWeb Console is accessed via the ingress controller   \nhttps://docs.openshift.com/container-platform/4.3/authentication/certificates/api-server.html\nhttps://docs.openshift.com/container-platform/4.3/registry/configuring-registry-operator.html\nhttps://blog.openshift.com/requesting-and-installing-lets-encrypt-certificates-for-openshift-4/   \n\nFor Application cert management, see:\nhttps://blog.openshift.com/self-serviced-end-to-end-encryption-approaches-for-applications-deployed-in-openshift/\nhttps://blog.openshift.com/self-serviced-end-to-end-encryption-for-kubernetes-applications-part-2-a-practical-example/\n\n\n\n## Implementing Security Management\n**TBD**  \n\n\n## Kubernetes\n**TBD**  \n\n\n\n## OpenShift\n**TBD**  \n\n\n\n\n## On IBM Cloud\nSecurity for Red Hat OpenShift on IBM Cloud:  \nhttps://cloud.ibm.com/docs/openshift?topic=openshift-security\n\n\n\n\n## With IBM Cloud Pak for MCM\n**TBD**\n\n\n\n## Others\n\n\n\n\n## Other consideration\n\n\n\n\n## Reference  \n\n**9 Kubernetes Security Best Practices Everyone Must Follow**  \nAs organizations accelerate their adoption of containers and container orchestrators, they will need to take necessary steps to protect such a critical part of their compute infrastructure. To help in this endeavor, check out these nine Kubernetes security best practices, based on customer input, you should follow to help protect your infrastructure.\n1. Upgrade to the Latest Version\n2. Enable Role-Based Access Control (RBAC)\n3. Use Namespaces to Establish Security Boundaries\n4. Separate Sensitive Workloads\n5. Secure Cloud Metadata Access\n6. Create and Define Cluster Network Policies\n7. Run a Cluster-wide Pod Security Policy\n8. Harden Node Security\n9. Turn on Audit Logging  \n\nhttps://www.cncf.io/blog/2019/01/14/9-kubernetes-security-best-practices-everyone-must-follow/  \n\n\n**Securing a Cluster**  \n- Controlling access to the Kubernetes API\n- Controlling access to the Kubelet\n- Controlling the capabilities of a workload or user at runtime\n- Protecting cluster components from compromise\n\nhttps://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/  \n\n\n**Overview of Cloud Native Security**   \nKubernetes Security (and security in general) is an immense topic that has many highly interrelated parts. In today’s era where open source software is integrated into many of the systems that help web applications run, there are some overarching concepts that can help guide your intuition about how you can think about security holistically.  \n\nhttps://kubernetes.io/docs/concepts/security/  \n\n\n**6 Kubernetes Security Best Practices: Secure Your Workloads**\n1. Enable Role-Based Access Control (RBAC)\n2. Keep Your Secrets Secret\n3. Restrict Pod-to-Pod Traffic With a Kubernetes Network Policy\n4. Enhance Pod Security\n5. Kubernetes Rolling Updates\n6. Define Audit Policies\n\nhttps://phoenixnap.com/kb/kubernetes-security-best-practices\n\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/day2/Security/index.mdx"}}}}