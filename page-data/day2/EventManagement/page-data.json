{"componentChunkName":"component---src-pages-day-2-event-management-index-mdx","path":"/day2/EventManagement/","result":{"pageContext":{"frontmatter":{"title":"OpenShift Platform Day2 - Alerting and Event Management","description":"OpenShift Day2 Event Management","keywords":"OpenShift, day2, eventmanagement"},"relativePagePath":"/day2/EventManagement/index.mdx","titleType":"page","MdxNode":{"id":"46c5340c-dea0-55b9-b25d-c8a133b53a69","children":[],"parent":"e15228bb-294c-528f-ab1a-5959541e00d7","internal":{"content":"---\ntitle: OpenShift Platform Day2 - Alerting and Event Management\ndescription: OpenShift Day2 Event Management\nkeywords: 'OpenShift, day2, eventmanagement'\n---\n\n## Event Management Overview\n\nMonitoring and observability refer to the process of becoming aware of a state of a system. This is done in two ways, proactive and reactive. The former typically involves watching visual indicators, such as timeseries and dashboards. The latter involves automated ways to deliver notifications to operators or SREs in order to bring to their attention a grave change in system’s state; this is usually referred to as alerting.\n\nAlerting is the capability of a monitoring system to detect and notify the operators about meaningful events that denote a considerable change of state. This approach is known as Management by Exception. The notification is referred to as an alert and is a simple message that may take multiple forms: email, SMS or instant message. The alert is passed on to the appropriate recipient, that is, a party responsible for dealing with the event. The alert is often logged in the form of a ticket in an Incident Management system.\n\nOne of key SRE (Site Reliability Engineering) concepts for Day 2 and beyond is Actionable Alerts which means that every notification sent to Engineer should be really actionable. Alerts should be deduplicated, consolidated, correlated, to manageable quantities; more importantly alerts should drive correction action. To achieve that you will need an Event Management System capable to perform Root Cause Analysis (RCA). IBM has a well established Event Management solutions - Netcool Operation Insight (NOI) and Cloud Event Management (CEM). IBM CloudPak for Multicloud Management comes bundled with IBM Cloud App Management which includes comprehensive event management solution - Cloud Event Management (CEM).\nIn this document we will focus on the alerting and event management features provided within OpenShift Container Platform. \n\n## Day 1 Platform\n\nOpenShift Container Platform includes a pre-configured, pre-installed, and self-updating monitoring stack that is based on the Prometheus open source project and its wider eco-system. It provides monitoring of cluster components and includes a predefined set of alerts to immediately notify the cluster administrator about any occurring problems and a set of Grafana dashboards. The cluster monitoring stack is only supported for monitoring OpenShift Container Platform clusters and adding additional monitoring targets is not supported. \nThe predefined set of OpenShift Platform alerting rules is immutable and changes/updates are not supported.\n\nThe scope of Day 1 for the OpenShift Platform includes also the initial configuration of notification settings and alert silences. These settings can be also reconfigured during Day 2 and were described in more details in the Day 2 section of this document. \n\nIf you need to define additional alerts for the application workload, you can deploy an additional, separate monitoring stack managed by Prometheus Operator. Additional computing power requirements like CPU, memory or storage needs to be planned in advance, depending on the expected volume of metrics coming to this additional monitoring stack.\n\n**Day 1 Platform tasks for Alerting:**\n\n- Monitoring and Alerting stack deployed together with the OpenShift Platform\n- Initial configuration of the notification and silences for OpenShift Platform alerting rules\n\n\n## Day 2 Platform\nOpenShift Platform monitoring is pre-configured and deployed during cluster installation. You can view all defined alerts using OpenShift Console:\n\n![](./images/2020-02-19-11-54-27.png)\n\nYou can also view details of the Alert Rule:\n\n![](./images/2020-02-19-11-58-35.png)\n\nand create [Silences](#silencing-alerts--sre-) for Alerts. Currently, it is not possible to modify or add new OpenShift Platform alert definitions to the predefined set of alerts. \n\n**Day 2 Platform tasks for Alerting:**\n\n- [Configuring Prometheus Alertmanager [ SRE ]](#configuring-prometheus-alertmanager--sre-)\n- [Silencing Alerts [ SRE ]](#silencing-alerts--sre-)\n\n## Day 1 Application\n\nSome aspects of the application workload like resource utilization by application pods or application related kubernetes resources status and configuration are already monitored by the cluster platform monitoring with pre-defined alerts like:\n\n- KubePodCrashLooping\n- KubePodNotReady\n- KubeDeploymentGenerationMismatch\n- KubeDeploymentReplicasMismatch\n- KubeStatefulSetReplicasMismatch\n- KubeStatefulSetGenerationMismatch\n- KubeStatefulSetUpdateNotRolledOut\n- KubeDaemonSetRolloutStuck\n- KubeContainerWaiting\n- KubeDaemonSetNotScheduled\n- KubeDaemonSetMisScheduled\n- KubeQuotaExceeded\n- CPUThrottlingHigh\n- KubePersistentVolumeUsageCritical\n- KubePersistentVolumeFullInFourDays\n- KubePersistentVolumeErrors\n\nOther application specific metrics like [Golden Signals](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/#xref_monitoring_golden-signals) or [RED method](https://grafana.com/blog/2018/08/02/the-red-method-how-to-instrument-your-services/) can be monitored by the Prometheus after developers added instrumentation to the application code via one of the Prometheus [client libraries](https://prometheus.io/docs/instrumenting/clientlibs/). Check our [Node.js microservice instrumentation lab](https://ibm-cloud-architecture.github.io/b2m-nodejs/) for practical examples how to implement RED method metrics (request rate, error rate, request duration). \n\nOther method of collecting application workload metrics is using dedicated [exporters](https://prometheus.io/docs/instrumenting/exporters/). This method is covered in more detail in the `Monitoring` section of this \"Day 2 Operations Guide\".\nIn the following `Day 2 Application` chapter we will explain how to define custom alert rules based on custom application metrics.\n\n**Day 1 Application tasks for Alerting:**\n\n- Configuration of the application monitoring\n- Initial configuration of the alerting\n\n\n## Day 2 Application\n\nOpenShift 4.3 provides two options for definition of Prometheus Alerting Rules for the application workload running on the cluster:\n\n- Use the [monitoring your own services](https://docs.openshift.com/container-platform/4.3/monitoring/monitoring-your-own-services.html) Technology Preview in OpenShift 4.3.\n- Deploy separate monitoring stack and define custom alerts there.\n\nConfiguration details for both methods had been described in more details in the `Monitoring` section of this \"Day 2 Operations Guide\".\n\n**Day 2 Application tasks for Alerting:**\n\n- [Define custom alerting rules within default Cluster Monitoring Operator stack [ SRE ]](#Define-custom-alerting-rules-within-default-Cluster-Monitoring-Operator-stack--SRE-)\n- [Configure custom alerting solution in the dedicated Application Monitoring stack [ SRE ]](#Configure-custom-alerting-solution-in-the-dedicated-Application-Monitoring-stack--SRE-)\n\n## Mapping to Personas\n\nPersona | task\n--- | ---\nSRE | Configuring Prometheus Alertmanager\nSRE | Silencing alerts\nSRE | Define custom alerting rules within default Cluster Monitoring Operator stack\nSRE | Configure custom alerting solution in the dedicated Application Monitoring stack\n \n\n## Day 2 operations tasks for Event Management  \n\n## Configuring Prometheus Alertmanager [ SRE ]  \n\nDefault Alertmanager instance managed by the Cluster Monitoring Operator can be configured via OpenShift 4.3 console.  \n\n\n![](./images/2020-02-20-08-29-45.png)  \n\n\nOpenShift 4.3 contains a new Alertmanager section on the cluster settings page `Cluster Settings -> Global Configuration -> Alertmanager`  \n\n\n![](./images/2020-02-20-08-41-07.png)\n\nHere you can edit **Alert Grouping** settings like:\n- `Group By` - The labels by which incoming alerts are grouped together. For example, multiple alerts coming in for cluster=A and alertname=LatencyHigh would be batched into a single group.\n- `Group Wait` - How long to initially wait to send a notification for a group of alerts. Allows to wait for an inhibiting alert to arrive or collect more initial alerts for the same group. Usually ~0s to few minutes.\n- `Group Interval` - How long to wait before sending a notification about new alerts that are added to a group of alerts for which an initial notification has already been sent. (Usually ~5m or more.)\n- `Repeat Interval` - How long to wait before sending a notification again if it has already been sent successfully for an alert. (Usually ~3h or more).\n\nand **Receivers** like `Pager Duty` or `Webhook`. Every OpenShift cluster needs a default receiver to handle any alerts not sent to other places. The default receiver that comes with a fresh install is initially very basic, so your first step should be to configure it to suit your needs. For more complex team structures, you may want to send different kinds of alerts to different places by creating more receivers.\n\nThe same can be configured within Alertmanager secret. For the default Alertmanager instance the secret name is `alertmanager-main` and you can edit it using command:\n```\noc -n openshift-monitoring get secret alertmanager-main --template='{{ index .data \"alertmanager.yaml\" }}' |base64 -d > alertmanager.yaml\n```\nThen edit the `alertmanager.yaml`:\n```\nglobal:\n  resolve_timeout: 5m\nroute:\n  group_wait: 30s\n  group_interval: 5m\n  repeat_interval: 12h\n  receiver: default\n  routes:\n  - match:\n      alertname: Watchdog\n    repeat_interval: 5m\n    receiver: watchdog\n  - match:\n      service: example-app\n    routes:\n    - match:\n        severity: critical\n      receiver: team-frontend-page\nreceivers:\n- name: default\n- name: watchdog\n- name: team-frontend-page\n  pagerduty_configs:\n  - service_key: \"your-key\"\n```\nWith this configuration, alerts of critical severity fired by the `example-app` service are sent using the `team-frontend-page` receiver, which means that these alerts are paged to a chosen person.\n\n```\n$ oc -n openshift-monitoring create secret generic alertmanager-main --from-file=alertmanager.yaml --dry-run -o=yaml |  oc -n openshift-monitoring replace secret --filename=-\n```\n\nMore information about PagerDuty integration in the [Pager Duty Integration Guide](https://www.pagerduty.com/docs/guides/prometheus-integration-guide/).\n\n**Integration with Netcool Operations Insight (NOI)**\nBelow you can find an example Alertmanager configuration for integration with IBM NOI via webhook sent to Message Bus Probe.\n\n```\nglobal:\nreceivers:\n  - name: default-receiver\n    webhook_configs:\n    - url: 'http://<msgbus_probe_ip>:<msgbus_probe_port>/probe/webhook/prometheus'\n      send_resolved: true\nroute:\n  group_by: ['alertname','instance']\n  group_wait: 10s\n  group_interval: 5m\n  receiver: default-receiver\n  repeat_interval: 3h\n  routes:\n  - receiver: default-receiver\n    match:\n      alertname: ICPMonitoringHeartbeat\n    repeat_interval: 5m\n```\n- [Sending alerts as SNMP Traps](./SNMP_Trap/)\n- [Official Prometheus Alertmanager documentation](https://prometheus.io/docs/alerting/configuration/)\n\n## Silencing Alerts [ SRE ]\nYou can either silence a specific alert or silence alerts that match a specification that you define. Navigate to the `Monitoring → Alerting → Silences` page of the OpenShift Container Platform web console.\n\n![](./images/2020-02-20-09-37-30.png)\n\nSimilar way you can [get information about silences](https://docs.openshift.com/container-platform/4.3/monitoring/cluster_monitoring/managing-cluster-alerts.html#getting-information-about-silences_managing-alerts), [edit silences](https://docs.openshift.com/container-platform/4.3/monitoring/cluster_monitoring/managing-cluster-alerts.html#editing-silences_managing-alerts) and [expire silences](https://docs.openshift.com/container-platform/4.3/monitoring/cluster_monitoring/managing-cluster-alerts.html#expiring-silences_managing-alerts).\n\n## Define custom alerting rules within default Cluster Monitoring Operator stack [ SRE ]\n\nOpenShift 4.3 provides a new Technology Preview feature: [Monitoring your own services](https://docs.openshift.com/container-platform/4.3/monitoring/monitoring-your-own-services.html). It deploys a new Prometheus Operator instance in the `openshift-user-workload-monitoring` namespace. The new Prometheus instance can be configured to monitor custom metrics coming from the application workloads running on the cluster. Follow the [OpenShift 4.3 documentation](https://docs.openshift.com/container-platform/4.3/monitoring/monitoring-your-own-services.html) in order to configure this feature. Custom alerting rules can be configured by creating a custom `Prometheus Rule` resource as described [here](https://docs.openshift.com/container-platform/4.3/monitoring/monitoring-your-own-services.html#creating-alerting-rules_monitoring-your-own-services). Generated alerts are forwarded to the default Alertmanager instance and then forwarded to notification receivers configured in the main Alertmanager instance. \n\n## Configure custom alerting solution in the dedicated Application Monitoring stack [ SRE ]\nIf you decided to use dedicated monitoring stack for application monitoring, you can create a dedicated Alertmanager instance within mentioned application monitoring stack. This stack (both Prometheus and Alertmanager) are managed independently to the default cluster monitoring stack. Details how to configure dedicated application monitoring stack are described in [openshift-custom-app-monitoring](https://github.ibm.com/CASE/openshift-custom-app-monitoring) repository. The instructions how to deploy and configure dedicated Alertmanager and custom Alerting Rules start [here](https://github.ibm.com/CASE/openshift-custom-app-monitoring#deploy-alertmanager).\n\n## Alerting features per platform\n\n## Kubernetes\nKubernetes platform does not provide monitoring, alerting and notification features.  \n\n## OpenShift\nOpenShift provides alerting solution described in details in this document. \n\n## On IBM Cloud\n\nThere are a few choices for the alert and event management for OpenShift on IBM Cloud such as Sysdig, Activity Tracker with LogDNA, and Cloud Event Management.  We will explain each of them.  \n\n\n\n### IBM Cloud Monitoring with Sysdig\nSysdig provides the event management capability and alert feature in addition to the monitoring capability as shown in below.\n\n![ibmcloud_event_sysdig](./images/ibmcloud_event_sysdig.png)  \n\nOn the Sysdig Events dashboard, you can see events which Sysdig received from OpenShift on IBM Cloud as shown in below.\n\n![ibmcloud_event_sysdig_dashboard](./images/ibmcloud_event_sysdig_dashboard.png) \n\nSysdig can generate notifications based on certain conditions or events you configure.  Using the alert feature in Sysdig, you can configure notifications based on the conditions or events you configure.  As an example, the following picture shows the email notification with CrashLoopBackOff alert.\n\n![ibmcloud_event_sysdig_alert](./images/ibmcloud_event_sysdig_alert.png)  \n\n\n### IBM Cloud Activity Tracker with LogDNA\nYou can view, manage, and audit user-initiated activities that change the state of a service in the OpenShift cluster on IBM Cloud by using the IBM Cloud Activity Tracker with LogDNA service.  \n\nOpenShift on IBM Cloud automatically generates cluster management events and forwards these event logs to IBM Cloud Activity Tracker with LogDNA as shown in below.\n\n![ibmcloud_event_activity_tracker](./images/ibmcloud_event_activity_tracker.png) \n\nhttps://cloud.ibm.com/docs/containers?topic=containers-at_events  \nhttps://cloud.ibm.com/docs/services/Activity-Tracker-with-LogDNA?topic=logdnaat-getting-started#getting-started  \n\n\n\n\n### IBM Cloud Event Management with Built-in Prometheus\nYou can use the IBM Cloud Event Management service to set up real-time incident management and create a consolidated view of problems that occur with your applications and infrastructure.  IBM Cloud Event Management will be a centralized operations platform for event management, incident management, notifications, and runbook automation.  \n\nIBM Cloud Event Management can receive events from various monitoring sources such as OpenShift on IBM Cloud.  \n\nYou can set up an integration with IBM Cloud Event Management to receive alert information from the Built-in Prometheus in OpenShift on IBM Cloud as shown in below.\n\n![ibmcloud_event_builtin_alertmanager](./images/ibmcloud_event_builtin_alertmanager.png) \n\nNote that as of writing, there is an issue which override the customized Alert Manager setting. In other worlds, once the issue is fixed, we can send alerts to IBM Cloud Event Management via the built-in Prometheus Alert Manager.  \n\n\n\n\n### IBM Cloud Event Management with Sysdig\nYou can also set up an integration with IBM Cloud Event Management to receive alert information through the Sysdig service which receives the events from OpenShift on IBM Cloud as shown in below.  \n\n![ibmcloud_event_sysdig_cem](./images/ibmcloud_event_sysdig_cem.png) \n\nIf the built-in Prometheus is NOT your choice, then you would choose this solution instead.\n\n\n## With IBM Cloud Pak for MCM\n\nThe IBM Cloud Pak for Multicloud Managements includes an integrated monitoring solution: [IBM Cloud App Management](https://www.ibm.com/support/knowledgecenter/en/SS8G7U_19.4.0/com.ibm.app.mgmt.doc/welcome.html).\n\n**IBM Cloud Event Management** installed along with IBM Cloud App Management gives you the capability to automate management of incidents and events that are associated with resources and applications. All events that are related to a single application, or to a particular cluster, are correlated with an incident. Event Management can receive events from various monitoring sources, either on-premises or in the cloud.\n\n","type":"Mdx","contentDigest":"ac99c7cb0f2d7229cb566ec17a672f50","counter":544,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"OpenShift Platform Day2 - Alerting and Event Management","description":"OpenShift Day2 Event Management","keywords":"OpenShift, day2, eventmanagement"},"exports":{},"rawBody":"---\ntitle: OpenShift Platform Day2 - Alerting and Event Management\ndescription: OpenShift Day2 Event Management\nkeywords: 'OpenShift, day2, eventmanagement'\n---\n\n## Event Management Overview\n\nMonitoring and observability refer to the process of becoming aware of a state of a system. This is done in two ways, proactive and reactive. The former typically involves watching visual indicators, such as timeseries and dashboards. The latter involves automated ways to deliver notifications to operators or SREs in order to bring to their attention a grave change in system’s state; this is usually referred to as alerting.\n\nAlerting is the capability of a monitoring system to detect and notify the operators about meaningful events that denote a considerable change of state. This approach is known as Management by Exception. The notification is referred to as an alert and is a simple message that may take multiple forms: email, SMS or instant message. The alert is passed on to the appropriate recipient, that is, a party responsible for dealing with the event. The alert is often logged in the form of a ticket in an Incident Management system.\n\nOne of key SRE (Site Reliability Engineering) concepts for Day 2 and beyond is Actionable Alerts which means that every notification sent to Engineer should be really actionable. Alerts should be deduplicated, consolidated, correlated, to manageable quantities; more importantly alerts should drive correction action. To achieve that you will need an Event Management System capable to perform Root Cause Analysis (RCA). IBM has a well established Event Management solutions - Netcool Operation Insight (NOI) and Cloud Event Management (CEM). IBM CloudPak for Multicloud Management comes bundled with IBM Cloud App Management which includes comprehensive event management solution - Cloud Event Management (CEM).\nIn this document we will focus on the alerting and event management features provided within OpenShift Container Platform. \n\n## Day 1 Platform\n\nOpenShift Container Platform includes a pre-configured, pre-installed, and self-updating monitoring stack that is based on the Prometheus open source project and its wider eco-system. It provides monitoring of cluster components and includes a predefined set of alerts to immediately notify the cluster administrator about any occurring problems and a set of Grafana dashboards. The cluster monitoring stack is only supported for monitoring OpenShift Container Platform clusters and adding additional monitoring targets is not supported. \nThe predefined set of OpenShift Platform alerting rules is immutable and changes/updates are not supported.\n\nThe scope of Day 1 for the OpenShift Platform includes also the initial configuration of notification settings and alert silences. These settings can be also reconfigured during Day 2 and were described in more details in the Day 2 section of this document. \n\nIf you need to define additional alerts for the application workload, you can deploy an additional, separate monitoring stack managed by Prometheus Operator. Additional computing power requirements like CPU, memory or storage needs to be planned in advance, depending on the expected volume of metrics coming to this additional monitoring stack.\n\n**Day 1 Platform tasks for Alerting:**\n\n- Monitoring and Alerting stack deployed together with the OpenShift Platform\n- Initial configuration of the notification and silences for OpenShift Platform alerting rules\n\n\n## Day 2 Platform\nOpenShift Platform monitoring is pre-configured and deployed during cluster installation. You can view all defined alerts using OpenShift Console:\n\n![](./images/2020-02-19-11-54-27.png)\n\nYou can also view details of the Alert Rule:\n\n![](./images/2020-02-19-11-58-35.png)\n\nand create [Silences](#silencing-alerts--sre-) for Alerts. Currently, it is not possible to modify or add new OpenShift Platform alert definitions to the predefined set of alerts. \n\n**Day 2 Platform tasks for Alerting:**\n\n- [Configuring Prometheus Alertmanager [ SRE ]](#configuring-prometheus-alertmanager--sre-)\n- [Silencing Alerts [ SRE ]](#silencing-alerts--sre-)\n\n## Day 1 Application\n\nSome aspects of the application workload like resource utilization by application pods or application related kubernetes resources status and configuration are already monitored by the cluster platform monitoring with pre-defined alerts like:\n\n- KubePodCrashLooping\n- KubePodNotReady\n- KubeDeploymentGenerationMismatch\n- KubeDeploymentReplicasMismatch\n- KubeStatefulSetReplicasMismatch\n- KubeStatefulSetGenerationMismatch\n- KubeStatefulSetUpdateNotRolledOut\n- KubeDaemonSetRolloutStuck\n- KubeContainerWaiting\n- KubeDaemonSetNotScheduled\n- KubeDaemonSetMisScheduled\n- KubeQuotaExceeded\n- CPUThrottlingHigh\n- KubePersistentVolumeUsageCritical\n- KubePersistentVolumeFullInFourDays\n- KubePersistentVolumeErrors\n\nOther application specific metrics like [Golden Signals](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/#xref_monitoring_golden-signals) or [RED method](https://grafana.com/blog/2018/08/02/the-red-method-how-to-instrument-your-services/) can be monitored by the Prometheus after developers added instrumentation to the application code via one of the Prometheus [client libraries](https://prometheus.io/docs/instrumenting/clientlibs/). Check our [Node.js microservice instrumentation lab](https://ibm-cloud-architecture.github.io/b2m-nodejs/) for practical examples how to implement RED method metrics (request rate, error rate, request duration). \n\nOther method of collecting application workload metrics is using dedicated [exporters](https://prometheus.io/docs/instrumenting/exporters/). This method is covered in more detail in the `Monitoring` section of this \"Day 2 Operations Guide\".\nIn the following `Day 2 Application` chapter we will explain how to define custom alert rules based on custom application metrics.\n\n**Day 1 Application tasks for Alerting:**\n\n- Configuration of the application monitoring\n- Initial configuration of the alerting\n\n\n## Day 2 Application\n\nOpenShift 4.3 provides two options for definition of Prometheus Alerting Rules for the application workload running on the cluster:\n\n- Use the [monitoring your own services](https://docs.openshift.com/container-platform/4.3/monitoring/monitoring-your-own-services.html) Technology Preview in OpenShift 4.3.\n- Deploy separate monitoring stack and define custom alerts there.\n\nConfiguration details for both methods had been described in more details in the `Monitoring` section of this \"Day 2 Operations Guide\".\n\n**Day 2 Application tasks for Alerting:**\n\n- [Define custom alerting rules within default Cluster Monitoring Operator stack [ SRE ]](#Define-custom-alerting-rules-within-default-Cluster-Monitoring-Operator-stack--SRE-)\n- [Configure custom alerting solution in the dedicated Application Monitoring stack [ SRE ]](#Configure-custom-alerting-solution-in-the-dedicated-Application-Monitoring-stack--SRE-)\n\n## Mapping to Personas\n\nPersona | task\n--- | ---\nSRE | Configuring Prometheus Alertmanager\nSRE | Silencing alerts\nSRE | Define custom alerting rules within default Cluster Monitoring Operator stack\nSRE | Configure custom alerting solution in the dedicated Application Monitoring stack\n \n\n## Day 2 operations tasks for Event Management  \n\n## Configuring Prometheus Alertmanager [ SRE ]  \n\nDefault Alertmanager instance managed by the Cluster Monitoring Operator can be configured via OpenShift 4.3 console.  \n\n\n![](./images/2020-02-20-08-29-45.png)  \n\n\nOpenShift 4.3 contains a new Alertmanager section on the cluster settings page `Cluster Settings -> Global Configuration -> Alertmanager`  \n\n\n![](./images/2020-02-20-08-41-07.png)\n\nHere you can edit **Alert Grouping** settings like:\n- `Group By` - The labels by which incoming alerts are grouped together. For example, multiple alerts coming in for cluster=A and alertname=LatencyHigh would be batched into a single group.\n- `Group Wait` - How long to initially wait to send a notification for a group of alerts. Allows to wait for an inhibiting alert to arrive or collect more initial alerts for the same group. Usually ~0s to few minutes.\n- `Group Interval` - How long to wait before sending a notification about new alerts that are added to a group of alerts for which an initial notification has already been sent. (Usually ~5m or more.)\n- `Repeat Interval` - How long to wait before sending a notification again if it has already been sent successfully for an alert. (Usually ~3h or more).\n\nand **Receivers** like `Pager Duty` or `Webhook`. Every OpenShift cluster needs a default receiver to handle any alerts not sent to other places. The default receiver that comes with a fresh install is initially very basic, so your first step should be to configure it to suit your needs. For more complex team structures, you may want to send different kinds of alerts to different places by creating more receivers.\n\nThe same can be configured within Alertmanager secret. For the default Alertmanager instance the secret name is `alertmanager-main` and you can edit it using command:\n```\noc -n openshift-monitoring get secret alertmanager-main --template='{{ index .data \"alertmanager.yaml\" }}' |base64 -d > alertmanager.yaml\n```\nThen edit the `alertmanager.yaml`:\n```\nglobal:\n  resolve_timeout: 5m\nroute:\n  group_wait: 30s\n  group_interval: 5m\n  repeat_interval: 12h\n  receiver: default\n  routes:\n  - match:\n      alertname: Watchdog\n    repeat_interval: 5m\n    receiver: watchdog\n  - match:\n      service: example-app\n    routes:\n    - match:\n        severity: critical\n      receiver: team-frontend-page\nreceivers:\n- name: default\n- name: watchdog\n- name: team-frontend-page\n  pagerduty_configs:\n  - service_key: \"your-key\"\n```\nWith this configuration, alerts of critical severity fired by the `example-app` service are sent using the `team-frontend-page` receiver, which means that these alerts are paged to a chosen person.\n\n```\n$ oc -n openshift-monitoring create secret generic alertmanager-main --from-file=alertmanager.yaml --dry-run -o=yaml |  oc -n openshift-monitoring replace secret --filename=-\n```\n\nMore information about PagerDuty integration in the [Pager Duty Integration Guide](https://www.pagerduty.com/docs/guides/prometheus-integration-guide/).\n\n**Integration with Netcool Operations Insight (NOI)**\nBelow you can find an example Alertmanager configuration for integration with IBM NOI via webhook sent to Message Bus Probe.\n\n```\nglobal:\nreceivers:\n  - name: default-receiver\n    webhook_configs:\n    - url: 'http://<msgbus_probe_ip>:<msgbus_probe_port>/probe/webhook/prometheus'\n      send_resolved: true\nroute:\n  group_by: ['alertname','instance']\n  group_wait: 10s\n  group_interval: 5m\n  receiver: default-receiver\n  repeat_interval: 3h\n  routes:\n  - receiver: default-receiver\n    match:\n      alertname: ICPMonitoringHeartbeat\n    repeat_interval: 5m\n```\n- [Sending alerts as SNMP Traps](./SNMP_Trap/)\n- [Official Prometheus Alertmanager documentation](https://prometheus.io/docs/alerting/configuration/)\n\n## Silencing Alerts [ SRE ]\nYou can either silence a specific alert or silence alerts that match a specification that you define. Navigate to the `Monitoring → Alerting → Silences` page of the OpenShift Container Platform web console.\n\n![](./images/2020-02-20-09-37-30.png)\n\nSimilar way you can [get information about silences](https://docs.openshift.com/container-platform/4.3/monitoring/cluster_monitoring/managing-cluster-alerts.html#getting-information-about-silences_managing-alerts), [edit silences](https://docs.openshift.com/container-platform/4.3/monitoring/cluster_monitoring/managing-cluster-alerts.html#editing-silences_managing-alerts) and [expire silences](https://docs.openshift.com/container-platform/4.3/monitoring/cluster_monitoring/managing-cluster-alerts.html#expiring-silences_managing-alerts).\n\n## Define custom alerting rules within default Cluster Monitoring Operator stack [ SRE ]\n\nOpenShift 4.3 provides a new Technology Preview feature: [Monitoring your own services](https://docs.openshift.com/container-platform/4.3/monitoring/monitoring-your-own-services.html). It deploys a new Prometheus Operator instance in the `openshift-user-workload-monitoring` namespace. The new Prometheus instance can be configured to monitor custom metrics coming from the application workloads running on the cluster. Follow the [OpenShift 4.3 documentation](https://docs.openshift.com/container-platform/4.3/monitoring/monitoring-your-own-services.html) in order to configure this feature. Custom alerting rules can be configured by creating a custom `Prometheus Rule` resource as described [here](https://docs.openshift.com/container-platform/4.3/monitoring/monitoring-your-own-services.html#creating-alerting-rules_monitoring-your-own-services). Generated alerts are forwarded to the default Alertmanager instance and then forwarded to notification receivers configured in the main Alertmanager instance. \n\n## Configure custom alerting solution in the dedicated Application Monitoring stack [ SRE ]\nIf you decided to use dedicated monitoring stack for application monitoring, you can create a dedicated Alertmanager instance within mentioned application monitoring stack. This stack (both Prometheus and Alertmanager) are managed independently to the default cluster monitoring stack. Details how to configure dedicated application monitoring stack are described in [openshift-custom-app-monitoring](https://github.ibm.com/CASE/openshift-custom-app-monitoring) repository. The instructions how to deploy and configure dedicated Alertmanager and custom Alerting Rules start [here](https://github.ibm.com/CASE/openshift-custom-app-monitoring#deploy-alertmanager).\n\n## Alerting features per platform\n\n## Kubernetes\nKubernetes platform does not provide monitoring, alerting and notification features.  \n\n## OpenShift\nOpenShift provides alerting solution described in details in this document. \n\n## On IBM Cloud\n\nThere are a few choices for the alert and event management for OpenShift on IBM Cloud such as Sysdig, Activity Tracker with LogDNA, and Cloud Event Management.  We will explain each of them.  \n\n\n\n### IBM Cloud Monitoring with Sysdig\nSysdig provides the event management capability and alert feature in addition to the monitoring capability as shown in below.\n\n![ibmcloud_event_sysdig](./images/ibmcloud_event_sysdig.png)  \n\nOn the Sysdig Events dashboard, you can see events which Sysdig received from OpenShift on IBM Cloud as shown in below.\n\n![ibmcloud_event_sysdig_dashboard](./images/ibmcloud_event_sysdig_dashboard.png) \n\nSysdig can generate notifications based on certain conditions or events you configure.  Using the alert feature in Sysdig, you can configure notifications based on the conditions or events you configure.  As an example, the following picture shows the email notification with CrashLoopBackOff alert.\n\n![ibmcloud_event_sysdig_alert](./images/ibmcloud_event_sysdig_alert.png)  \n\n\n### IBM Cloud Activity Tracker with LogDNA\nYou can view, manage, and audit user-initiated activities that change the state of a service in the OpenShift cluster on IBM Cloud by using the IBM Cloud Activity Tracker with LogDNA service.  \n\nOpenShift on IBM Cloud automatically generates cluster management events and forwards these event logs to IBM Cloud Activity Tracker with LogDNA as shown in below.\n\n![ibmcloud_event_activity_tracker](./images/ibmcloud_event_activity_tracker.png) \n\nhttps://cloud.ibm.com/docs/containers?topic=containers-at_events  \nhttps://cloud.ibm.com/docs/services/Activity-Tracker-with-LogDNA?topic=logdnaat-getting-started#getting-started  \n\n\n\n\n### IBM Cloud Event Management with Built-in Prometheus\nYou can use the IBM Cloud Event Management service to set up real-time incident management and create a consolidated view of problems that occur with your applications and infrastructure.  IBM Cloud Event Management will be a centralized operations platform for event management, incident management, notifications, and runbook automation.  \n\nIBM Cloud Event Management can receive events from various monitoring sources such as OpenShift on IBM Cloud.  \n\nYou can set up an integration with IBM Cloud Event Management to receive alert information from the Built-in Prometheus in OpenShift on IBM Cloud as shown in below.\n\n![ibmcloud_event_builtin_alertmanager](./images/ibmcloud_event_builtin_alertmanager.png) \n\nNote that as of writing, there is an issue which override the customized Alert Manager setting. In other worlds, once the issue is fixed, we can send alerts to IBM Cloud Event Management via the built-in Prometheus Alert Manager.  \n\n\n\n\n### IBM Cloud Event Management with Sysdig\nYou can also set up an integration with IBM Cloud Event Management to receive alert information through the Sysdig service which receives the events from OpenShift on IBM Cloud as shown in below.  \n\n![ibmcloud_event_sysdig_cem](./images/ibmcloud_event_sysdig_cem.png) \n\nIf the built-in Prometheus is NOT your choice, then you would choose this solution instead.\n\n\n## With IBM Cloud Pak for MCM\n\nThe IBM Cloud Pak for Multicloud Managements includes an integrated monitoring solution: [IBM Cloud App Management](https://www.ibm.com/support/knowledgecenter/en/SS8G7U_19.4.0/com.ibm.app.mgmt.doc/welcome.html).\n\n**IBM Cloud Event Management** installed along with IBM Cloud App Management gives you the capability to automate management of incidents and events that are associated with resources and applications. All events that are related to a single application, or to a particular cluster, are correlated with an incident. Event Management can receive events from various monitoring sources, either on-premises or in the cloud.\n\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/day2/EventManagement/index.mdx"}}}}